{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1097da",
   "metadata": {},
   "source": [
    "### Data Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd51161",
   "metadata": {},
   "source": [
    "### Document Structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5c6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3954f624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'sample_source', 'pages': 1, 'author': 'Siddhant Kochhar', 'date': '2025-06-12'}, page_content='This is the main content I am using to create RAG.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content=\"This is the main content I am using to create RAG.\",\n",
    "    metadata={\n",
    "        \"source\": \"sample_source\",\n",
    "        \"pages\": 1,\n",
    "        \"author\": \"Siddhant Kochhar\",\n",
    "        \"date\": \"2025-06-12\",\n",
    "\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea8eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60392cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5f1d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddhantkochhar/Downloads/projects/rag_tutorial/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### Text Loader \n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\")\n",
    "documents = loader.load()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc00404b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.'),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    ')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db546788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/cn.pdf', 'file_path': '../data/pdf_files/cn.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'cn', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Computer Networks enable communication between devices through a set of rules called protocols. Among \\nthese, TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) are two of the most \\nwidely used transport-layer protocols defined in the TCP/IP model. \\nTCP is a connection-oriented protocol, meaning a connection must be established before data can be \\nexchanged. It ensures reliable and ordered delivery through mechanisms like acknowledgment (ACK), \\nretransmission, flow control, and congestion control. TCP breaks large messages into segments, assigns \\nsequence numbers, and uses ACKs to confirm receipt. If a segment is lost, TCP retransmits it automatically. \\nThis reliability makes TCP ideal for applications like web browsing (HTTP/HTTPS), email, and file transfers, \\nwhere accuracy is more important than speed. \\nUDP, in contrast, is a connectionless protocol. It sends datagrams without establishing a connection and does \\nnot guarantee delivery, ordering, or error correction. Since UDP eliminates overhead, it is extremely fast and \\nsuitable for real-time applications like gaming, live video streaming, and VoIP, where speed and low latency \\nare more important than perfect accuracy. For example, in a video call, losing a few packets is acceptable \\ncompared to waiting for delayed retransmissions. \\nReliable data transfer depends heavily on the characteristics of these protocols. TCP provides reliability using \\nseveral techniques: \\n●\\u200b Three-Way Handshake:\\u200b\\n Ensures both sender and receiver are ready for communication before data transfer begins.\\u200b\\n \\n●\\u200b Flow Control (Sliding Window):\\u200b\\n Prevents the sender from overwhelming the receiver by adjusting the sending rate.\\u200b\\n \\n●\\u200b Congestion Control (AIMD, Slow Start):\\u200b\\n Adjusts sending rate based on network load to avoid congestion collapse.\\u200b\\n \\n●\\u200b Checksums:\\u200b\\n Detects errors in transmitted segments.\\u200b\\n \\nUDP, though unreliable by design, can still achieve reliability when necessary through application-level \\nmechanisms, such as added sequence numbers or manual acknowledgments. Some modern protocols like \\nQUIC use UDP as a base but add reliability features for improved performance. \\nUnderstanding TCP vs UDP helps developers choose the right protocol for different scenarios. TCP suits \\ndata-sensitive applications, whereas UDP is the better choice for latency-critical tasks.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 0}, page_content='sustainability\\nReview\\nArtiﬁcial Intelligence and Machine Learning\\nApplications in Smart Production: Progress, Trends,\\nand Directions\\nRaﬀaele Cioﬃ1, Marta Travaglioni 1, Giuseppina Piscitelli 1, Antonella Petrillo 1,*\\nand\\nFabio De Felice 2\\n1\\nDepartment of Engineering, Parthenope University, Isola C4, Centro Direzionale, 80143 Napoli NA, Italy;\\nraﬀaele.cioﬃ@uniparthenope.it (R.C.); marta.travaglioni@uniparthenope.it (M.T.);\\ngiuseppina.piscitelli@uniparthenope.it (G.P.)\\n2\\nDepartment of Civil and Mechanical Engineering, University of Cassino and Southern Lazio, Via G. Di\\nBiasio, 43, 03043 Cassino FR, Italy; defelice@unicas.it\\n*\\nCorrespondence: antonella.petrillo@uniparthenope.it\\nReceived: 1 December 2019; Accepted: 5 January 2020; Published: 8 January 2020\\n\\x01\\x02\\x03\\x01\\x04\\x05\\x06\\x07\\x08\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\nAbstract: Adaptation and innovation are extremely important to the manufacturing industry.\\nThis development should lead to sustainable manufacturing using new technologies. To promote\\nsustainability, smart production requires global perspectives of smart production application\\ntechnology. In this regard, thanks to intensive research eﬀorts in the ﬁeld of artiﬁcial intelligence (AI),\\na number of AI-based techniques, such as machine learning, have already been established in the\\nindustry to achieve sustainable manufacturing. Thus, the aim of the present research was to analyze,\\nsystematically, the scientiﬁc literature relating to the application of artiﬁcial intelligence and machine\\nlearning (ML) in industry. In fact, with the introduction of the Industry 4.0, artiﬁcial intelligence and\\nmachine learning are considered the driving force of smart factory revolution. The purpose of this\\nreview was to classify the literature, including publication year, authors, scientiﬁc sector, country,\\ninstitution, and keywords. The analysis was done using the Web of Science and SCOPUS database.\\nFurthermore, UCINET and NVivo 12 software were used to complete them. A literature review on\\nML and AI empirical studies published in the last century was carried out to highlight the evolution\\nof the topic before and after Industry 4.0 introduction, from 1999 to now. Eighty-two articles were\\nreviewed and classiﬁed. A ﬁrst interesting result is the greater number of works published by the\\nUSA and the increasing interest after the birth of Industry 4.0.\\nKeywords: artiﬁcial intelligence; machine learning; systematic literature review; applications;\\nIndustry 4.0; smart production; sustainability\\n1. Introduction\\nSmart production systems require innovative solutions to increase the quality and sustainability\\nof manufacturing activities while reducing costs. In this context, artiﬁcial intelligence (AI)-driven\\ntechnologies, leveraged by I4.0 Key Enabling Technologies (e.g., Internet of Thing, advanced embedded\\nsystems, cloud computing, big data, cognitive systems, virtual and augmented reality), are ready to\\ngenerate new industrial paradigms [1].\\nIn this regard, it is interesting to remember that the father of artiﬁcial intelligence, John McCarthy [2],\\nin the 1990s, deﬁned artiﬁcial intelligence as “artiﬁcial intelligence is the science and engineering of\\nmaking intelligent machines, especially intelligent computer programs”. Generally, the term “AI” is\\nused when a machine simulates functions that humans associate with other human minds, such as\\nlearning and problem solving [3].\\nSustainability 2020, 12, 492; doi:10.3390/su12020492\\nwww.mdpi.com/journal/sustainability'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 1}, page_content='Sustainability 2020, 12, 492\\n2 of 26\\nOn a very broad account, the areas of artiﬁcial intelligence are classiﬁed into 16 categories [4–8].\\nThese are reasoning, programming, artiﬁcial life, belief revision, data mining, distributed AI, expert\\nsystems, genetic algorithms, systems, knowledge representation, machine learning, natural language\\nunderstanding, neural networks, theorem proving, constraint satisfaction, and theory of computation [9–\\n11].\\nIn the 21st century, AI has become an important area of research in all ﬁelds: Engineering, science,\\neducation, medicine, business, accounting, ﬁnance, marketing, economics, stock market, and law,\\namong others [12–18]. The range of AI has grown enormously since the intelligence of machines with\\nmachine learning capabilities has created profound impacts on business, governments, and society [19].\\nThey also inﬂuence the larger trends in global sustainability. Artiﬁcial intelligence can be useful to solve\\ncritical issue for sustainable manufacturing (e.g., optimization of energy resources, logistics, supply\\nchain management, waste management, etc.). In this context, in smart production, there is a trend to\\nincorporate AI into green manufacturing processes for stricter environmental policies [20]. In fact, as\\nsaid in March 2019 by Hendrik Fink, head of Sustainability Services at PricewaterhouseCoopers, “If we\\nproperly incorporate artiﬁcial intelligence, we can achieve a revolution with regard to sustainability.\\nAI will be the driving force of the fourth industrial revolution” [21].\\nThus, subﬁelds of AI, such as machine learning, natural language processing, image processing,\\nand data mining, have also become an important topic for today’s tech giants. The subject of AI\\ngenerates considerable interest in the scientiﬁc community, by virtue of the continuous evolution of\\nthe technologies available today.\\nThe development of ML as a branch of AI is now very fast. Its usage has spread to various\\nﬁelds, such as learning machines, which are currently used in smart manufacturing, medical science,\\npharmacology, agriculture, archeology, games, business, and so forth.\\nAccording to the above considerations, in this work, a systematic literature review of research\\nfrom 1999 to 2019 was performed on AI and the ML technique. Therefore, it is considered necessary to\\ncreate a classiﬁcation system that refers to the articles that jointly treat the two topics, in order to have\\ngreater variance and reﬂection. Furthermore, to gain a deeper understanding, the inﬂuence of other\\nvariables was explored, such as the thematic areas and the sectors in which the technologies are most\\ninﬂuential. The main contribution of this work is that it provides an overview of the research carried\\nout to date.\\nA number of impressive documentations of established research methods and philosophy have\\nbeen discussed for several years. Unfortunately, little comparison and integration across studies exists.\\nIn this article, a common understanding of AI and ML research and its variations was created.\\nThis paper is not attempting to provide an all-encompassing framework on the literature on AI\\nand ML research. Rather, it attempts to provide a starting point for integrating knowledge across\\nresearch in this domain and suggests paths for future research. It explores studies in certain novel\\ndisciplines: Environmental pollution, medicine, maintenance, manufacturing, etc.\\nFurther research is needed to extend the present boundary of knowledge in AI by integrating\\nprinciples and philosophies of some traditional disciplines into the existing AI frameworks [22–24].\\nThe target that this document would like to assume is not the trigger of a sudden proliferation of\\nan already consolidated sector, but it is hoped that this research could be an important intellectual tool\\nfor both the refocusing of the work and creating new intellectual opportunities. This paper presents\\nvaluable ideas and perspectives for undergoing research on AI and ML.\\nThe ﬁnal aim was to anticipate the transformation of the discipline in the future age. This would\\nbe a journey that may experience change in its course as new generations of scholars contribute to the\\ndialogue and to the action. As noted earlier, this work presents a review, hence it lays a foundation for\\nfuture inquiry. It not only oﬀers a basis for future comparisons but prompts a number of new questions\\nfor investigations as well. While topics that might be considered as results of this work are numerous,\\nsome are of particularly broad interest or impact.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 2}, page_content='Sustainability 2020, 12, 492\\n3 of 26\\nThe paper is organized as follows. Section 2 presents the proposed methodology and details the\\nresearch methodology adopted for the literature survey. Section 3 analyzes the main results of the\\nbibliometric analysis. Finally, in Section 4, the main contribution of the research is summarized.\\n2. Methodology\\nThe methodological approach used mixes bibliometric, content analysis, and social network\\ntechniques. In this study, a state-of-the-art research was conducted through the SCOPUS and Web\\nof Science databases. For the publication time span, the time from 1999 to 2019 was considered with\\nthe intent to understand how the level of attention towards the topic has changed before and after\\nthe introduction of Industry 4.0. The research methodology chosen for this study was a systematic\\nliterature review [25]. The main phases of the study were as follows:\\n1.\\nPhase 1: Research and Classiﬁcation. The present phase was divided into three steps:\\n•\\nStep 1: Identiﬁcation;\\n•\\nStep 2: Screening; and\\n•\\nStep 3: Inclusion.\\nIn phase 1, bibliometric data was collected (step 1). Then, a screening of the overall result was\\ncarried out to identify which documents can be taken into consideration, in line with the research areas\\ndeemed interesting and relevant (step 2). At the end of this step, the last step (step 3) aimed to select\\nthe documents to be analyzed in detail.\\n2.\\nPhase 2: Analysis. Once phase 1 was completed, the next phase was phase 2, which was the\\nanalysis of the results. The approach used for the bibliometric analysis included:\\n•\\nThe use of indicators for the parameters studied; and\\n•\\nSNA (social network analysis) for the keywords.\\nThe indicators chosen to perform the analysis were total papers (TPs), which is the total number\\nof publications, and total citations (TCs), which is the total number of citations.\\nSNA ﬁnds application in various social sciences, and has lately been employed in the study of\\nvarious phenomena, such as international trade, information dissemination, the study of institutions,\\nand the functioning of organizations. The analysis of the use of the term SNA in the scientiﬁc literature\\nhas undergone exponential growth in the use of this mode of computable representation of complex\\nand interdependent phenomena. For the purpose of the study, UCINET, NetDraw software was used,\\nwhich was expressly designed for the creation and graphic processing of networks, and was used to\\nrepresent the keywords in the network, and Excel for data input.\\nThe software UCINET, NetDraw returned a sociometric network that describes the relationships\\nbetween the classes, that is, data entered as input.\\nFurthermore, NVivo 12 software, the leading program for computer-assisted qualitative analysis\\n(CAQDAS), was used to analyze keywords of all documents. In this speciﬁc case, it was used to\\nidentify the possible links between the keywords of the various documents examined, developing\\nconceptual schemes from which to make interpretative hypotheses.\\n3.\\nPhase 3: Discussion. At the end of the second phase, a third and ﬁnal one followed, where the\\nresults were discussed, and conclusions were drawn.\\nIn Figure 1, the main phases and steps followed for the analysis are shown.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 3}, page_content='Sustainability 2020, 12, 492\\n4 of 26\\n \\nFigure 1. Process ﬂow chart.\\n3. Results of the Bibliometric Analysis\\n3.1. Phase 1: Research and Classiﬁcation\\nThe ﬁrst phase consisted of the search for documents, which included the activities of collecting the\\nmaterial belonging to the academic universe. This ﬁrst phase was divided into three steps as follows.\\n3.1.1. Identiﬁcation (Step 1)\\nFor a comprehensive survey of the phenomenon, an investigation on the Scopus (SCP) and Web of\\nScience (WoS) databases was carried out using Boolean operators. We began by making a search query\\non the Scopus and WoS databases with the general keywords “artiﬁcial intelligence” AND “machine\\nlearning” AND “application”, as shown in Table 1.\\nIn order to maintain the consistency of the results, the same keywords were used in both databases\\nand a time horizon of 20 years was chosen, from 1999 to 2019.\\nThe choice of keywords for performing the survey was based on the awareness that AI and ML\\ncan be an important tool in the eﬀort to adopt responsible business practices in the context of smart\\nproduction. In this regard, it is worthy to note that with the increasingly urgent discussions of climate\\nchange, it seemed appropriate to focus our research on the topic of sustainability. Thus, the selection of\\npapers also considered applications on sustainability.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 4}, page_content='Sustainability 2020, 12, 492\\n5 of 26\\nTable 1. Keywords and time period.\\nKeywords\\nTime Period\\nArtiﬁcial Intelligence\\n1999–2019\\nMachine Learning\\nApplication\\nThe search returned in total 13,512 documents.\\nThe results extracted by Scopus are numerically superior to Web of Science (WoS): 12,445 for the\\nﬁrst and only 1081 for the second one (Table 2).\\nTable 2. Total results of research on Scopus and WoS.\\nResearch Carried out on 2019\\nSource of research\\nScopus\\nWeb of Science\\nResults\\n12,445\\n1081\\nThe result is not entirely unexpected, and the reason is to be found in the fact that Scopus, being\\nan Elsevier product, collects data from all the other databases, in particular Science Direct and those\\nqueried by the Scirus search engine, while Web of Science (WoS) collects fewer documents.\\nFrom the documents extracted in Scopus, it was found that most of them are conference papers\\n(57.28%) and, subsequently, articles (33.85%).\\nOn the contrary, the research on Web of Science (WoS) underlines that most of the documents are\\narticles (46.12%) and, subsequently, proceedings papers (42.86%).\\nAll the document types are ﬁlled in Table 3.\\nTable 3. Distribution of document types in Scopus and Web of Science.\\nWeb of Science\\nScopus\\nDocument Types\\nRecords\\nContribute %\\nDocument Types\\nRecords\\nContribute %\\nArticle\\n481\\n46.12\\nConference Paper\\n7128\\n57.28\\nProceedings paper\\n447\\n42.86\\nArticle\\n4212\\n33.85\\nReview\\n133\\n12.76\\nReview\\n412\\n3.31\\nEditorial material\\n16\\n1.53\\nArticle in Press\\n194\\n1.56\\nMeeting abstract\\n2\\n0.19\\nBook Chapter\\n177\\n1.42\\nBook chapter\\n1\\n0.1\\nConference Review\\n177\\n1.42\\nRetracted publication\\n1\\n0.1\\nBook\\n90\\n0.72\\n-\\n-\\n-\\nEditorial\\n27\\n0.22\\n-\\n-\\n-\\nNote\\n10\\n0.08\\n-\\n-\\n-\\nLetter\\n9\\n0.07\\n-\\n-\\n-\\nShort Survey\\n9\\n0.07\\nAI began working in the 1940s and researchers showed strong expectations until the 1970s when\\nthey began to encounter serious diﬃculties and investments were greatly reduced.\\nSince then, a long period began, known as the “AI winter” [26]: Despite some great successes,\\nsuch as IBM’s Deep Blue system, which in the late 1990s defeated the then chess world champion\\nGarri Kasparov, the study of solutions for AI has only come back for a few years. The push for a new\\ntechnological development has been given by the I4.0, which considered AI as one of the primary key\\nenabling technologies (KETs).\\nFrom this period onwards, the literature has been enriched with documents, as shown in Figure 2.\\nGrowth is apparent after 2011 when new technologies began to be implemented more frequently.\\nIn fact, the Industry 4.0 term ﬁrst appeared at Hannover Messe in 2011 when Professor Wolfgang'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 5}, page_content='Sustainability 2020, 12, 492\\n6 of 26\\nWahlster, Director and CEO of the German Research Center for Artiﬁcial Intelligence, addressed the\\nopening ceremony audience.\\nFigure 2. Research growth on Scopus and Web of Science.\\nIn fact, this research indicates that over the time period considered (1999–2019), the number of\\npublished articles remains almost constant until 2013, from which it undergoes an increase.\\nSubsequently, the increase in the adoption of these ones has led researchers to keep pace with the\\ngrowth of I4.0 [27].\\n3.1.2. Screening (Step 2)\\nTrying to give an overview of the topics and areas interface, in the screening phase, an analysis\\nof documents characterized by free access was chosen, excluding those that have restrictions, and to\\nrestrict the ﬁeld to the thematic areas of scientiﬁc interest.\\nWith this in mind, the number of open access items has been drastically reduced (1288 results\\nfor Scopus and 149 for WoS) and, also applying the ﬁlter related to the thematic areas (Table 4), it\\ndetermined a further reduction: 947 for Scopus and 60 for WoS.\\nTable 4. Subject area ﬁlter on Scopus and WoS.\\nSubject Area\\nScopus\\nWeb of Science (WoS)\\nComputer Science\\nChemical\\nEngineering\\nComputer Science\\nInformation Systems\\nComputer Science\\nArtiﬁcial Intelligence\\nAutomation Control\\nSystems\\nEngineering\\nEnergy\\nMaterials Science\\nMultidisciplinary\\nEnvironmental\\nSciences\\nEnvironmental Studies\\nMaterials Science\\nDecision Science\\nEngineering Electrical\\nElectronic\\nComputer Science\\nHardware Architecture\\nOperations Research\\nManagement Science\\nEnvironmental\\nScience\\nBusiness\\nManagement and\\naccounting\\nTelecommunications\\nIndustrial Relations\\nLabor\\nRobotics\\nEngineering\\nEnvironmental\\nEngineering\\nManufacturing\\nThermodynamics\\nEngineering Industrial\\nComputer Science\\nTheory Methods\\nEnergy Fuels\\nEngineering Civil\\nEngineering\\nMechanical\\nComputer Science\\nCybernetics\\nComputer Science\\nSoftware Engineering\\nMultidisciplinary\\nSciences'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 6}, page_content='Sustainability 2020, 12, 492\\n7 of 26\\nNote how the number of ﬁlters applied is diﬀerent. The databases, in fact, oﬀer the same search\\noptions, but, in the speciﬁc case of the thematic areas, the latter are more numerous and structured on\\nWeb of Science (WoS) compared to Scopus.\\n3.1.3. Inclusion (Step 3)\\nAt the end of the screening process, the inclusion step was started, which consisted in the selection\\nof documents, which was extracted from the last passage, destined to be included in the sample on\\nwhich bibliometric analysis was performed. In this review step, for the purposes of eligibility, we\\nexamined the complete text of each document independently. For each article, we examined whether\\nthere was interest from the academic world, and if it contained case studies or real applications,\\nproposals for new AI and ML algorithms, or possible future scenarios.\\nTherefore, the ﬁnal sample to be analyzed consisted of 60 documents for Scopus and 22 for WoS.\\n3.2. Phase 2: Analysis\\nThis section presents and discusses the ﬁndings of this review.\\nFirst, an overview of the selected studies is presented. Second, the review ﬁndings according to\\nthe research criteria, one by one in the separate subsections, are reported.\\n3.2.1. Top Highly Inﬂuential Analysis\\nThis section lists the most highly cited documents in WoS and Scopus. The list is structured by\\nresearch source, date, title, authors, source title, and top citation (TP) in WoS or Scopus, according\\nto the research source. The whole list is available in the Appendix A. Looking into the Appendix A,\\nit is possible underline that the document by Larrañaga, Calvo, Santana et al. in 2006 [28] has the\\nhighest citation count of 298. This article reviews machine learning methods for bioinformatics and\\nit presents modelling methods. Moreover, the document year is 2006, so before I4.0 was introduced.\\nTherefore, having more years than today has an advantage in terms of diﬀusion. This means that it is\\none of the most inﬂuential documents in the academic world, as it proposes some of the most useful\\ntechniques for modelling, giving the document the opportunity to become a pioneer in the computer\\nscience research area.\\nObviously, all documents before I4.0, in general, have more citations than the most recent\\ndocuments. However, it is signiﬁcant to note that even recent documents have a very high number\\nof citations compared to the year of publication. This denotes the interest in the topic from the\\nscientiﬁc community.\\nThe citation analysis revealed that the ﬁrst article that we can identify among the most cited in\\nthe I4.0 period dates to 2016. The work, published by Krawczyk [29], proposes application models to\\nfurther develop the ﬁeld of unbalanced learning, to focus on computationally eﬀective, adaptive, and\\nreal-time methods, and provides a discussion and suggestions on the lines of future research in the\\napplication subject of the study. It received 119 citations. Moreover, an article published by Wuest,\\nWeimer, Irgens et al. [30] received much attention among the scientiﬁc community. It contributes by\\npresenting an overview of the available machine learning techniques.\\nFinally, the citation analysis pointed out that the average number of citations of all documents is\\n16.58. This value is expected to increase rapidly considering the interest in the issues of ML and AI.\\n3.2.2. Publications by Years\\nConsistent with what is deﬁned in Section 3.1.1., the study shows that the number of items included\\nin the analysis is deﬁnitely low for the entire period before I4.0 and then suddenly increases, starting\\nin 2012. The data shown in Figure 3 also show two holes in the 2001–2008 and 2008–2011 intervals.\\nThis means that the technological applications were limited before it became an enabling technology of\\nI4.0 in all respects, only to have a peak of technological implementation, as was foreseeable.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 7}, page_content='Sustainability 2020, 12, 492\\n8 of 26\\nFigure 3. Years of publications.\\nWith reference to 2019, the ﬁgure refers to the ﬁrst months of the year, so it is plausible that during\\nthe year, there will be a further increase in the documents in the literature. Furthermore, an increase is\\nexpected in the coming years, in parallel with the growth of I4.0\\n3.2.3. Most Collaborative Authors\\nThe analysis highlighted that most of publications have more than one author. From this point of\\nview, it is possible to identify the number of authors for each document. As shown in Figure 4, most of\\nthe manuscripts were produced by groups ranging from two to ﬁve authors. The indicators chosen to\\nperform the analysis were total papers (TPs), which is the total number of publications.\\n \\nFigure 4. Collaborative groups.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 8}, page_content='Sustainability 2020, 12, 492\\n9 of 26\\n3.2.4. Research Areas Analysis\\nThe total research area analysis collected from the 82 papers was 164 because each paper can be\\nconsidered as more than one research area analysis. Given the small number of documents identiﬁed\\nin the period before I4.0, the ranking refers mostly to the current industrial revolution. Also, in this\\ncase, the result is consistent with the introduction of paradigm 4.0, which has intensiﬁed research and\\nthe adoption of technology.\\nThe ﬁrst thematic areas and disciplines that are at the top of the ranking are computer science,\\nengineering and biochemistry, genetics, and molecular Biology, respectively, with 29%, 23%, and 6% of\\npublications. Furthermore, the other disciplines identiﬁed for which applicative ﬁndings are found are\\nconsidered transversal to the ﬁrst three disciplines and this is a consequence of I4.0. In terms of the\\npercentage contribution, the ﬁrst three areas cover about 60% of the papers considered.\\nConsidering the top 20 research areas, given the frequency of the research areas’ distribution,\\nFigure 5 shows a higher level of concentration in the disciplines indicated above.\\nFigure 5. Top 20 research areas contributions.\\nIn fact, in terms of the percentage contribution, the ﬁrst ﬁve areas cover about 70% of the papers\\nconsidered. Regardless, by only counting research areas found once, there is a total of 27.\\nThis means two things:\\n•\\nThe large number of ﬁelds in which this kind of research is involved; and\\n•\\nMost papers have a transversal approach, that is, the object of each research crosses more than\\none ﬁeld of application, thus involving more research areas.\\nThis conﬁrms the wide interest in these subjects from several ﬁelds.\\n3.2.5. Top Source Journals Analysis\\nIn this section, the top 20 sources or journals that were published most frequently were extracted.\\nA journal is a time-bound publication with the objective of promoting and monitoring the progress\\nof the discipline it represents.\\nIn this speciﬁc case, the total source journals detected from the documents is 74, but, considering\\nthe top 20, given the frequency of the source journals’ distribution, only the ﬁrst 13 sources have more\\nthan one paper published, with a total percentage contribution of 43% of the total.\\nAfter analyzing the sources separately, the results obtained in the two databases were found to\\nnot be the same. In WoS, the top source journal was IEEE Access with two publications while in Scopus,'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 9}, page_content='Sustainability 2020, 12, 492\\n10 of 26\\nthe top source journals are Procedia Computer Science, Matec Web of Conferences, and Machine Learning\\nwith four publications, which contribute 5% of the total.\\nAggregating the data collected from the two databases, the ranking moves to that obtained by\\nScopus, making sure that IEEE Access is no longer ﬁrst in the standings, but only eighth, and that the\\nformer are precisely those of Scopus: Procedia Computer Science, Matec Web Of Conferences, and Machine\\nLearning, with the same number of publications. Next, the 10 source journals have a 3% publication\\ncontribution while the rest have a one-to-one relationship (1%) with the corresponding source journal.\\nThe low level of concentration of the sources suggests that there is a great deal of interest in\\nthese topics from several scientiﬁc journals. As a matter of fact, it is foreseeable that specialized sector\\nsources (AI Magazine and Machine Learning) are among the ﬁrst 13; however, it is interesting to note\\nthat other sources are involved, such as Sustainability Switzerland or BMC Bioinformatics and Nuclear\\nEngineering and Design.\\nFigure 6 shows the top 20 source journals contributions.\\nFigure 6. Top 20 source journals contributions.\\n3.2.6. Country Analysis\\nThe results that emerged through research on the two databases are consistent with each other.\\nIn both cases, in fact, the countries that give the greatest contribution to the research are China and\\nthe United States (Figure 8). The result is obvious since in China and the United States, more than\\n1.3 billion and 0.3 millions of people live, respectively, and so there are more researchers than in\\nthe single European nations. Focusing on Europe, Germany published more papers than any other\\nEuropean country. This is not a random result: I4.0 was born in Germany, so this outcome was expected.\\nHowever, the following observation cannot be ignored from this data: The USA and China carry the\\nﬁrst two places in the list while it is not the same for European countries. Europe, despite its talents\\nand resources, has lost ground. Presenting its report on artiﬁcial intelligence, the French deputy and\\nmathematician Cédric Villani declared that, “Europe must be able to compete with China and the\\nUnited States while protecting its citizens and pointing the way to go on ethical issues”. If we are not\\ncareful, the 21st century rules will not be deﬁned in Brussels, but in Shanghai. Artiﬁcial intelligence is\\nalso a land marked by intense geopolitical rivalry that could redeﬁne global power relations.\\nEven so, regarding Europe, it is worthy to also note that since 2017, France, Germany, and Italy\\nhave intensiﬁed their trilateral cooperation to promote digitizing the manufacturing industry. In this'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 10}, page_content='Sustainability 2020, 12, 492\\n11 of 26\\nregard, in the near future, we expect a signiﬁcant evolution of smart production initiatives and therefore\\nan increase in scientiﬁc research.\\nFigure 7 shows the country contribution distribution.\\n \\nFigure 7. Top 20 countries contributions.\\n3.2.7. Aﬃliation Analysis\\nThe total number of aﬃliation detected from the 82 papers is 153. Also, in this case, considering\\nthe top 20, the frequency of the aﬃliation distribution shows that most papers have a one-to-one\\nrelationship with the corresponding aﬃliation. Only the ﬁrst four aﬃliations have three papers (2% of\\nthe contribution) and the second four have two papers (1.3% of the contribution). This result gives us\\ninformation about the wide interest on this subject from several universities and research centers all\\nover the world. Then, the aﬃliation analysis conﬁrms the result of the country analysis (Figure 8). In\\nfact, if we try to sum the ﬁrst eight aﬃliations by their own country, the outcome is:\\n•\\nNine papers from China;\\n•\\nSix papers from Germany; and\\n•\\nFive papers from the USA.\\nIn September 2018, the most important event on artiﬁcial intelligence was held in Shanghai. China\\nis very determined to focus on future technologies.\\nFor some months, China has become the world’s leading power in terms of scientiﬁc publications.\\nLate in the 20th century technologies, China chose to do what the English-speaking people call a “frog\\njump” and focus on 21st century technologies.\\nChina, with its 800 million Internet users and without any privacy protection policy, has access to\\nmore personal data than the United States and Europe.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 11}, page_content='Sustainability 2020, 12, 492\\n12 of 26\\nFigure 8. Top 20 institute aﬃliations contributions.\\n3.2.8. Top Keywords Analysis\\nThrough NVivo 12, the top 20 keywords were extracted directly, which are those that always\\nappear in association with each document.\\nStarting from this classiﬁcation, the graphic representation, a word cloud shape, of the keywords\\n(Figure 9) was extracted. It can be noted that the most used term is precisely “machine”, “learning”,\\nand “intelligence”, which the software represents with greater characters than all the other terms.\\nFigure 9. Top 20 keywords cloud contribution by NVivo 12.\\nThe font size describes how much the keyword is indexed. Another mode of representation is\\nthe tree words (Figure 10). Also, in this case, the most indexed words are those represented in the\\nlarger boxes.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 12}, page_content='Sustainability 2020, 12, 492\\n13 of 26\\nFigure 10. Top 20 keywords tree contribution by Nvivo 12.\\nAs expected, the most indexed words are obviously “learning”, “machine”, and “intelligence”,\\nwith high numbers. It is logical that among the ﬁrst results, words that recall the technology itself were\\nobtained, but it is interesting to note that words referring to other ﬁelds of AI applications are also\\nindexed. The reason is to be found in the fact that AI and ML are technologies that cross all the sectors\\ninvolved in I4.0 and that, therefore, do not remain circumscribed.\\nSpeciﬁcally, words, such as “data”, “neural”, “decision”, and “management”, are very or average\\nindexed, demonstrating the fact that AI also extends to many other sectors.\\nAnother tool for the analysis for keywords is the UCINET software, through which social networks\\nanalysis is carried out.\\nSocial network analysis (SNA), which is also often called social network theory, is a modern\\ntechnology of social relations.\\nSNA ﬁnds application in various social sciences, and has recently been used in the study of\\nvarious phenomena, such as international trade, information dissemination, the study of institutions,\\nand the functioning of organizations. The analysis of the use of the term SNA in the scientiﬁc\\nliterature shows that in the last ﬁve years, there has been exponential growth of the use of this mode of\\ncomputable representation of complex and interdependent phenomena. The software returns a graph\\nrepresenting a socio-metric network (Figure 11), which draws the relationships that exist within the\\nclass. Each relationship is represented by an oriented arrow.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 13}, page_content='Sustainability 2020, 12, 492\\n14 of 26\\nFigure 11. Keywords Network by UCINET Software.\\nIn Figure 11, nodes and leaves can be identiﬁed. The nodes are represented by red circles and are\\ncorrespond to the most common keywords, where the words “machine”, “learning”, “artiﬁcial”, and\\n“intelligence” have been united to form the key words “machine learning” and “artiﬁcial intelligence”.\\nThe leaves, on the other hand, are represented by blue squares and correspond to the articles. To\\nfacilitate reading, the document titles were not inserted, but the (Identiﬁcation) ID count for each of\\nthem is shown in the Appendix A.\\nThe ﬁrst thing that can be noticed is the isolation of many leaves that are not connected to the\\nnodes. This means that the corresponding documents are not described by the keywords represented\\nby the nodes. Really, they are characterized by keywords that have a frequency of the order of units.\\nAnother thing that easily jumps to the eye is a density that is larger around the keywords\\n“machine learning”, “decision”, “data”, “algorithm”, “system”, “artiﬁcial intelligence”, “method”,\\nand “optimization”. This density is reﬂected in the cloud and the box chart produced by NVivo 12.\\nTherefore, we can say that those are the words that most often appear in the documents analyzed,\\nemphasizing, once again, that they include terms that do not just refer to the technology object of study\\nbut also to other ﬁelds of application.\\n3.3. Phase 3: Discussion\\n3.3.1. Beneﬁts of Artiﬁcial Intelligence and Machine Learning in Industrial Contexts\\nFrom the analysis of the research carried out, the ﬁrst information that emerged is that there is a\\ngrowing importance of innovation and digitalization in products, services, and processes. Consequently,\\nit means that the adoption of advanced manufacturing technologies, such AI and ML, is an emerging\\nissue. In other words, AI/ML algorithms represent an opportunity to handle high dimensional problems\\nand data. The interest in the subject is extended to all scientiﬁc sectors, but with a focus on computer\\nscience and engineering.\\nThe most signiﬁcant beneﬁts of using AI and ML in industrial sectors include: (1) Greater\\ninnovation, (2) process optimization, (3) resources optimization, and (4) improved quality.\\nAfter all, AI with ML is one of the most important technologies today and is transforming the\\neconomy and society, as demonstrated by the over 340,000 patent applications ﬁled since the 1950s.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 14}, page_content='Sustainability 2020, 12, 492\\n15 of 26\\nOther information that emerged is about the authors and aﬃliation. Many of these are in a 1:1 ratio\\ncompared to the selected documents and this supports the fact that there is no interest in technological\\napplications in one direction, but that, once again, the interest is very wide in the scientiﬁc community.\\nFurthermore, it can be said that the countries most interested in scientiﬁc research are the USA,\\nChina and European countries. This result is not a surprise.\\nIn terms of investment, the eﬀort currently being deployed by the United States and China to\\nacquire dominance in the AI sector is far superior to that of other countries. More speciﬁcally, China\\nhas clearly stated its ambition to become a world leader in AI by 2030 [31]. Among the Chinese plans,\\nof absolute interest is the “Made in China 2025” plan, dedicated to the manufacturing sector; the\\n“Internet +” plan is also dedicated to smart manufacturing and innovation.\\nA direct consequence of the above considerations could be having new generations of researchers\\nwho will contribute to future comparisons, accompanied by new questions for investigations.\\n3.3.2. Emerging Trends of Artiﬁcial Intelligence and Machine Learning in Sustainable Manufacturing\\nFrom the perspective of sustainability, the analysis highlighted that the new paradigm of smart\\nmanufacturing has the potential to bring fundamental improvements in the industry by addressing\\nthe issue of scarce resources and improving productivity.\\nIn fact, the survey pointed out a growing interest on applications related to green manufacturing\\nand sustainable development, proving that AI/ML play an important role in increasing sustainability\\nthrough the intelligent utilization of materials and energy consumption (i.e., reduction of energy\\nconsumption and pollutant emissions, environmental footprint monitoring and evaluation, etc.).\\nFurthermore, it emerged that AI/ML algorithms present a wide array of applications that provide\\nan opportunity for sustainable development, which will involve several stakeholders from diﬀerent\\ncountries and sectors, including inventory and supply chain management, predictive maintenance,\\nand production.\\nIn particular, Pérez-Ortiz, Jiménez-Fernández, Gutiérrez et al. [32] reviewed the most important\\nclassiﬁcation algorithms applied to renewable energy (RE) problems. The main use of algorithms\\nis as a tool for predictive analysis and consequently for data preprocessing, result interpretation, or\\nevaluation in order to improve energy and resource management.\\nIn this context, it also emerged that AI/ML have been successfully utilized in various processes’\\noptimization, applications in manufacturing, and predictive maintenance in diﬀerent industries.\\nThe work published by Lieber, Stolpe, Konrad et al. [33] represents a good research within steel\\nindustry production. It proposes an approach for automatically preprocessing value series data to\\nimprove the quality of the process and products. It means that AI/ML techniques were found to\\nprovide promising potential for improved quality control optimization in manufacturing systems.\\nAppropriate adoption of AI/ML technologies will promote sustainable manufacturing and the\\nformation of a new generation of intelligent manufacturing, including all areas that characterize a\\nsustainable process, ranging from the supply chain management to quality control, to predictive\\nmaintenance, to energy consumption.\\nTable 5 summarizes the main areas in sustainable manufacturing, their respective key objectives,\\nand the main AI/ML applications.\\nHowever, the relationship between I.4 technologies, AI/ML, and sustainability demands a more\\nconceptual and empirical investigation. This is corroborated by an article recently published in Nature\\nSustainability by the director of the Earth Institute at Columbia University, Jeﬀrey Sachs, and other\\nexperts, and the so-called Fourth Industrial Revolution (made of artiﬁcial intelligence and other digital\\ntechnologies) is even cited as one of the six transformations necessary to achieve the sustainable\\ndevelopment goals [34].'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 15}, page_content='Sustainability 2020, 12, 492\\n16 of 26\\nTable 5. Main areas in sustainable manufacturing.\\nMain Areas in Sustainable\\nManufacturing\\nKey\\nObjective\\nAI/ML\\nApplications\\nSupply Chain Management\\nReady product available in the\\nappropriate place at a speciﬁc time\\nImproves transparency, accelerates\\ndecision-making, and produces accurate\\ndemand forecasting\\nQuality Control\\nRecognize the early signs of\\npotential production failures\\nwithin the shortest terms in order\\nto save resources and sustain\\noperational eﬃciency\\nImproves the response time and allows\\neliminating possible failures\\nPredictive Maintenance\\nDetects possible production\\nmalfunctions that may cause\\nproduct quality issues\\nCreates accurate forecasts as to when\\nthe machinery must be repaired\\nEnergy consumption\\nRecommendations that will strike\\na balance in energy use\\nImproves excessive use of certain\\nmaterials, redundant production scrap\\nwaste, ineﬃcient supply chain\\nmanagement, logistics, and unequal\\ndistribution of energy resources.\\n4. Conclusions\\nThis research focused on the study of the state of the art of AI and ML applications, selecting\\nliterature on what has now become a particularly hot topic in scientiﬁc research. The literature available\\non any subject is now wide and a complete coverage of all the documents published with respect to a\\nparticular topic can be challenging or even impossible. Therefore, a systematic selection of the most\\nrelevant literature was implemented. This document provides a systematic review of applications\\nin various scientiﬁc ﬁelds using ML techniques. For the selection of documents, objective and clear\\nmethods of investigation were used, independent of the experience of the researchers. Among the\\nobjectives of the document, it aimed to not only provide a comprehensive framework on the literature\\non the research of AI and ML but also a starting point for integrating knowledge through research in\\nthis area and to suggest future research paths. It is important to underline that this document was\\nproduced using only two databases, i.e., WoS and Scopus, in which only documents with open access\\nwere included. There are, therefore, many other documents with restricted access and other indexing\\ndatabases, such as Google Scholar, that could be integrated for future research.\\nAuthor Contributions: All authors contributed equally to this work. All authors have read and agreed to the\\npublished version of the manuscript.\\nFunding:\\nThis work has been conducted under the framework of the Italian project “Linee Guida per\\nI4.0-Campania”—funded by Regione Campania within POR FSE 2014–2020 Asse IV “Capacità istituzionale e\\namministrativa” objectives 18 (RA) 11.3 and 21 (RA) 11.6.\\nConﬂicts of Interest: The authors declare no conﬂict of interest.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 16}, page_content='Sustainability 2020, 12, 492\\n17 of 26\\nAppendix A\\nID Count\\nResearch\\nSource\\nID\\nDoc\\nYear\\nTitle\\nAuthors\\nSource Title\\nTC\\n1\\nSCP\\n2\\n2006\\nMachine learning in bioinformatics\\nLarrañaga, P.; Calvo, B.; Santana,\\nR.; Bielza, C.; Galdiano, J.; Inza, I.;\\nLozano, J.A.; Armañanzas, R.;\\nSantafé, G.; Pérez, A.; Robles, V.\\nBrieﬁngs in Bioinformatics\\n298\\n2\\nWoS\\n62\\n2008\\nData-driven modelling: Some past experiences\\nand new approaches\\nSolomatine, D.P.; Ostfeld, A.\\nJournal of Hydroinformatics\\n160\\n3\\nSCP\\n26\\n2016\\nLearning from imbalanced data: Open\\nchallenges and future directions\\nKrawczyk, B.\\nProgress in Artiﬁcial\\nIntelligence\\n119\\n4\\nWoS\\n63\\n2001\\nComputer go: An AI oriented survey\\nBouzy, B; Cazenave, T\\nArtiﬁcial Intelligence\\n114\\n5\\nSCP\\n6\\n2008\\nStructured machine learning: The next ten years\\nDietterich, T.G.; Domingos, P.;\\nGetoor, L.; Muggleton, S.;\\nTadepalli, P.\\nMachine Learning\\n75\\n6\\nSCP\\n28\\n2016\\nMachine learning in manufacturing:\\nAdvantages, challenges, and applications\\nWuest, T.; Weimer, D.; Irgens, C.;\\nThoben, K.D.\\nProduction and\\nManufacturing Research\\n52\\n7\\nWoS\\n64\\n2017\\nMachine learning paradigms for next-generation\\nwireless networks\\nJiang, C.; Zhang, H.; Ren, Y.; Han,\\nZ.; Chen, K.C.; Hanzo, L.\\nIeee Wireless\\nCommunications\\n50\\n8\\nSCP\\n3\\n2006\\nMachine learning techniques in disease\\nforecasting: A case study on rice blast prediction\\nKaundal, R.; Kapoor, A.A.;\\nRaghava, G.P.S.\\nBMC Bioinformatics\\n48\\n9\\nSCP\\n4\\n2008\\nA comparison of machine learning algorithms\\nfor chemical toxicity classiﬁcation using a\\nsimulated multi-scale data model\\nJudson, R.; Elloumi, F.; Woodrow,\\nR.W.; Li, Z.; Shah, I.\\nBMC Bioinformatics\\n45\\n10\\nSCP\\n19\\n2015\\nA review of intelligent driving style analysis\\nsystems and related artiﬁcial intelligence\\nalgorithms\\nMeiring, G.A.M.; Myburgh, H.C.\\nSensors (Switzerland)\\n33\\n11\\nSCP\\n21\\n2016\\nA machine learning framework for gait\\nclassiﬁcation using inertial sensors: Application\\nto elderly, post-stroke and huntington’s disease\\npatients\\nMannini, A.; Trojaniello, D.;\\nCereatti, A.; Sabatini, A.M.\\nSensors\\n31'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 17}, page_content='Sustainability 2020, 12, 492\\n18 of 26\\nID Count\\nResearch\\nSource\\nID\\nDoc\\nYear\\nTitle\\nAuthors\\nSource Title\\nTC\\n12\\nSCP\\n1\\n2006\\nApplication of machine learning in SNP\\ndiscovery\\nMatukumalli, L.K.; Grefenstette,\\nJ.J.; Hyten, D.L.; Choi, I.Y.;\\nCregan, P.B.; Van Tassell, C.P.\\nBMC Bioinformatics\\n30\\n13\\nSCP\\n10\\n2013\\nBeam search algorithms for multilabel learning\\nKumar, A.; Vembu, S.; Menon,\\nA.K.; Elkan, C.\\nMachine Learning\\n29\\n14\\nWoS\\n65\\n2011\\nRecommender Systems: An Overview\\nBurke, Robin; Felfernig,\\nAlexander; Goeker, M.H.\\nAi Magazine\\n29\\n15\\nSCP\\n11\\n2013\\nBiomedical informatics for computer-aided\\ndecision support systems: A survey\\nBelle, A.; Kon, M.A.; Najarian, K.\\nThe Scientiﬁc World Journal\\n27\\n16\\nSCP\\n23\\n2016\\nApplication of machine learning to construction\\ninjury prediction\\nTixier, A.J.P.; Hallowell, M.R.;\\nRajagopalan, B.; Bowman, D.\\nAutomation in Construction\\n21\\n17\\nSCP\\n12\\n2013\\nQuality prediction in interlinked manufacturing\\nprocesses based on supervised & unsupervised\\nmachine learning\\nLieber, D.; Stolpe, M.; Konrad, B.;\\nDeuse, J.; Morik, K.\\nProcedia CIRP\\n18\\n18\\nSCP\\n29\\n2016\\nSemantic framework of internet of things for\\nsmart cities: Case studies\\nZhang, N.; Chen, H.; Chen, X.;\\nChen, J.\\nSensors\\n17\\n19\\nSCP\\n20\\n2015\\nSupport vector machines in structural\\nengineering: A review\\nÇevik, A.; KURTO ˘GLU, A.E.;\\nBilgehan, M.; Gül¸san, M.E.;\\nAlbegmprli, H.M.\\nJournal of Civil Engineering\\nand Management\\n15\\n20\\nSCP\\n25\\n2016\\nA review of classiﬁcation problems and\\nalgorithms in renewable energy applications\\nPérez-Ortiz, M.;\\nJiménez-Fernández, S.; Gutiérrez,\\nP.A.; ( . . . ); Hervás-Martínez, C.;\\nSalcedo-Sanz, S.\\nEnergies\\n15\\n21\\nSCP\\n43\\n2018\\nArtiﬁcial intelligence (AI) methods in optical\\nnetworks: A comprehensive survey\\nMata, J.; de Miguel, I.; Durán, R.J.;\\n( . . . ); Jukan, A.; Chamania, M.\\nOptical Switching and\\nNetworking\\n15\\n22\\nSCP\\n14\\n2014\\nFault diagnosis of automobile gearbox based on\\nmachine learning techniques\\nPraveenkumar, T.; Saimurugan,\\nM.; Krishnakumar, P.;\\nRamachandran, K.I.\\nProcedia Engineering\\n14\\n23\\nSCP\\n16\\n2014\\nImproving active Mealy machine learning for\\nprotocol conformance testing\\nAarts, F.; Kuppens, H.; Tretmans,\\nJ.; Vaandrager, F.; Verwer, S.\\nMachine Learning\\n11'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 18}, page_content='Sustainability 2020, 12, 492\\n19 of 26\\nID Count\\nResearch\\nSource\\nID\\nDoc\\nYear\\nTitle\\nAuthors\\nSource Title\\nTC\\n24\\nWoS\\n66\\n2016\\nStrategies and Principles of Distributed Machine\\nLearning on Big Data\\nXing, E.P.; Ho, Q.; Xie, P.; Wei, D.\\nEngineering\\n11\\n25\\nWoS\\n67\\n2015\\nRecent advances on artiﬁcial intelligence and\\nlearning techniques in cognitive radio networks\\nAbbas, N.; Nasser, Y.; El Ahmad,\\nK.\\nEurasip Journal on Wireless\\nCommunications and\\nNetworking\\n11\\n26\\nWoS\\n68\\n2018\\nArtiﬁcial intelligence (AI) methods in optical\\nnetworks: A comprehensive survey\\nMata, J.; de Miguel, I.; Duran, R.J.;\\nMerayo, N.; Singh, S.K.; Jukan, A.;\\nChamania, M.\\nOptical Switching and\\nNetworking\\n9\\n27\\nSCP\\n40\\n2018\\nA big data driven sustainable manufacturing\\nframework for condition-based maintenance\\nprediction\\nKumar, A.; Shankar, R.; Thakur,\\nL.S.\\nJournal of Computational\\nScience\\n27, pp. 428–439\\n8\\n28\\nWoS\\n69\\n2017\\nResearch and Application of a Novel Hybrid\\nModel Based on Data Selection and Artiﬁcial\\nIntelligence Algorithm for Short Term Load\\nForecasting\\nYang, W.; Wang, J.; Wang, R.\\nEntropy\\n8\\n29\\nSCP\\n33\\n2017\\nContext Aware Process Mining in Logistics\\nBecker, T.; Intoyoad, W.\\nProcedia CIRP\\n7\\n30\\nSCP\\n24\\n2016\\nApplications of machine learning methods to\\nidentifying and predicting building retroﬁt\\nopportunities\\nMarasco, D.E.; Kontokosta, C.E.\\nEnergy and Buildings\\n6\\n31\\nSCP\\n37\\n2017\\nOperational Demand Forecasting in District\\nHeating Systems Using Ensembles of Online\\nMachine Learning Algorithms\\nJohansson, C.; Bergkvist, M.;\\nGeysen, D.; ( . . . ); Lavesson, N.;\\nVanhoudt, D.\\nEnergy Procedia\\n6\\n32\\nWoS\\n70\\n2018\\nAdvances in Multiple Criteria Decision Making\\nfor Sustainability: Modeling and Applications\\nShen, K.Y.; Tzeng, G.H.\\nSustainability\\n6\\n33\\nWoS\\n71\\n2017\\nHybrid-augmented intelligence: Collaboration\\nand cognition\\nZheng, N.N.; Liu, Z.Y.; Ren, P.J.;\\nMa, Y.Q.; Chen, S.T.; Yu, S.Y.; Xue,\\nJ.R.; Chen, B.D.; Wang, F.Y.\\nFrontiers of Information\\nTechnology & Electronic\\nEngineering\\n6'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 19}, page_content='Sustainability 2020, 12, 492\\n20 of 26\\nID Count\\nResearch\\nSource\\nID\\nDoc\\nYear\\nTitle\\nAuthors\\nSource Title\\nTC\\n34\\nSCP\\n5\\n2008\\nPerformance evaluation of the NVIDIA GeForce\\n8800 GTX GPU for machine learning\\nEl Zein, A.; McCreath, E.; Rendell,\\nA.; Smola, A.\\nLecture Notes in Computer\\nScience (including subseries\\nLecture Notes in Artiﬁcial\\nIntelligence and Lecture\\nNotes in Bioinformatics) 5101\\nLNCS(PART 1), pp. 466–475\\n5\\n35\\nSCP\\n7\\n2011\\nA review of artiﬁcial intelligence algorithms in\\ndocument classiﬁcation\\nBilski, A.\\nInternational Journal of\\nElectronics and\\nTelecommunications\\n5\\n36\\nSCP\\n18\\n2015\\nAn architecture for agile machine learning in\\nreal-time applications\\nSchleier-Smith, J.\\nProceedings of the ACM\\nSIGKDD International\\nConference on Knowledge\\nDiscovery and Data Mining\\n4\\n37\\nSCP\\n52\\n2018\\nMachine learning in agriculture: A review\\nLiakos, K.G.; Busato, P.; Moshou,\\nD.; Pearson, S.; Bochtis, D.\\nSensors\\n4\\n38\\nSCP\\n22\\n2016\\nApplication of Information Processes\\nApplicative Modelling to Virtual Machines Auto\\nConﬁguration\\nZykov, S.; Shumsky, L.\\nProcedia Computer Science\\n3\\n39\\nSCP\\n34\\n2017\\nGeometry-aware principal component analysis\\nfor symmetric positive deﬁnite matrices\\nHorev, I.; Yger, F.; Sugiyama, M.\\nMachine Learning\\n3\\n40\\nSCP\\n17\\n2015\\nA Fuzzy Least Squares Support Tensor Machines\\nin Machine Learning\\nZhang, R.; Zhou, Z.\\nInternational Journal of\\nEmerging Technologies in\\nLearning\\n2\\n41\\nSCP\\n36\\n2017\\nNuclear energy system’s behavior and decision\\nmaking using machine learning\\nGomez Fernandez, M.; Tokuhiro,\\nA.; Welter, K.; Wu, Q.\\nNuclear Engineering and\\nDesign\\n2\\n42\\nSCP\\n9\\n2013\\nApplication study of machine learning in\\nlightning forecasting\\nQiu, T.; Zhang, S.; Zhou, H.; Bai,\\nX.; Liu, P.\\nInformation Technology\\nJournal\\n1\\n43\\nSCP\\n30\\n2016\\nWOWMON: A machine learning-based proﬁler\\nfor self-adaptive instrumentation of scientiﬁc\\nworkﬂows\\nZhang, X.; Abbasi, H.; Huck, K.;\\nMalony, A.D.\\nProcedia Computer Science\\n1'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 20}, page_content='Sustainability 2020, 12, 492\\n21 of 26\\nID Count\\nResearch\\nSource\\nID\\nDoc\\nYear\\nTitle\\nAuthors\\nSource Title\\nTC\\n44\\nSCP\\n31\\n2017\\nAn event search platform using machine\\nlearning\\nRodrigues, M.A.; Silva, R.R.;\\nBernardino, J.\\nProceedings of the\\nInternational Conference on\\nSoftware Engineering and\\nKnowledge Engineering,\\nSEKE\\n1\\n45\\nSCP\\n32\\n2017\\nAutomated business process management-in\\ntimes of digital transformation using machine\\nlearning or artiﬁcial intelligence\\nPaschek, D.; Luminosu, C.T.;\\nDraghici, A.\\nMATEC Web of Conferences\\n1\\n46\\nSCP\\n42\\n2018\\nApplication of machine learning methods in big\\ndata analytics at management of contracts in the\\nconstruction industry\\nValpeters, M.; Kireev, I.; Ivanov,\\nN.\\nMATEC Web of Conferences\\n1\\n47\\nSCP\\n48\\n2018\\nData mining and machine learning in textile\\nindustry\\nYildirim, P.; Birant, D.; Alpyildiz,\\nT.\\nWiley Interdisciplinary\\nReviews: Data Mining and\\nKnowledge Discovery\\n1\\n48\\nWoS\\n72\\n2018\\nBig Data Analytics, Machine Learning, and\\nArtiﬁcial Intelligence in Next-Generation\\nWireless Networks\\nKibria, M.G.; Kien, N.; Villardi,\\nG.P.; Zhao, O.; Ishizu, K.; Kojima,\\nF.\\nIeee Access\\n1\\n49\\nWoS\\n73\\n2017\\nQuantum neuromorphic hardware for quantum\\nartiﬁcial intelligence\\nPrati, E.\\n8th International Workshop\\nDice2016: Spacetime - Matter -\\nQuantum Mechanics\\n1\\n50\\nWoS\\n74\\n2015\\nExploiting Computational intelligence\\nParadigms in e-Technologies and Activities\\nSaid, H.M.; Salem, A.M.\\nInternational Conference on\\nCommunications,\\nManagement, and\\nInformation Technology\\n(Iccmit’2015)\\n1\\n51\\nWoS\\n75\\n2012\\nSentiment Analysis of Products Using Web\\nUnnamalai, K.\\nInternational Conference on\\nModelling Optimization and\\nComputing\\n1\\n52\\nSCP\\n8\\n2012\\nTaxonomy development and its impact on a\\nself-learning e-recruitment system\\nFaliagka, E.; Karydis, I.; Rigou,\\nM.; ( . . . ); Tsakalidis, A.; Tzimas,\\nG.\\nIFIP Advances in Information\\nand Communication\\nTechnology\\n0'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 21}, page_content='Sustainability 2020, 12, 492\\n22 of 26\\nID Count\\nResearch\\nSource\\nID\\nDoc\\nYear\\nTitle\\nAuthors\\nSource Title\\nTC\\n53\\nSCP\\n13\\n2013\\nResearch on adaptive multi-ﬁltering model of\\nnetwork sensitive information\\nCao, X.F.; Kang, W.; Shi, Q.; Shi,\\nF.F.\\nInformation Technology\\nJournal\\n0\\n54\\nSCP\\n15\\n2014\\nGrade: Machine-learning support for graduate\\nadmissions\\nWaters, A.; Miikkulainen, R.\\nAI Magazine\\n0\\n55\\nSCP\\n27\\n2016\\nLeveraging linked open data information\\nextraction for data mining applications\\nMahule, R.; Vyas, O.P.\\nTurkish Journal of Electrical\\nEngineering and Computer\\nSciences\\n0\\n56\\nSCP\\n38\\n2017\\nRapid prototyping IoT solutions based on\\nMachine Learning\\nRizzo, A.; Montefoschi, F.;\\nCaporali, M.; ( . . . ); Burresi, G.;\\nGiorgi, R.\\nACM International\\nConference Proceeding Series\\n0\\n57\\nSCP\\n39\\n2017\\nTowards automatic learning of heuristics for\\nmechanical transformations of procedural code\\nVigueras, G.; Carro, M.; Tamarit,\\nS.; Mariño, J.\\nElectronic Proceedings in\\nTheoretical Computer Science,\\nEPTCS\\n0\\n58\\nSCP\\n41\\n2018\\nApplication of artiﬁcial intelligence principles in\\nmechanical engineering\\nZajaˇcko, I.; Gál, T.; Ságová, Z.;\\nMateichyk, V.; Wiecek, D.\\nMATEC Web of Conferences\\n0\\n59\\nSCP\\n44\\n2018\\nArtiﬁcial Intelligence in Medical Applications\\nChan, Y.K.; Chen, Y.F.; Pham, T.;\\nChang, W.; Hsieh, M.Y.\\nJournal of Healthcare\\nEngineering\\n0\\n60\\nSCP\\n45\\n2018\\nA semantic internet of things framework using\\nmachine learning approach based on cloud\\ncomputing\\nDing, P.W.; Hsu, I.C.\\nACM International\\nConference Proceeding Series\\n0\\n61\\nSCP\\n46\\n2018\\nA Survey on Machine Learning-Based Mobile\\nBig Data Analysis: Challenges and Applications\\nXie, J.; Song, Z.; Li, Y.; ( . . . );\\nZhang, J.; Guo, J.\\nWireless Communications\\nand Mobile Computing\\n0\\n62\\nSCP\\n47\\n2018\\nBig Data and Machine Learning Based Secure\\nHealthcare Framework\\nKaur, P.; Sharma, M.; Mittal, M.\\nProcedia Computer Science\\n0\\n63\\nSCP\\n49\\n2018\\nDiscovering discontinuity in big ﬁnancial\\ntransaction data\\nTuarob, S.; Strong, R.; Chandra,\\nA.; Tucker, C.S.\\nACM Transactions on\\nManagement Information\\nSystems\\n0'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 22}, page_content='Sustainability 2020, 12, 492\\n23 of 26\\nID Count\\nResearch\\nSource\\nID\\nDoc\\nYear\\nTitle\\nAuthors\\nSource Title\\nTC\\n64\\nSCP\\n50\\n2018\\nIntroducing children to machine learning\\nconcepts through hands-on experience\\nHitron, T.; Erel, H.; Wald, I.;\\nZuckerman, O.\\nIDC 2018 - Proceedings of the\\n2018 ACM Conference on\\nInteraction Design and\\nChildren\\n0\\n65\\nSCP\\n51\\n2018\\nMachine learning for software engineering:\\nModels, methods, and applications\\nMeinke, K.; Bennaceur, A.\\nProceedings - International\\nConference on Software\\nEngineering\\n0\\n66\\nSCP\\n53\\n2018\\nMachine Learning in IT Service Management\\nZuev, D.; Kalistratov, A.; Zuev, A.\\nProcedia Computer Science\\n0\\n67\\nSCP\\n54\\n2018\\nResearch and application of computer control\\nsystem based on complex neural network\\nYang, R.\\nMATEC Web of Conferences\\n0\\n68\\nSCP\\n55\\n2018\\nText classiﬁcation techniques: A literature\\nreview\\nThangaraj, M.; Sivakami, M.\\nInterdisciplinary Journal of\\nInformation, Knowledge, and\\nManagement\\n0\\n69\\nSCP\\n56\\n2019\\nA Machine Learning Method for Predicting\\nDriving Range of Battery Electric Vehicles\\nSun, S.; Zhang, J.; Bi, J.; Wang, Y.;\\nMoghaddam, M.H.Y.\\nJournal of Advanced\\nTransportation\\n0\\n70\\nSCP\\n57\\n2019\\nAn empirical comparison of machine-learning\\nmethods on bank client credit assessments\\nMunkhdalai, L.; Munkhdalai, T.;\\nNamsrai, O.E.; Lee, J.Y.; Ryu, K.H.\\nSustainability\\n0\\n71\\nSCP\\n58\\n2019\\nComparison of multiple linear regression,\\nartiﬁcial neural network, extreme learning\\nmachine, and support vector machine in\\nderiving operation rule of hydropower reservoir\\nNiu, W.J.; Feng, Z.K.; Feng, B.F.; (\\n. . . ); Cheng, C.T.; Zhou, J.Z.\\nWater\\n0\\n72\\nSCP\\n59\\n2019\\nDevelopment and evaluation of a low-cost and\\nsmart technology for precision weed\\nmanagement utilizing artiﬁcial intelligence\\nPartel, V.; Charan Kakarla, S.;\\nAmpatzidis, Y.\\nComputers and Electronics in\\nAgriculture\\n0\\n73\\nSCP\\n60\\n2019\\nIdentifying known and unknown mobile\\napplication traﬃc using a multilevel classiﬁer\\nZhao, S.; Chen, S.; Sun, Y.; ( . . . );\\nSu, J.; Su, C.\\nSecurity and Communication\\nNetworks\\n0'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 23}, page_content='Sustainability 2020, 12, 492\\n24 of 26\\nID Count\\nResearch\\nSource\\nID\\nDoc\\nYear\\nTitle\\nAuthors\\nSource Title\\nTC\\n74\\nSCP\\n61\\n2019\\nOptimized Clustering Algorithms for Large\\nWireless Sensor Networks: A Review\\nWohwe Sambo, D.; Yenke, B.O.;\\nFörster, A.; Dayang, P.\\nSensors\\n0\\n75\\nWoS\\n76\\n2019\\nFPGA-Based Accelerators of Deep Learning\\nNetworks for Learning and Classiﬁcation: A\\nReview\\nShawahna, A.; Sait, S.M.;\\nEl-Maleh, A.\\nIeee Access\\n0\\n76\\nWoS\\n77\\n2018\\nA quantum machine learning algorithm based\\non generative models\\nGao, X.; Zhang, Z.Y.; Duan, L.M.\\nScience Advances\\n0\\n77\\nWoS\\n78\\n2018\\nMachine Learning for Network Automation:\\nOverview, Architecture, and Applications\\nRaﬁque, D.; Velasco, L.\\nJournal of Optical\\nCommunications and\\nNetworking\\n0\\n78\\nWoS\\n79\\n2018\\nA wireless sensor data-based coal mine gas\\nmonitoring algorithm with least squares support\\nvector machines optimized by swarm\\nintelligence techniques\\nChen, P.; Xie, Y.; Jin, P.; Zhang, D.\\nInternational Journal of\\nDistributed Sensor Networks\\n0\\n79\\nWoS\\n80\\n2017\\nNuclear energy system’s behavior and decision\\nmaking using machine learning\\nFernandez, M.G.; Tokuhiro, A.;\\nWelter, K.; Wu, Q.\\nNuclear Engineering and\\nDesign\\n0\\n80\\nWoS\\n81\\n2017\\nAutomated business process management—In\\ntimes of digital transformation using machine\\nlearning or artiﬁcial intelligence\\nPaschek, D.; Luminosu, C.T.;\\nDraghici, A.\\n8th International Conference\\non Manufacturing Science\\nand Education (Mse\\n2017)—Trends in New\\nIndustrial Revolution\\n0\\n81\\nWoS\\n82\\n2017\\nThe Evaluation of Resonance Frequency for\\nPiezoelectric Transducers by Machine Learning\\nMethods\\nChang, F.M.\\n27Th International\\nConference on Flexible\\nAutomation and Intelligent\\nManufacturing, Faim 2017\\n0\\n82\\nWoS\\n83\\n2017\\nFrom Extraction to Generation of Design\\nInformation Paradigm Shift in Data Mining via\\nEvolutionary Learning Classiﬁer System\\nChiba, K.; Nakata, M.\\nInternational Conference on\\nComputational Science (Iccs\\n2017)\\n0'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 24}, page_content='Sustainability 2020, 12, 492\\n25 of 26\\nReferences\\n1.\\nGupta, N.A. Literature Survey on Artiﬁcial Intelligence. 2017. Available online: https://www.ijert.org/\\nresearch/a-literature-survey-on-artiﬁcial-intelligence-IJERTCONV5IS19015.pdf (accessed on 7 January 2020).\\n2.\\nMcCarthy, J.; Minsky, M.L.; Rochester, N.; Shannon, C.E. A Proposal for the Dartmouth Summer Research\\nProject on Artiﬁcial Intelligence. AI Mag. 2006, 27, 12.\\n3.\\nMoore,\\nA.\\nCarnegie\\nMellon\\nDean\\nof\\nComputer\\nScience\\non\\nthe\\nFuture\\nof\\nAI.\\nAvailable\\nonline: https://www.forbes.com/sites/peterhigh/2017/10/30/carnegie-mellon-dean-of-computer-science-on-\\nthe-future-of-ai/#3a283c652197 (accessed on 7 January 2020).\\n4.\\nBecker, A.; Bar-Yehuda, R.; Geiger, D. Randomised algorithms for the loop cutset problem. J. Artif. Intell. Res.\\n2000, 12, 219–234. [CrossRef]\\n5.\\nSinger, J.; Gent, I.P.; Smaill, A. Backbone fragility and the local search cost peak. J. Artif. Intell. Res. 2000,\\n12, 235–270. [CrossRef]\\n6.\\nChen, X.; Van Beek, P. Conﬂict-directed backjumping revisited. J. Artif. Intell. Res. 2001, 14, 53–81. [CrossRef]\\n7.\\nHong, J. Goal recognition through goal graph analysis. J. Artif. Intell. Res. 2001, 15, 1–30. [CrossRef]\\n8.\\nStone, P.; Littman, M.L.; Singh, S.; Kearns, M. ATTAC-2000: An adaptive autonomous bidding agent. J. Artif.\\nIntell. Res. 2000, 15, 189–206. [CrossRef]\\n9.\\nPeng, Y.; Zhang, X. Integrative data mining in systems biology: from text to network mining. Artif. Intell. Med.\\n2007, 41, 83–86. [CrossRef]\\n10.\\nZhou, X.; Liu, B.; Wu, Z.; Feng, Y. Integrative mining of traditional Chines medicine literature and MEDLINE\\nfor functional gene networks. Artif. Intell. Med. 2007, 41, 87–104. [CrossRef]\\n11.\\nWang, S.; Wang, Y.; Du, W.; Sun, F.; Wang, X.; Zhou, C.; Liang, Y. A multi-approaches-guided genetic\\nalgorithm with application to operon prediction. Artif. Intell. Med. 2007, 41, 151–159. [CrossRef]\\n12.\\nHalal, W.E. Artiﬁcial intelligence is almost here.\\nHorizon 2003, 11, 37–38.\\nAvailable online: https://\\nwww.emerald.com/insight/content/doi/10.1108/10748120310486771/full/html (accessed on 7 January 2020).\\n[CrossRef]\\n13.\\nMasnikosa, V.P. The fundamental problem of an artiﬁcial intelligence realization. Kybernetes 1998, 27, 71–80.\\n[CrossRef]\\n14.\\nMetaxiotis, K.; Ergazakis, K.; Samouilidis, E.; Psarras, J. Decision support through knowledge management:\\nThe role of the artiﬁcial intelligence. Inf. Manag. Comput. Secur. 2003, 11, 216–221. [CrossRef]\\n15.\\nRaynor, W.J. The international dictionary of artiﬁcial intelligence. Ref. Rev. 2000, 14, 1–380.\\n16.\\nStefanuk, V.L.; Zhozhikashvili, A.V. Productions and rules in artiﬁcial intelligence.\\nKybernetes 2002,\\n31, ty817–826. [CrossRef]\\n17.\\nTay, D.P.H.; Ho, D.K.H. Artiﬁcial intelligence and the mass appraisal of residential apartments. J. Prop.\\nValuat. Invest. 1992, 10, 525–540. [CrossRef]\\n18.\\nWongpinunwatana, N.; Ferguson, C.; Bowen, P. An experimental investigation of the eﬀects of artiﬁcial\\nintelligence systems on the training of novice auditors. Manag. Audit. J. 2000, 15, 306–318. [CrossRef]\\n19.\\nOke, S.A. A literature review on artiﬁcial intelligence. Int. J. Inf. Manag. Sci. 2008, 19, 535–570.\\n20.\\nCarvalho, T.P.; Soares, F.A.A.M.N.; Vita, R.; da Francisco, P.R.; Basto, J.P.; Alcalá, S.G.S. A systematic literature\\nreview of machine learning methods applied to predictive maintenance. Comput. Ind. Eng. 2019, 1, 1–12.\\n[CrossRef]\\n21.\\nMajorel Deutschland GmbH Artiﬁcial Intelligence and Sustainability. Available online: https://www.future-\\ncustomer.com/artiﬁcial-intelligence-and-sustainability/ (accessed on 8 January 2020).\\n22.\\nMarkham, I.S.; Mathieu, R.G.; Wray, B.A. Kanban setting through artiﬁcial intelligence: A comparative study\\nof artiﬁcial neural networks and decision trees. Integr. Manuf. Syst. 2000, 11, 239–246. [CrossRef]\\n23.\\nKotsiantis, S.B.; Zaharakis, I.; Pintelas, P. Supervised machine learning: A review of classiﬁcation techniques.\\nEmerg. Artif. Intell. Appl. Comput. Eng. 2007, 160, 3–24.\\n24.\\nCortes, C.; Vapnik, V. Support-vector networks. Mach. Learn. 1995, 20, 273–297. [CrossRef]\\n25.\\nKitchenham, B. Procedures for Performing Systematic Reviews. Technical Report TR/SE-0401. 2004. Available\\nonline: https://pdfs.semanticscholar.org/2989/0a936639862f45cb9a987dd599dce9759bf5.pdf?_ga=2.7241591.\\n47522378.1578382825-243572483.1578382825 (accessed on 7 January 2020).\\n26.\\nDuan, Y.; Edwards, J.S.; Dwivedi, Y.K. Artiﬁcial intelligence for decision making in the era of Big\\nData—Evolution, challenges and research agenda. Int. J. Inf. Manag. 2019, 48, 63–71. [CrossRef]'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-01-18T09:04:12+08:00', 'source': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'file_path': '../data/pdf_files/Artificial_Intelligence_and_Machine_Learning_Appli.pdf', 'total_pages': 26, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-01-18T09:04:12+08:00', 'trapped': '', 'modDate': \"D:20200118090412+08'00'\", 'creationDate': \"D:20200118090412+08'00'\", 'page': 25}, page_content='Sustainability 2020, 12, 492\\n26 of 26\\n27.\\nDe Felice, F.; Petrillo, A.; Zomparelli, F. Prospective design of smart manufacturing: An Italian pilot case\\nstudy. Manuf. Lett. 2018, 15, 81–85. [CrossRef]\\n28.\\nLarrañaga, P.; Calvo, B.; Santana, R.; Bielza, C.; Galdiano, J.; Inza, I.; Lozano, J.A.; Armañanzas, R.; Santafé, G.;\\nPérez, A.; et al. Machine Learning. in Bioinformatics. Brief. Bioinform. 2006, 7, 86–112. [CrossRef] [PubMed]\\n29.\\nKrawczyk, B. Learning from imbalanced data: Open challenges and future directions. Prog. Artif. Intell.\\n2016, 5, 221–232. [CrossRef]\\n30.\\nWuest, T.; Weimer, D.; Irgens, C.; Thoben, K.D. Machine learning in manufacturing: Advantages, challenges,\\nand applications. Prod. Manuf. Res. 2016, 4, 23–45. [CrossRef]\\n31.\\nDutton, T. An Overview of National AI Strategies. Available online: http://www.jaist.ac.jp/~{}bao/AI/\\nOtherAIstrategies/An%20Overview%20of%20National%20AI%20Strategies%20%E2%80%93%20Politics%\\n20+%20AI%20%E2%80%93%20Medium.pdf (accessed on 8 January 2020).\\n32.\\nPérez-Ortiz, M.; Jiménez-Fernández, S.; Gutiérrez, P.A.; Alexandre, E.; Hervás-Martínez, C.; Salcedo-Sanz, S.\\nA Review of Classiﬁcation Problems and Algorithms in Renewable Energy Applications. Energies 2016,\\n9, 607. [CrossRef]\\n33.\\nLieber, D.; Stolpe, M.; Konrad, B.; Deuse, J.; Morik, K. Quality prediction in interlinked manufacturing\\nprocesses based on supervised & unsupervised machine learning. Procedia CIRP 2013, 7, 193–198.\\n34.\\nSachs, J.D.;\\nSchmidt-Traub, G.;\\nMazzucato, M.;\\nMessner, D.;\\nNakicenovic, N.;\\nRockström, J.\\nSix Transformations to Achieve the Sustainable Development Goals. Nat. Sustain.\\n2019, 2, 805–814.\\n[CrossRef]\\n© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access\\narticle distributed under the terms and conditions of the Creative Commons Attribution\\n(CC BY) license (http://creativecommons.org/licenses/by/4.0/).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/dsa.pdf', 'file_path': '../data/pdf_files/dsa.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'dsa', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Time complexity measures how the performance of an algorithm changes with input size (n). Big-O notation \\nexpresses the worst-case complexity. For example, O(1) means constant time; O(n) means linear time; O(n²) \\nrepresents quadratic time; O(log n) is logarithmic; and O(n log n) represents efficient divide-and-conquer \\nalgorithms like merge sort. \\nSearching algorithms determine how to locate a specific value in a dataset. The simplest is linear search \\n(O(n)), which checks each element sequentially. It works on unsorted data but is slow for large inputs. Binary \\nsearch (O(log n)) works on sorted arrays by repeatedly dividing the search space in half, dramatically \\nimproving efficiency. \\nSorting algorithms arrange elements in ascending or descending order: \\n●\\u200b Bubble Sort: Repeatedly swaps adjacent elements—simple but slow (O(n²)).\\u200b\\n \\n●\\u200b Selection Sort: Selects the smallest element in each iteration—also O(n²).\\u200b\\n \\n●\\u200b Insertion Sort: Efficient for small or nearly sorted arrays (O(n²) worst case).\\u200b\\n \\n●\\u200b Merge Sort: A divide-and-conquer algorithm that splits, sorts, and merges; stable and fast with O(n log \\nn) time.\\u200b\\n \\n●\\u200b Quick Sort: Uses a pivot to partition the array; average O(n log n) but worst O(n²). One of the fastest \\npractical algorithms.\\u200b\\n \\n●\\u200b Heap Sort: Builds a heap and extracts the maximum/minimum repeatedly; guarantees O(n log n).\\u200b\\n \\nUnderstanding time complexity helps you choose the right algorithm for large-scale problems. Sorting and \\nsearching form the foundation of most real-world applications, from databases to competitive programming.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m139', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36', 'creationdate': '2025-08-30T19:12:18+00:00', 'source': '../data/pdf_files/Shreya_Sharma_Resume.pdf', 'file_path': '../data/pdf_files/Shreya_Sharma_Resume.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'shreya_sharma_resume.docx - Google Docs', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-30T19:12:18+00:00', 'trapped': '', 'modDate': \"D:20250830191218+00'00'\", 'creationDate': \"D:20250830191218+00'00'\", 'page': 0}, page_content='\\u202dSHREYA SHARMA\\u202c\\n\\u202dB.Tech (Computer Science and Engineering\\u202c\\n\\u202dwith Specialization in Cyber Security)\\u202c\\n\\u202dEmail:\\u202c\\u202dshreya.sharma110404@gmail.com\\u202c\\u202d|\\u202c\\u202dPhone:\\u202c\\u202d9770766139\\u202c\\n\\u202dLinkedIn:\\u202c\\u202dhttps://www.linkedin.com/in/shreya-sharma1104/\\u202c\\n\\u202dGitHub:\\u202c\\u202dhttps://github.com/shreyasharma-1\\u202c\\n\\u202dACADEMICS\\u202c\\n\\u202dQualification\\u202c\\n\\u202dInstitute\\u202c\\n\\u202dBoard / University\\u202c\\n\\u202d% / CGPA\\u202c\\n\\u202dYear\\u202c\\n\\u202dB.Tech (CY - 6th sem)\\u202c\\n\\u202dXII\\u202c\\n\\u202dLakshmi Narain College of\\u202c\\n\\u202dTechnology & Science\\u202c\\n\\u202dMaharishi Vidya Mandir, Jabalpur\\u202c\\n\\u202dRGPV\\u202c\\n\\u202dCBSE\\u202c\\n\\u202d8.34/10\\u202c\\n\\u202d73%\\u202c\\n\\u202d2026\\u202c\\n\\u202d2022\\u202c\\n\\u202dX\\u202c\\n\\u202dMaharishi Vidya Mandir, Jabalpur\\u202c\\n\\u202dCBSE\\u202c\\n\\u202d70.8%\\u202c\\n\\u202d2020\\u202c\\n\\u202dCertifications\\u202c\\n\\u202d/Publications\\u202c\\n\\u202d●\\u202c\\n\\u202dComprehensive Study Of MD5 and SHA-256\\u202c\\n\\u202d●\\u202c\\n\\u202dIntroduction to Cybersecurity offered through Cisco Networking Academy\\u202c\\n\\u202d●\\u202c\\n\\u202dThe Complete Python Developer Certification Course offered by Udemy\\u202c\\n\\u202d●\\u202c\\n\\u202dDatabase Management System Part - 1 offered by Infosys Springboard\\u202c\\n\\u202d●\\u202c\\n\\u202dPython (Basic) offered by Hacker-Rank\\u202c\\n\\u202d2024\\u202c\\n\\u202d2024\\u202c\\n\\u202d2024\\u202c\\n\\u202d2024\\u202c\\n\\u202d2023\\u202c\\n\\u202dAchievements\\u202c\\n\\u202d●\\u202c\\n\\u202dBest\\u202c \\u202dPaper\\u202c \\u202dAward\\u202c \\u202d(2nd\\u202c \\u202dPosition)\\u202c \\u202dfor\\u202c \\u202dthe\\u202c\\u202d\"Comprehensive\\u202c\\u202dStudy\\u202c\\u202dof\\u202c\\u202dMD5\\u202c\\u202dand\\u202c\\u202dSHA-256\"\\u202c\\n\\u202dResearch Paper at ICEHAIDS.\\u202c\\n\\u202d2024\\u202c\\n\\u202dPROJECTS\\u202c\\n\\u202dMulti-Agent\\u202c\\n\\u202dTelegram Bot\\u202c\\n\\u202d●\\u202c\\n\\u202dTech Stack – Python, FastAPI, MongoDB, OpenAI Whisper, Google Nearby Search API\\u202c\\n\\u202d●\\u202c\\n\\u202dDeveloped a Multi-Agent Telegram Bot with intelligent query routing for dynamic decision-making\\u202c\\n\\u202d●\\u202c\\n\\u202dIntegrated\\u202c \\u202dreal-time\\u202c \\u202dservices\\u202c \\u202dincluding\\u202c \\u202dweather\\u202c \\u202dupdates,\\u202c \\u202dstock\\u202c \\u202dprice\\u202c \\u202dretrieval,\\u202c \\u202dnews\\u202c \\u202daggregation,\\u202c\\n\\u202dimage generation, meme creation, and voice-to-text interaction using OpenAI Whisper.\\u202c\\n\\u202d●\\u202c\\n\\u202dImplemented\\u202c \\u202dlocation-based\\u202c \\u202dfeatures\\u202c \\u202dwith\\u202c \\u202dGoogle\\u202c \\u202dNearby\\u202c \\u202dSearch\\u202c \\u202dAPI\\u202c \\u202dand\\u202c \\u202densured\\u202c \\u202dscalable\\u202c\\n\\u202darchitecture with secure chat history storage in MongoDB..\\u202c\\n\\u202dFace Tracer -\\u202c\\n\\u202dIntelligent Presence\\u202c\\n\\u202dDetection\\u202c\\n\\u202d●\\u202c\\n\\u202dTech Stack\\u202c\\u202d– Python, OpenCV, face_recognition, Flask, HTML/CSS, SQLite, NumPy, Pandas\\u202c\\n\\u202d●\\u202c\\n\\u202dDeveloped\\u202c\\u202da\\u202c\\u202dreal-time\\u202c\\u202dattendance\\u202c\\u202dmanagement\\u202c\\u202dweb\\u202c\\u202dapplication\\u202c\\u202dusing\\u202c\\u202dfacial\\u202c\\u202drecognition.\\u202c\\u202dIntegrated\\u202c\\n\\u202dwebcam-based face detection with automatic attendance logging and CSV/SQLite storage.\\u202c\\n\\u202d●\\u202c\\n\\u202dBuilt\\u202c \\u202da\\u202c \\u202dresponsive\\u202c \\u202dfrontend\\u202c \\u202dusing\\u202c \\u202dFlask\\u202c \\u202dand\\u202c \\u202dHTML\\u202c \\u202dwith\\u202c \\u202doptions\\u202c \\u202dfor\\u202c \\u202dregistration,\\u202c \\u202dtraining,\\u202c \\u202dand\\u202c\\n\\u202dviewing attendance logs.\\u202c\\n\\u202dHouse Price\\u202c\\n\\u202dPrediction – Data\\u202c\\n\\u202danalysis\\u202c\\n\\u202d●\\u202c\\n\\u202dTech Stack\\u202c\\u202d- Python, Pandas, Numpy, Matplotlib, Seaborn\\u202c\\n\\u202d●\\u202c\\n\\u202dHouse\\u202c \\u202dPrice\\u202c \\u202dPrediction\\u202c \\u202duses\\u202c \\u202dmachine\\u202c \\u202dlearning\\u202c \\u202dtechniques\\u202c \\u202dto\\u202c \\u202dpredict\\u202c \\u202dhousing\\u202c \\u202dprices,\\u202c \\u202dshowcasing\\u202c\\n\\u202danalytical and problem-solving skills.\\u202c\\n\\u202dIris Flower\\u202c\\n\\u202dclassification\\u202c\\n\\u202d●\\u202c\\n\\u202dTech Stack -\\u202c\\u202dPython, Pandas, Numpy, Matplotlib, Seaborn, filter warnings\\u202c\\n\\u202d●\\u202c\\n\\u202dIris\\u202c\\u202dFlower\\u202c\\u202dClassification\\u202c\\u202dusing\\u202c\\u202dApplied\\u202c\\u202dlinear\\u202c\\u202dregression\\u202c\\u202dfor\\u202c\\u202daccurate\\u202c\\u202dclassification\\u202c\\u202dof\\u202c\\u202dIris\\u202c\\u202dflowers,\\u202c\\n\\u202ddemonstrating a solid understanding of statistical modeling\\u202c\\n\\u202d√√√\\u202c\\n\\u202dSKILLS\\u202c\\n\\u202dProgramming\\u202c\\n\\u202dJava, Python, Pandas, Numpy, Matplotlib, Machine Learning (Beginner)\\u202c\\n\\u202dDatabases\\u202c\\n\\u202dSQL\\u202c\\n\\u202dAnalytics\\u202c\\n\\u202dPower BI, Matplotlib, Seaborn\\u202c\\n\\u202dTools\\u202c\\n\\u202dJupyter Notebook, Cursor, VS Code, Canva, MS Word, MS Excel, MS PowerPoint\\u202c\\n\\u202dSoft Skills\\u202c\\n\\u202dLeadership, Communication, Problem Solving, Analytical Skills, Learning Agility\\u202c\\n\\u202dPOSITIONS OF RESPONSIBILITY\\u202c\\n\\u202dLNCTS BHOPAL\\u202c\\n\\u202d●\\u202c\\n\\u202dVolunteer\\u202c \\u202dat\\u202c \\u202dthe\\u202c \\u202dInternational\\u202c \\u202dConference\\u202c \\u202don\\u202c \\u202dExpanding\\u202c \\u202dHorizons\\u202c \\u202din\\u202c \\u202dArtificial\\u202c\\n\\u202dIntelligence & Data Science\\u202c\\n\\u202d●\\u202c\\n\\u202dResearch Paper Presenter at ICEHAIDS\\u202c\\n\\u202dMVM, Jabalpur\\u202c\\n\\u202d●\\u202c\\n\\u202dVice Captain, Assembly Committee\\u202c\\n\\u202d●\\u202c\\n\\u202dMember, Hygiene Committee\\u202c\\n\\u202dCO-CURRICULAR & EXTRACURRICULAR ACTIVITIES\\u202c\\n\\u202dLakshmi Narain College of Technology & Science | BATCH OF 2026\\u202c\\n\\u202dTechnical\\u202c\\n\\u202d●\\u202c\\n\\u202dChairperson (Innovation Vertical), Young Indians CII-YUVA\\u202c\\n\\u202d●\\u202c\\n\\u202dCompetitive Programming – LeetCode, Hacker-Rank\\u202c'),\n",
       " Document(metadata={'producer': 'Skia/PDF m139', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36', 'creationdate': '2025-08-30T18:04:57+00:00', 'source': '../data/pdf_files/Shreya_Sharma_Transcript.pdf', 'file_path': '../data/pdf_files/Shreya_Sharma_Transcript.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': '\".:: Rajiv Gandhi Proudyogiki Vishwavidyalaya, Bhopal ::.\"', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-30T18:04:57+00:00', 'trapped': '', 'modDate': \"D:20250830180457+00'00'\", 'creationDate': \"D:20250830180457+00'00'\", 'page': 0}, page_content='Rajiv Gandhi Proudyogiki\\nVishwavidyalaya, Bhopal\\nStatement of Marks\\xa0- June-2025\\nName\\nSHREYA SHARMA\\nRoll No.\\n0157CY221129\\nCourse\\nB.Tech\\nBranch\\nCY\\nSemester\\n6\\nStatus\\nRegular\\nSubject\\nTotal Credit\\nEarned Credit\\nGrade\\nCY601- [T]\\n3\\n3\\nA\\nCY602- [T]\\n3\\n3\\nB+\\nCY603- [T]\\n4\\n4\\nA+\\nCY604- [T]\\n4\\n4\\nA\\nCY601- [P]\\n1\\n1\\nA+\\nCY602- [P]\\n1\\n1\\nA+\\nCY605- [P]\\n3\\n3\\nA+\\nCY606- [P]\\n3\\n3\\nA+\\nCY608- [P]\\n2\\n2\\nA+\\nResult Des.\\nSGPA\\nCGPA\\nPASS\\n9.46\\n8.34\\nRevaluation Date\\nRevaluation Date with\\nLate Fee\\n12/08/2025\\n14/08/2025\\nData Source : Rajiv Gandhi Proudyogiki Vishwavidyalaya, Bhopal\\nDisclaimer : The data belongs to RGPV,Bhopal. For any communication related to the\\npublished data, please contact examination cell of RGPV or respective College.\\nPrint Marksheet\\n30/08/2025, 23:34\\n\".:: Rajiv Gandhi Proudyogiki Vishwavidyalaya, Bhopal ::.\"\\nabout:blank\\n1/1'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/dbms.pdf', 'file_path': '../data/pdf_files/dbms.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'dbms', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Database Management Systems (DBMS) provide structured storage, retrieval, and management of data. One \\nessential concept in relational databases is normalization, which aims to reduce redundancy and ensure data \\nintegrity. Normalization involves decomposing tables into well-structured forms called normal forms. \\n1NF (First Normal Form) requires that table cells contain atomic values and that each record is unique. 2NF \\nremoves partial dependency, meaning no non-key attribute should depend on only part of a composite primary \\nkey. 3NF removes transitive dependency, ensuring non-key attributes depend only on primary keys. Proper \\nnormalization prevents anomalies such as update, insertion, and deletion anomalies. \\nIn DBMS, relationships between tables are established through joins: \\n●\\u200b INNER JOIN: Returns only matching rows from both tables.\\u200b\\n \\n●\\u200b LEFT JOIN: Returns all rows from the left table and matching rows from the right.\\u200b\\n \\n●\\u200b RIGHT JOIN: Opposite of left join.\\u200b\\n \\n●\\u200b FULL OUTER JOIN: Returns all rows when there is a match in either table.\\u200b\\n \\n●\\u200b CROSS JOIN: Produces a Cartesian product.\\u200b\\n \\nJoins enable relational databases to maintain normalized structures while still retrieving meaningful combined \\ndata. \\nAnother key concept is transactions, which represent a sequence of operations performed as a single logical \\nunit of work. Transactions must satisfy the ACID properties: \\n●\\u200b Atomicity: All steps succeed or none.\\u200b\\n \\n●\\u200b Consistency: The database must remain valid before and after the transaction.\\u200b\\n \\n●\\u200b Isolation: Concurrent transactions must not interfere with each other.\\u200b\\n \\n●\\u200b Durability: Changes persist even if the system crashes.\\u200b\\n \\nDatabase systems use locking, logging, checkpoints, and isolation levels to ensure transaction safety. \\nUnderstanding normalization, joins, and transactions is critical because these concepts form the backbone of \\nrelational data handling.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/ml.pdf', 'file_path': '../data/pdf_files/ml.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'ml', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Machine Learning (ML) is a subfield of Artificial Intelligence that enables systems to learn patterns from data \\nand improve over time without explicit programming. ML focuses on building models that can analyze data, \\nmake predictions, classify outcomes, and find hidden structures. It has revolutionized multiple industries \\nthrough automation and intelligent decision-making. \\nML is broadly divided into three major types: \\n1. Supervised Learning \\nModels learn using labeled data—each input has a correct output. Algorithms try to generalize these \\nrelationships to predict outcomes on new data.\\u200b\\n Common algorithms include Linear Regression, Logistic Regression, Decision Trees, Random Forests, and \\nSupport Vector Machines.\\u200b\\n Applications: Spam detection, credit scoring, medical diagnosis, stock price prediction. \\n2. Unsupervised Learning \\nUsed when data has no labels. The model identifies hidden patterns, clusters, or structures.\\u200b\\n Popular methods include K-means clustering, PCA, and Apriori for association mining.\\u200b\\n Applications: Customer segmentation, anomaly detection, dimensionality reduction. \\n3. Reinforcement Learning \\nThe model interacts with an environment and learns from rewards and penalties. It is used in robotics, \\ngaming (e.g., AlphaGo), autonomous vehicles, and resource optimization. \\nOther important ML concepts include: \\n●\\u200b Feature Engineering: Selecting important variables to improve model accuracy.\\u200b\\n \\n●\\u200b Model Evaluation: Using metrics like accuracy, precision, recall, F1-score.\\u200b\\n \\n●\\u200b Overfitting & Underfitting: Overfitting happens when the model memorizes training data; underfitting \\noccurs when it fails to learn enough patterns.\\u200b\\n \\n●\\u200b Training vs Testing: Models are trained on historical data and tested on unseen data to validate \\nperformance.\\u200b\\n \\nML applications are everywhere—image recognition, natural language processing, fraud detection, \\nrecommendation systems, autonomous driving, and healthcare analytics. As data grows, machine learning \\ncontinues to shape how modern systems make intelligent decisions.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/os.pdf', 'file_path': '../data/pdf_files/os.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'os', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='An Operating System (OS) is responsible for managing hardware, providing an environment for applications, \\nand ensuring efficient utilization of system resources. Two of the most fundamental concepts in OS are \\nprocesses and threads. A process is an independent program in execution. It has its own memory space \\n(code, data, stack, heap) and is represented in the system by a Process Control Block (PCB). Every process \\noperates in isolation, which ensures stability but also increases overhead because switching between \\nprocesses is expensive—each context switch requires saving and loading a separate memory space. \\nA thread, on the other hand, is a smaller execution unit inside a process. Multiple threads within the same \\nprocess share memory such as global variables and heap, but each thread has its own stack and program \\ncounter. This shared memory makes threads lightweight and suitable for parallelism within the same \\napplication. For example, a browser may have separate threads for rendering, downloading, and handling user \\ninteractions. \\nOS uses CPU scheduling to determine which process or thread gets processor time. Scheduling becomes \\nnecessary because the CPU can execute only one instruction flow at a time (ignoring multi-core scenarios). \\nThe goal is to maximize CPU utilization, throughput, and responsiveness. \\nCommon scheduling algorithms include: \\n1.\\u200b First-Come, First-Served (FCFS):\\u200b\\n The simplest algorithm where the first process to arrive is the first to execute. It suffers from the \\n\"convoy effect,\" where a long job delays all others.\\u200b\\n \\n2.\\u200b Shortest Job First (SJF):\\u200b\\n Prioritizes processes with the smallest execution time. It reduces waiting time but requires predicting \\nprocess burst time.\\u200b\\n \\n3.\\u200b Round Robin (RR):\\u200b\\n Each process gets a fixed time slice (quantum). It is ideal for time-sharing systems because no \\nprocess can monopolize the CPU.\\u200b\\n \\n4.\\u200b Priority Scheduling:\\u200b\\n Each process is assigned a priority. The CPU selects the highest-priority process. It may lead to \\nstarvation if low-priority processes never execute.\\u200b\\n \\n5.\\u200b Multilevel Queue Scheduling:\\u200b\\n Processes are divided into multiple queues (e.g., system, interactive, batch), each with different \\nscheduling rules.\\u200b\\n \\nUnderstanding processes, threads, and scheduling is crucial because they determine how efficiently an \\napplication and system perform. Multi-threading improves concurrency, while effective scheduling ensures \\nfairness and maximizes CPU usage.'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 0}, page_content='Natural Language Processing: State of The Art, Current Trends and \\nChallenges \\nDiksha Khurana1, Aditya Koli1, Kiran Khatter1,2\\n and Sukhdev Singh1,2\\n \\n1Department of Computer Science and Engineering \\nManav Rachna International University, Faridabad-121004, India \\n2Accendere Knowledge Management Services Pvt. Ltd., India \\n \\nAbstract  \\nNatural language processing (NLP) has recently gained much attention for representing and \\nanalysing human language computationally. It has spread its applications in various fields \\nsuch as machine translation, email spam detection, information extraction, summarization, \\nmedical, and question answering etc. The paper distinguishes four phases by discussing \\ndifferent levels of NLP and components of Natural Language Generation (NLG) followed by \\npresenting the history and evolution of NLP, state of the art presenting the various \\napplications of NLP and current trends and challenges.  \\n \\n1. Introduction \\nNatural Language Processing (NLP) is a tract of Artificial Intelligence and Linguistics, \\ndevoted to make computers understand the statements or words written in human languages. \\nNatural language processing came into existence to ease the user’s work and to satisfy the \\nwish to communicate with the computer in natural language. Since all the users may not be \\nwell-versed in machine specific language, NLP caters those users who do not have enough \\ntime to learn new languages or get perfection in it.  \\nA language can be defined as a set of rules or set of symbol. Symbol are combined and used \\nfor conveying information or broadcasting the information. Symbols are tyrannized by the \\nRules. Natural Language Processing basically can be classified into two parts i.e. Natural \\nLanguage Understanding and Natural Language Generation which evolves the task to \\nunderstand and generate the text (Figure 1).'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 1}, page_content='Figure 1. Broad Classification of NLP \\nLinguistics is the science of language which includes Phonology that refers to sound, \\nMorphology word formation, Syntax sentence structure, Semantics syntax and Pragmatics \\nwhich refers to understanding. \\nNoah Chomsky, one of the first linguists of twelfth century that started syntactic theories, \\nmarked a unique position in the field of theoretical linguistics because he revolutionised the \\narea of syntax (Chomsky, 1965) [1]. Which can be broadly categorized into two levels Higher \\nLevel which include speech recognition and Lower Level which corresponds to natural \\nlanguage. Few of the researched tasks of NLP are Automatic Summarization, Co-Reference \\nResolution, Discourse Analysis, Machine Translation, Morphological Segmentation, Named \\nEntity Recognition, Optical Character Recognition, Part Of Speech Tagging etc. Some of \\nthese tasks have direct real world applications such as Machine translation, Named entity \\nrecognition, Optical character recognition etc. Automatic summarization produces an \\nunderstandable summary of a set of text and provides summaries or detailed information of \\ntext of a known type. Co-reference resolution it refers to a sentence or larger set of text that \\ndetermines which word refer to the same object. Discourse analysis refers to the task of \\nidentifying the discourse structure of connected text. Machine translation which refers to \\nautomatic translation of text from one human language to another. Morphological \\nsegmentation which refers to separate word into individual morphemes and identify the class \\nof the morphemes. Named entity recognition (NER) it describes a stream of text, determine \\nwhich items in the text relates to proper names. Optical character recognition (OCR) it gives \\nan image representing printed text, which help in determining the corresponding or related \\ntext. Part of speech tagging it describes a sentence, determines the part of speech for each \\nword. Though NLP tasks are obviously very closely interweaved but they are used \\nfrequently, for convenience. Some of the task such as automatic summarisation, co-reference \\nanalysis etc. act as subtask that are used in solving larger tasks.'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 2}, page_content='The goal of Natural Language Processing is to accommodate one or more specialities of an \\nalgorithm or system. The metric of NLP assess on an algorithmic system allows for the \\nintegration of language understanding and language generation. It is even used in \\nmultilingual event detection Rospocher et al. [2] purposed a novel modular system for cross-\\nlingual event extraction for English, Dutch and Italian texts by using different pipelines for \\ndifferent languages. The system incorporates a modular set of foremost multilingual Natural \\nLanguage Processing (NLP) tools. The pipeline integrates modules for basic NLP processing \\nas well as more advanced tasks such as cross-lingual named entity linking, semantic role \\nlabelling and time normalization. Thus, the cross-lingual framework allows for the \\ninterpretation of events, participants, locations and time, as well as the relations between \\nthem. Output of these individual pipelines is intended to be used as input for a system that \\nobtains event centric knowledge graphs. All modules behave like UNIX pipes: they all take \\nstandard input, to do some annotation, and produce standard output which in turn is the input \\nfor the next module pipelines are built as a data centric architecture so that modules can be \\nadapted and replaced. Furthermore, modular architecture allows for different configurations \\nand for dynamic distribution. \\nMost of the work in Natural Language Processing is conducted by computer scientists while \\nvarious other professionals have also shown interest such as linguistics, psychologist and \\nphilosophers etc. One of the most ironical aspect of NLP is that it adds up to the knowledge \\nof human language. The field of Natural Language Processing is related with different \\ntheories and techniques that deal with the problem of natural language of communicating \\nwith the computers. Ambiguity is one of the major problem of natural language which is \\nusually faced in syntactic level which has subtask as lexical and morphology which are \\nconcerned with the study of words and word formation. Each of these levels can produce \\nambiguities that can be solved by the knowledge of the complete sentence. The ambiguity \\ncan be solved by various methods such as Minimising Ambiguity, Preserving Ambiguity, \\nInteractive Disambiguity and Weighting Ambiguity [3]. Some of the methods proposed by \\nresearchers to remove ambiguity is preserving ambiguity, e.g. (Shemtov 1997; Emele & \\nDorna 1998; Knight & Langkilde 2000) [3][4][5] Their objectives are closely in line with the \\nlast of these: they cover a wide range of ambiguities and there is a statistical element implicit \\nin their approach.  \\n2. Levels of NLP \\nThe ‘levels of language’ are one of the most explanatory method for representing the Natural \\nLanguage processing which helps to generate the NLP text by realising Content Planning, \\nSentence Planning and Surface Realization phases (Figure 2).'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 3}, page_content='Figure 2. Phases of NLP architecture \\nLinguistic is the science which involves meaning of language, language context and various \\nforms of the language. The various important terminologies of Natural Language Processing \\nare: - \\n1. Phonology \\nPhonology is the part of Linguistics which refers to the systematic arrangement of sound. The \\nterm phonology comes from Ancient Greek and the term phono- which means voice or \\nsound, and the suffix –logy refers to word or speech. In 1993 Nikolai Trubetzkoy stated that \\nPhonology is “the study of sound pertaining to the system of language\". Whereas Lass in \\n1998 wrote that phonology refers broadly with the sounds of language, concerned with the to \\nlathe sub discipline of linguistics, whereas it could be explained as, \"phonology proper is \\nconcerned with the function, behaviour and organization of sounds as linguistic items. \\nPhonology include semantic use of sound to encode meaning of any Human language.  \\n(Clark et al.,2007) [6]. \\n2. Morphology \\nThe different parts of the word represent the smallest units of meaning known as Morphemes. \\nMorphology which comprise of Nature of words, are initiated by morphemes. An example of \\nMorpheme could be, the word precancellation can be morphologically scrutinized into three \\nseparate morphemes: the prefix pre, the root cancella, and the suffix -tion. The interpretation \\nof morpheme stays same across all the words, just to understand the meaning humans can \\nbreak any unknown word into morphemes. For example, adding the suffix –ed to a verb, \\nconveys that the action of the verb took place in the past. The words that cannot be divided \\nand have meaning by themselves are called Lexical morpheme (e.g.: table, chair) The words \\n(e.g. -ed, -ing, -est, -ly, -ful) that are combined with the lexical morpheme are known as \\nGrammatical morphemes (eg. Worked, Consulting, Smallest, Likely, Use). Those \\ngrammatical morphemes that occurs in combination called bound morphemes( eg. -ed, -ing) \\nGrammatical morphemes can be divided into bound morphemes and derivational morphemes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 4}, page_content='3. Lexical \\nIn Lexical, humans, as well as NLP systems, interpret the meaning of individual words. \\nSundry types of processing bestow to word-level understanding – the first of these being a \\npart-of-speech tag to each word. In this processing, words that can act as more than one part-\\nof-speech are assigned the most probable part-of speech tag based on the context in which \\nthey occur. At the lexical level, Semantic representations can be replaced by the words that \\nhave one meaning. In NLP system, the nature of the representation varies according to the \\nsemantic theory deployed. \\n4. Syntactic \\nThis level emphasis to scrutinize the words in a sentence so as to uncover the grammatical \\nstructure of the sentence. Both grammar and parser are required in this level. The output of \\nthis level of processing is representation of the sentence that divulge the structural \\ndependency relationships between the words. There are various grammars that can be \\nimpeded, and which in twirl, whack the option of a parser. Not all NLP applications require a \\nfull parse of sentences, therefore the abide challenges in parsing of prepositional phrase \\nattachment and conjunction audit no longer impede that plea for which phrasal and clausal \\ndependencies are adequate [7]. Syntax conveys meaning in most languages because order and \\ndependency contribute to connotation. For example, the two sentences: ‘The cat chased the \\nmouse.’ and ‘The mouse chased the cat.’ differ only in terms of syntax, yet convey quite \\ndifferent meanings. \\n5. Semantic \\nIn semantic most people think that meaning is determined, however, this is not it is all the \\nlevels that bestow to meaning. Semantic processing determines the possible meanings of a \\nsentence by pivoting on the interactions among word-level meanings in the sentence. This \\nlevel of processing can incorporate the semantic disambiguation of words with multiple \\nsenses; in a cognate way to how syntactic disambiguation of words that can errand as \\nmultiple parts-of-speech is adroit at the syntactic level. For example, amongst other \\nmeanings, ‘file’ as a noun can mean either a binder for gathering papers, or a tool to form \\none’s fingernails, or a line of individuals in a queue (Elizabeth D. Liddy,2001) [7]. The \\nsemantic level scrutinizes words for their dictionary elucidation, but also for the elucidation \\nthey derive from the milieu of the sentence. Semantics milieu that most words have more \\nthan one elucidation but that we can spot the appropriate one by looking at the rest of the \\nsentence. [8] \\n6. Discourse \\nWhile syntax and semantics travail with sentence-length units, the discourse level of NLP \\ntravail with units of text longer than a sentence i.e, it does not interpret multi sentence texts as \\njust sequence sentences, apiece of which can be elucidated singly. Rather, discourse focuses \\non the properties of the text as a whole that convey meaning by making connections between \\ncomponent sentences (Elizabeth D. Liddy,2001) [7]. The two of the most common levels are \\nAnaphora Resolution - Anaphora resolution is the replacing of words such as pronouns, \\nwhich are semantically stranded, with the pertinent entity to which they refer. Discourse/Text \\nStructure Recognition - Discourse/text structure recognition sway the functions of sentences \\nin the text, which, in turn, adds to the meaningful representation of the text. \\n7. Pragmatic:'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 5}, page_content='Pragmatic is concerned with the firm use of language in situations and utilizes nub over and \\nabove the nub of the text for understanding the goal and to explain how extra meaning is read \\ninto texts without literally being encoded in them. This requisite much world knowledge, \\nincluding the understanding of intentions, plans, and goals. For example, the following two \\nsentences need aspiration of the anaphoric term ‘they’, but this aspiration requires pragmatic \\nor world knowledge (Elizabeth D. Liddy,2001) [7]. \\n3. Natural Language Generation \\nNatural Language Generation (NLG) is the process of producing phrases, sentences and \\nparagraphs that are meaningful from an internal representation. It is a part of Natural \\nLanguage Processing and happens in four phases: identifying the goals, planning on how \\ngoals maybe achieved by evaluating the situation and available communicative sources and \\nrealizing the plans as a text [Figure 3]. It is opposite to Understanding. \\n \\n                                      Figure 3. Components of NLG \\nComponents of NLG are as follows: \\nSpeaker and Generator – To generate a text we need to have a speaker or an application \\nand a generator or a program that renders the application’s intentions into fluent phrase \\nrelevant to the situation.  \\nComponents and Levels of Representation -The process of language generation involves \\nthe following interweaved tasks. Content selection: Information should be selected and \\nincluded in the set. Depending on how this information is parsed into representational units, \\nparts of the units may have to be removed while some others may be added by default. \\nTextual Organization: The information must be textually organized according the grammar, it \\nmust be ordered both sequentially and in terms of linguistic relations like modifications. \\nLinguistic Resources: To support the information’s realization, linguistic resources must be'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 6}, page_content='chosen. In the end these resources will come down to choices of particular words, idioms, \\nsyntactic constructs etc. Realization: The selected and organized resources must be realized \\nas an actual text or voice output.  \\nApplication or Speaker –  This is only for maintaining the model of the situation. Here the \\nspeaker just initiates the process doesn’t take part in the language generation. It stores the \\nhistory, structures the content that is potentially relevant and deploys a representation of what \\nit actually knows. All these form the situation, while selecting subset of propositions that \\nspeaker has. The only requirement is the speaker has to make sense of the situation. [9] \\n \\n4. History of NLP \\nIn late 1940s the term wasn’t even in existence, but the work regarding machine translation \\n(MT) had started. Research in this period was not completely localised. Russian and English \\nwere the dominant languages for MT, but others, like Chinese were used for MT (Booth \\n,1967) [10]. MT/NLP research was almost died in 1966 according to ALPAC report, which \\nconcluded that MT is going nowhere. But later on some MT production systems were \\nproviding output to their customers (Hutchins, 1986) [11]. By this time, work on the use of \\ncomputers for literary and linguistic studies had also started.  \\nAs early as 1960 signature work influenced by AI began, with the BASEBALL Q-A systems \\n(Green et al., 1961) [12]. LUNAR (Woods ,1978) [13] and Winograd SHRDLU were natural \\nsuccessors of these systems but they were seen as stepped up sophistication, in terms of their \\nlinguistic and their task processing capabilities. There was a widespread belief that progress \\ncould only be made on the two sides, one is ARPA Speech Understanding Research (SUR) \\nproject (Lea, 1980) and other in some major system developments projects building database \\nfront ends. The front-end projects (Hendrix et al., 1978) [14] were intended to go beyond \\nLUNAR in interfacing the large databases. \\nIn early 1980s computational grammar theory became a very active area of research linked \\nwith logics for meaning and knowledge’s ability to deal with the user’s beliefs and intentions \\nand with functions like emphasis and themes. \\nBy the end of the decade the powerful general purpose sentence processors like SRI’s Core \\nLanguage Engine (Alshawi,1992) [15] and Discourse Representation Theory (Kamp and \\nReyle,1993) [16] offered a means of tackling more extended discourse within the \\ngrammatico-logical framework. This period was one of the growing community. Practical \\nresources, grammars, and tools and parsers became available (e.g the Alvey Natural \\nLanguage Tools (Briscoe et al., 1987) [17]. The (D)ARPA speech recognition and message \\nunderstanding (information extraction) conferences were not only for the tasks they \\naddressed but for the emphasis on heavy evaluation, starting a trend that became a major \\nfeature in 1990s (Young and Chase, 1998; Sundheim and Chinchor ,1993) [18][19]. Work on \\nuser modelling (Kobsa and Wahlster , 1989) [20]  was one strand in research paper and on \\ndiscourse structure serving this (Cohen et al., 1990)  [21]. At the same time, as McKeown \\n(1985) [22] showed, rhetorical schemas could be used for producing both linguistically \\ncoherent and communicatively effective text. Some researches in NLP marked important \\ntopics for future like word sense disambiguation (Small et al., 1988) [23] and probabilistic \\nnetworks, statistically coloured NLP, the work on the lexicon, also pointed in this direction.'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 7}, page_content='Statistical language processing was a major thing in 90s (Manning and Schuetze,1999) [24], \\nbecause this not only involves data analysts. Information extraction and automatic \\nsummarising (Mani and Maybury ,1999) [25] was also a point of focus. \\nRecent researches are mainly focused on unsupervised and semi-supervised learning \\nalgorithms. \\n5. Related Work \\nMany researchers worked on NLP, building tools and systems which makes NLP what it is \\ntoday. Tools like Sentiment Analyser, Parts of Speech (POS)Taggers, Chunking, Named \\nEntity Recognitions (NER), Emotion detection, Semantic Role Labelling made NLP a good \\ntopic for research.  \\nSentiment analyser (Jeonghee etal.,2003) [26] works by extracting sentiments about given \\ntopic. Sentiment analysis consists of a topic specific feature term extraction, sentiment \\nextraction, and association by relationship analysis. Sentiment Analysis utilizes two linguistic \\nresources for the analysis: the sentiment lexicon and the sentiment pattern database. It \\nanalyses the documents for positive and negative words and try to give ratings on scale -5 to \\n+5. \\nParts of speech taggers for the languages like European languages, research is being done on \\nmaking parts of speech taggers for other languages like Arabic, Sanskrit (Namrata Tapswi , \\nSuresh Jain ., 2012) [27], Hindi (Pradipta Ranjan Ray et al., 2003 )[28] etc. It can efficiently \\ntag and classify words as nouns, adjectives, verbs etc. The most procedures for part of speech \\ncan work efficiently on European languages, but it won’t on Asian languages or middle \\neastern languages. Sanskrit part of speech tagger is specifically uses treebank technique. \\nArabic uses Support Vector Machine (SVM) (Mona Diab etal.,2004) [29] approach to \\nautomatically tokenize, parts of speech tag and annotate base phrases in Arabic text. \\n \\nChunking – it is also known as Shadow Parsing, it works by labelling segments of sentences \\nwith syntactic correlated keywords like Noun Phrase and Verb Phrase (NP or VP). Every \\nword has a unique tag often marked as Begin Chunk (B-NP) tag or Inside Chunk (I-NP) tag. \\nChunking is often evaluated using the CoNLL 2000 shared task.  CoNLL 2000 provides test \\ndata for Chunking. Since then, a certain number of systems arised (Sha and Pereira, 2003; \\nMcDonald et al., 2005; Sun et al., 2008) [30] [31] [32], all reporting around 94.3% F1 score. \\nThese systems use features composed of words, POS tags, and tags. \\n \\nUsage of Named Entity Recognition in places such as Internet is a problem as people don’t \\nuse traditional or standard English. This degrades the performance of standard natural \\nlanguage processing tools substantially. By annotating the phrases or tweets and building \\ntools trained on unlabelled, in domain and out domain data (Alan Ritter., 2011) [33]. It \\nimproves the performance as compared to standard natural language processing tools. \\n \\nEmotion Detection (Shashank Sharma, 2016) [34] is similar to sentiment analysis, but it \\nworks on social media platforms on mixing of two languages (English + Any other Indian \\nLanguage). It categorizes statements into six groups based on emotions. During this process, \\nthey were able to identify the language of ambiguous words which were common in Hindi \\nand English and tag lexical category or parts of speech in mixed script by identifying the base \\nlanguage of the speaker.'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 8}, page_content='Sematic Role Labelling – SRL works by giving a semantic role to a sentence. For example in \\nthe PropBank (Palmer et al., 2005) [35] formalism, one assigns roles to words that are \\narguments of a verb in the sentence. The precise arguments depend on verb frame and if there \\nexists multiple verbs  in a sentence, it might have multiple tags. State-of-the-art SRL systems \\ncomprise of several stages: creating a parse tree, identifying which parse tree nodes represent \\nthe arguments of a given verb, and finally classifying these nodes to compute the \\ncorresponding SRL tags. \\n \\nEvent discovery in social media feeds (Edward Benson et al.,2011) [36], using a graphical \\nmodel to analyse any social media feeds to determine whether it contains name of a person or \\nname of a venue, place, time etc. The model operates on noisy feeds of data to extract records \\nof events by aggregating multiple information across multiple messages, despite the noise of \\nirrelevant noisy messages and very irregular message language, this model was able to extract \\nrecords with high accuracy. However, there is some scope for improvement using broader \\narray of features on factors. \\n \\n6. Applications of NLP \\nNatural Language Processing can be applied into various areas like Machine Translation, \\nEmail Spam detection, Information Extraction, Summarization, Question Answering etc. \\n6.1 Machine Translation  \\nAs most of the world is online, the task of making data accessible and available to all is a \\nchallenge. Major challenge in making data accessible is the language barrier. There are \\nmultitude of languages with different sentence structure and grammar. Machine Translation is \\ngenerally translating phrases from one language to another with the help of a statistical \\nengine like Google Translate. The challenge with machine translation technologies is not \\ndirectly translating words but keeping the meaning of sentences intact along with grammar \\nand tenses. The statistical machine learning gathers as many data as they can find that seems \\nto be parallel between two languages and they crunch their data to find the likelihood that \\nsomething in Language A corresponds to something in Language B. As for Google, in \\nSeptember 2016, announced a new machine translation system based on Artificial neural \\nnetworks and Deep learning . In recent years, various methods have been proposed to \\nautomatically evaluate machine translation quality by comparing hypothesis translations with \\nreference translations. Examples of such methods are word error rate, position-independent \\nword error rate (Tillmann et al., 1997) [37], generation string accuracy (Bangalore et al., \\n2000) [38], multi-reference word error rate (Nießen et al., 2000) [39], BLEU score (Papineni \\net al., 2002) [40], NIST score (Doddington, 2002) [41]  All these criteria try to approximate \\nhuman assessment and often achieve an astonishing degree of correlation to human subjective \\nevaluation of fluency and adequacy (Papineni et al., 2001; Doddington, 2002) [42][43].  \\n6.2 Text Categorization  \\nCategorization systems inputs a large flow of data like official documents, military casualty \\nreports, market data, newswires etc. and assign them to predefined categories or indices. For \\nexample, The Carnegie Group’s Construe system (Hayes PJ ,Westein ; 1991)[44] , inputs \\nReuters articles and saves much time by doing the work that is to be done by staff or human'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 9}, page_content='indexers. Some companies have been using categorization systems to categorize trouble \\ntickets or complaint requests and routing to the appropriate desks. Another application of text \\ncategorization is email spam filters. Spam filters is becoming important as the first line of \\ndefence against the unwanted emails. A false negative and false positive issues of spam filters \\nare at the heart of NLP technology, its brought down to the challenge of extracting meaning \\nfrom strings of text. A filtering solution that is applied to an email system uses a set of \\nprotocols to determine which of the incoming messages are spam and which are not. There \\nare several types of spam filters available. Content filters: Review the content within the \\nmessage to determine whether it is a spam or not. Header filters: Review the email header \\nlooking for fake information. General Blacklist filters: Stopes all emails from blacklisted \\nrecipients. Rules Based Filters: It uses user-defined criteria. Such as stopping mails from \\nspecific person or stopping mail including a specific word. Permission Filters: Require \\nanyone sending a message to be pre-approved by the recipient. Challenge Response Filters: \\nRequires anyone sending a message to enter a code in order to gain permission to send email. \\n6.3 Spam Filtering  \\nIt works using text categorization and in recent times, various machine learning techniques \\nhave been applied to text categorization or Anti-Spam Filtering  like Rule Learning (Cohen \\n1996)[45], Naïve Bayes (Sahami et al., 1998 ;Androutsopoulos et al.,2000b ;Rennie \\n.,2000)[46][47][48],Memory based Learning (Androutsopoulos et al.,2000b)[47], Support \\nvector machines (Druker et al., 1999)[49], Decision Trees (Carreras and Marquez , 2001)[50] \\nMaximum Entropy Model (Berger et al. 1996)[51]. Sometimes combining different learners \\n(Sakkis et al., 2001) [52]. Using these approaches is better as classifier is learned from \\ntraining data rather than making by hand. The naïve bayes is preferred because of its \\nperformance despite its simplicity (Lewis, 1998) [53] In Text Categorization two types of \\nmodels have been used (McCallum and Nigam, 1998) [54]. Both modules assume that a fixed \\nvocabulary is present. But in first model a document is generated by first choosing a subset of \\nvocabulary and then using the selected words any number of times, at least once irrespective \\nof order. This is called Multi-variate Bernoulli model. It takes the information of which \\nwords are used in a document irrespective of number of words and order. In second model, a \\ndocument is generated by choosing a set of word occurrences and arranging them in any \\norder. this model is called multi-nomial model, in addition to the Multi-variate Bernoulli \\nmodel, it also captures information on how many times a word is used in a document. Most \\ntext categorization approaches to anti spam Email filtering have used multi variate Bernoulli \\nmodel (Androutsopoulos et al.,2000b) [47] \\n6.4 Information Extraction \\nInformation extraction is concerned with identifying phrases of interest of textual data. For \\nmany applications, extracting entities such as names, places, events, dates, times and prices is \\na powerful way of summarize the information relevant to a user’s needs. In the case of a \\ndomain specific search engine, the automatic identification of important information can \\nincrease accuracy and efficiency of a directed search. There is use of hidden Markov models \\n(HMMs) to extract the relevant fields of research papers. These extracted text segments are'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 10}, page_content='used to allow searched over specific fields and to provide effective presentation of search \\nresults and to match references to papers. For example, noticing the pop up ads on any \\nwebsites showing the recent items you might have looked on an online store with discounts. \\nIn Information Retrieval two types of models have been used (McCallum and Nigam, 1998) \\n[55]. Both modules assume that a fixed vocabulary is present. But in first model a document \\nis generated by first choosing a subset of vocabulary and then using the selected words any \\nnumber of times, at least once without any order. This is called Multi-variate Bernoulli \\nmodel. It takes the information of which words are used in a document irrespective of number \\nof words and order. In second model, a document is generated by choosing a set of word \\noccurrences and arranging them in any order. this model is called multi-nomial model, in \\naddition to the Multi-variate Bernoulli model , it also captures information on how many \\ntimes a word is used in a document \\nDiscovery of knowledge is becoming important areas of research over the recent years. \\nKnowledge discovery research use a variety of techniques in order to extract useful \\ninformation from source documents like  \\nParts of Speech (POS) tagging, Chunking or Shadow Parsing, Stop-words (Keywords that \\nare used and must be removed before processing documents), Stemming (Mapping words to \\nsome base for, it has two methods, dictionary based stemming and Porter style stemming \\n(Porter, 1980) [55]. Former one has higher accuracy but higher cost of implementation while \\nlatter has lower implementation cost and is usually insufficient for IR). Compound or \\nStatistical Phrases (Compounds and statistical phrases index multi token units instead of \\nsingle tokens.) Word Sense Disambiguation (Word sense disambiguation is the task of \\nunderstanding the correct sense of a word in context. When used for information retrieval, \\nterms are replaced by their senses in the document vector.) \\n \\nIts extracted information can be applied on a variety of purpose, for example to prepare a \\nsummary, to build databases, identify keywords, classifying text items according to some pre-\\ndefined categories etc. For example   CONSTRUE, it was developed for Reuters, that is used \\nin classifying news stories (Hayes, 1992) [57]. It has been suggested that many IE systems \\ncan successfully extract terms from documents, acquiring relations between the terms is still a \\ndifficulty. PROMETHEE is a system that extracts lexico-syntactic patterns relative to a \\nspecific conceptual relation (Morin,1999) [58]. IE systems should work at many levels, from \\nword recognition to discourse analysis at the level of the complete document. An application \\nof the Blank Slate Language Processor (BSLP) (Bondale et al., 1999) [59] approach for the \\nanalysis of a real life natural language corpus that consists of responses to open-ended \\nquestionnaires in the field of advertising. \\nThere’s a system called MITA (Metlife’s Intelligent Text Analyzer) (Glasgow et al. (1998) \\n[60]) that extracts information from life insurance applications. Ahonen et al. (1998) [61] \\nsuggested a mainstream framework for text mining that uses pragmatic and discourse level \\nanalyses of text. \\n6.5 Summarization \\nOverload of information is the real thing in this digital age, and already our reach and access \\nto knowledge and information exceeds our capacity to understand it. This trend is not slowing \\ndown, so an ability to summarize the data while keeping the meaning intact is highly'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 11}, page_content='required. This is important not just allowing us the ability to recognize the understand the \\nimportant information for a large set of data, it is used to understand the deeper emotional \\nmeanings; For example, a company determine the general sentiment on social media and use \\nit on their latest product offering. This application is useful as a valuable marketing asset. \\nThe types of text summarization depends on the basis of the number of documents and  the \\ntwo important categories are single document summarization and multi document \\nsummarization (Zajic et al. 2008 [62]; Fattah and Ren 2009 [63]). Summaries can also be of \\ntwo types: generic or query-focused (Gong and Liu 2001 [64]; Dunlavy et al. 2007 [65]; Wan \\n2008 [66]; Ouyang et al. 2011 [67]). Summarization task can be either supervised or \\nunsupervised (Mani and Maybury 1999 [68]; Fattah and Ren 2009 [63]; Riedhammer et al. \\n2010 [69]). Training data is required in a supervised system for selecting relevant material \\nfrom the documents. Large amount of annotated data is needed for learning techniques. Few \\ntechniques are as follows– \\n- \\nBayesian Sentence based Topic Model (BSTM) uses both term-sentences and term \\ndocument associations for summarizing multiple documents. (Wang et al. 2009 \\n[70])   \\n- \\nFactorization with Given Bases (FGB) is a language model where sentence bases \\nare the given bases and it utilizes document-term and sentence term matrices. \\nThis approach groups and summarizes the documents simultaneously. (Wang et \\nal. 2011) [71]) \\n- \\nTopic Aspect-Oriented Summarization (TAOS) is based on topic factors. These \\ntopic factors are various features that describe topics such as capital words are \\nused to represent entity. Various topics can have various aspects and various \\npreferences of features are used to represent various aspects. (Fang et al. 2015 [72]) \\n \\n6.6 Dialogue System \\nPerhaps the most desirable application of the future, in the systems envisioned by large \\nproviders of end user applications, Dialogue systems, which focuses on a narrowly defined \\napplications (like refrigerator or home theater systems) currently uses the phonetic and lexical \\nlevels of language. It is believed that these dialogue systems when utilizing all levels of \\nlanguage processing offer potential for fully automated dialog systems. (Elizabeth D. Liddy, \\n2001) [7]. Whether on text or via voice. This could lead to produce systems that can enable \\nrobots to interact with humans in natural languages. Examples like Google’s assistant, \\nWindows Cortana, Apple’s Siri and Amazon’s Alexa are the software and devices that follow \\nDialogue systems. \\n6.7 Medicine \\nNLP is applied in medicine field as well. The Linguistic String Project-Medical Language \\nProcessor is one the large scale projects of NLP in the field of medicine [74][75][76][77][78]. \\nThe LSP-MLP helps enabling physicians to extract and summarize information of any signs \\nor symptoms, drug dosage and response data with aim of identifying possible side effects of \\nany medicine while highlighting or flagging data items [74]. The National Library of \\nMedicine is developing The Specialist System [79][80][81][82][83]. It is expected to function \\nas Information Extraction tool for Biomedical Knowledge Bases, particularly Medline'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 12}, page_content='abstracts. The lexicon was created using MeSH (Medical Subject Headings), Dorland’s \\nIllustrated Medical Dictionary and general English Dictionaries. The Centre d’Informatique \\nHospitaliere of the Hopital Cantonal de Geneve is working on an electronic archiving \\nenvironment with NLP features [84][85]. In first phase, patient records were archived . At \\nlater stage the LSP-MLP has been adapted for French [86][87][88][89] , and finally , a proper \\nNLP system called RECIT  [90][91][92][93] has been developed using a method called \\nProximity Processing [94]. It’s task was to implement a robust and multilingual system able \\nto analyze/comprehend medical sentences, and to preserve a knowledge of free text into a \\nlanguage independent knowledge representation [95][96]. The Columbia university of New \\nYork has developed an NLP system called MEDLEE (MEDical Language Extraction and \\nEncoding System) that identifies clinical information in narrative reports and transforms the \\ntextual information into structured representation [97]. \\n7. Approaches \\nRationalist approach or symbolic approach assume that crucial part of the knowledge in the \\nhuman mind is not derived by the sense but is firm in advance, probably by genetic in \\nheritance. Noam Chomsky was the strongest advocate of this approach. It was trusted that \\nmachine can be  made to function like human brain by giving some fundamental knowledge \\nand reasoning mechanism linguistics  knowledge is directly encoded in rule or other forms of \\nrepresentation. This helps automatic process of natural languages. [98] Statistical and \\nmachine learning entail evolution of algorithms that allow a program to infer patterns. An \\niterative process is used to characterize a given algorithm’s underlying algorithm that are \\noptimised by a numerical measure that characterize numerical parameters and learning phase. \\nMachine-learning models can be predominantly categorized as either generative or \\ndiscriminative. Generative methods can generate synthetic data because of which they create \\nrich models of probability distributions. Discriminative methods are more functional and \\nhave right estimating posterior probabilities and are based on observations.  \\nSrihari [99] explains the different generative models as one with a resemblance that is used to \\nspot an unknown speaker’s language and would bid the deep knowledge of numerous \\nlanguage to perform the match. Whereas discriminative methods rely on a less knowledge-\\nintensive approach and using distinction between language.  Whereas generative models, can \\nbecome troublesome when many features are used and discriminative models allow use of \\nmore features. [100] Few of the examples of discriminative methods are Logistic regression \\nand conditional random fields (CRFs), generative methods are Naive Bayes classifiers and \\nhidden Markov models (HMMs). \\n7.1 Hidden Markov Model (HMM) \\nAn HMM is a system where a shifting takes place between several states, generating feasible \\noutput symbols with each switch. The sets of viable states and unique symbols may be large, \\nbut finite and known. We can descry the outputs, but the system’s internals are hidden. Few \\nof the problem could be solved are by Inference A certain sequence of output symbols, \\ncompute the probabilities of one or more candidate states with sequences. Pattern matching \\nthe state-switch sequence is realised are most likely to have generated a particular output-'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 13}, page_content='symbol sequence. Training the output-symbol chain data, reckon the state-switch/output \\nprobabilities that fit this data best. \\nHidden Markov Models are extensively used for speech recognition, where the output \\nsequence is matched to the sequence of individual phonemes. Frederick Jelinek, a statistical-\\nNLP advocate who first instigated HMMs at IBM’s Speech Recognition Group, reportedly \\njoked, every time a linguist leaves my group, the speech recognizer’s performance improves. \\n[101] HMM is not restricted to this application it has several others such as bioinformatics \\nproblems, for example, multiple sequence alignment [102]. Sonnhammer mentioned that \\nPfam hold multiple alignments and hidden Markov model based profiles (HMM-profiles) of \\nentire protein domains. The cue of domain boundaries, family members and alignment is \\ndone semi-automatically found on expert knowledge, sequence similarity, other protein \\nfamily databases and the capability of HMM-profiles to correctly identify and align the \\nmembers. [103]  \\n7.2 Naive Bayes Classifiers \\n The choice of area is wide ranging covering usual items like word segmentation and \\ntranslation but also unusual areas like segmentation for infant learning and identifying \\ndocuments for opinions and facts. In addition, exclusive article was selected for its use of \\nBayesian methods to aid the research in designing algorithms for their investigation. \\n8. NLP in Talk \\nThis section discusses the recent developments in the NLP projects implemented by various \\ncompanies and these are as follows: \\n8.1 ACE Powered GDPR Robot Launched by RAVN Systems [104] \\nRAVN Systems, an leading expert in Artificial Intelligence (AI), Search and Knowledge \\nManagement Solutions, announced the launch of a RAVN (\"Applied Cognitive Engine\") i.e \\npowered software Robot to help and facilitate the GDPR (\"General Data Protection \\nRegulation\") compliance. \\nThe Robot uses AI techniques to automatically analyse documents and other types of data in \\nany business system which is subject to GDPR rules. It allows users to quickly and easily \\nsearch, retrieve, flag, classify and report on data mediated to be supersensitive under GDPR. \\nUsers also have the ability to identify personal data from documents, view feeds on the latest \\npersonal data that requires attention and provide reports on the data suggested to be deleted or \\nsecured.  RAVN\\'s GDPR Robot is also able to hasten requests for information (Data Subject \\nAccess Requests - \"DSAR\") in a simple and efficient way, removing the need for a physical \\napproach to these requests which tends to be very labour thorough. Peter Wallqvist, CSO at \\nRAVN Systems commented, \"GDPR compliance is of universal paramountcy as it will \\nexploit to any organisation that control and process data concerning EU citizens. \\nLINK:http://markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po\\nwered_GDPR_Robot \\n8.2 Eno A Natural Language Chatbot Launched by Capital One [105]'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 14}, page_content='Capital one announces chatbot for customers called Eno. Eno is a natural language chatbot \\nthat people socialize through texting. Capital one claims that Eno is First natural language \\nSMS chatbot from a U.S. bank that allows customer to ask questions using natural language. \\nCustomers can interact with Eno asking questions about their savings and others using a text \\ninterface. Eno makes such an environment that it feels that a human is interacting. Ken \\nDodelin, Capital One’s vice president of digital product development, said “We kind of \\nlaunched a chatbot and didn’t know it.”  \\nThis provides a different platform than other brands that launch chatbots like Facebook \\nMessenger and Skype. They believed that Facebook has too much access of private \\ninformation of a person, which could get them into trouble with privacy laws of U.S. \\nfinancial institutions work under. Like any Facebook Page admin can access full transcripts \\nof the bot’s conversations. If that would be the case then the admins could easily view the \\npersonal banking information of customers with is not correct \\n LINK: https://www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/ \\n8.3 Future of BI in Natural Language Processing [106] \\nSeveral companies in Bi spaces are trying to get with the trend and trying hard to ensure that \\ndata becomes more friendly and easily accessible. But still there is long way for this.BI will \\nalso make it easier to access as GUI is not needed. Because now a days the queries are made \\nby text or voice command on smartphones.one of the most common example is Google might \\ntell you today what will be the tomorrows weather. But soon enough, we will be able to ask \\nour personal data chatbot about customer sentiment today, and how do we feel about their \\nbrand next week; all while walking down the street. Today, NLP tends to be based on turning \\nnatural language into machine language. But with time the technology matures – especially \\nthe AI component –the computer will get better at “understanding” the query and start to \\ndeliver answers rather than search results. \\n Initially, the data chatbot will probably ask the question as how have revenues changed over \\nthe last three-quarters?’ and then return pages of data for you to analyse. But once it learns \\nthe semantic relations and inferences of the question, it will be able to automatically perform \\nthe filtering and formulation necessary to provide an intelligible answer, rather than simply \\nshowing you data. \\nLink: http://www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi \\n8.4 Using Natural Language Processing and Network Analysis to \\nDevelop a Conceptual Framework for Medication Therapy \\nManagement Research [107] \\nNatural Language Processing and Network Analysis to Develop a Conceptual Framework for \\nMedication Therapy Management Research describes a theory derivation process that is used \\nto develop conceptual framework for medication therapy management (MTM) research. The \\nMTM service model and chronic care model are selected as parent theories. Review article'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 15}, page_content='abstracts target medication therapy management in chronic disease care that were retrieved \\nfrom Ovid Medline (2000-2016). \\nUnique concepts in each abstract are extracted using Meta Map and their pairwise \\ncooccurrence are determined. Then the information is used to construct a network graph of \\nconcept co-occurrence that is further analysed to identify content for the new conceptual \\nmodel. 142 abstracts are analysed. Medication adherence is the most studied drug therapy \\nproblem and co-occurred with concepts related to patient-centred interventions targeting self-\\nmanagement. The enhanced model consists of 65 concepts clustered into 14 constructs. The \\nframework requires additional refinement and evaluation to determine its relevance and \\napplicability across a broad audience including underserved settings. \\nLink: https://www.ncbi.nlm.nih.gov/pubmed/28269895?dopt=Abstract \\n8.5 Meet the Pilot, world’s first language translating earbuds [108] \\nThe world’s first smart earpiece Pilot will soon be transcribed over 15 languages. According \\nto Spring wise, Waverly Labs’ Pilot can already transliterate five spoken languages, English, \\nFrench, Italian, Portuguese and Spanish, and seven written affixed languages, German, Hindi, \\nRussian, Japanese, Arabic, Korean and Mandarin Chinese. The Pilot earpiece is connected \\nvia Bluetooth to the Pilot speech translation app, which uses speech recognition, machine \\ntranslation and machine learning and speech synthesis technology. \\nSimultaneously, the user will hear the translated version of the speech on the second earpiece. \\nMoreover, it is not necessary that conversation would be taking place between two people \\nonly the users can join in and discuss as a group. As if now the user may experience a few \\nsecond lag interpolated the speech and translation, which Waverly Labs pursue to reduce. \\nThe Pilot earpiece will be available from September, but can be pre-ordered now for $249. \\nThe earpieces can also be used for streaming music, answering voice calls and getting audio \\nnotifications. \\nLink:https://www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-\\nheadphones-travel#/ \\n \\nREFRENCES \\n[1] Chomsky, Noam, 1965, Aspects of the Theory of Syntax, Cambridge, Massachusetts: \\nMIT Press.  \\n [2] Rospocher, M., van Erp, M., Vossen, P., Fokkens, A., Aldabe,I., Rigau, G., Soroa, A., \\nPloeger, T., and Bogaard, T.(2016). Building event-centric knowledge graphs from news. \\nWeb Semantics: Science, Services and Agents on the World Wide Web, In Press. \\n[3] Shemtov, H. (1997). Ambiguity management in natural language generation. Stanford \\nUniversity.  \\n[4] Emele, M. C., & Dorna, M. (1998, August). Ambiguity preserving machine translation \\nusing packed representations. In Proceedings of the 36th Annual Meeting of the Association'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 16}, page_content='for Computational Linguistics and 17th International Conference on Computational \\nLinguistics-Volume 1 (pp. 365-371). Association for Computational Linguistics. \\n[5] Knight, K., & Langkilde, I. (2000, July). Preserving ambiguities in generation via \\nautomata intersection. In AAAI/IAAI (pp. 697-702). \\n[6] Nation, K., Snowling, M. J., & Clarke, P. (2007). Dissecting the relationship between \\nlanguage skills and learning to read: Semantic and phonological contributions to new \\nvocabulary learning in children with poor reading comprehension. Advances in Speech \\nLanguage Pathology, 9(2), 131-139. \\n[7] Liddy, E. D. (2001). Natural language processing. \\n[8] Feldman, S. (1999). NLP Meets the Jabberwocky: Natural Language Processing in \\nInformation Retrieval. ONLINE-WESTON THEN WILTON-, 23, 62-73. \\n[9] \"Natural Language Processing.\" Natural Language Processing RSS. N.p., n.d. Web. 25 \\nMar. 2017 \\n[10] Hutchins, W. J. (1986). Machine translation: past, present, future (p. 66). Chichester: \\nEllis Horwood. \\n[11] Hutchins, W. J. (Ed.). (2000). Early years in machine translation: memoirs and \\nbiographies of pioneers (Vol. 97). John Benjamins Publishing. \\n[12] Green Jr, B. F., Wolf, A. K., Chomsky, C., & Laughery, K. (1961, May). Baseball: an \\nautomatic question-answerer. In Papers presented at the May 9-11, 1961, western joint IRE-\\nAIEE-ACM computer conference (pp. 219-224). ACM. \\n[13] Woods, W. A. (1978). Semantics and quantification in natural language question \\nanswering. Advances in computers, 17, 1-87. \\n[14] Hendrix, G. G., Sacerdoti, E. D., Sagalowicz, D., & Slocum, J. (1978). Developing a \\nnatural language interface to complex data. ACM Transactions on Database Systems \\n(TODS), 3(2), 105-147. \\n[15] Alshawi, H. (1992). The core language engine. MIT press. \\n[16] Kamp, H., & Reyle, U. (1993). Tense and Aspect. In From Discourse to Logic (pp. 483-\\n689). Springer Netherlands. \\n[17] Lea , W.A Trends in speech recognition , Englewoods Cliffs , NJ: Prentice Hall , 1980. \\n[18] Young, S. J., & Chase, L. L. (1998). Speech recognition evaluation: a review of the US \\nCSR and LVCSR programmes. Computer Speech & Language, 12(4), 263-279. \\n[19] Sundheim, B. M., & Chinchor, N. A. (1993, March). Survey of the message \\nunderstanding conferences. In Proceedings of the workshop on Human Language \\nTechnology (pp. 56-60). Association for Computational Linguistics.'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 17}, page_content='[20] Wahlster, W., & Kobsa, A. (1989). User models in dialog systems. In User models in \\ndialog systems (pp. 4-34). Springer Berlin Heidelberg. \\n[21] McKeown, K.R. Text generation , Cambridge: Cambridge University Press , 1985. \\n[22] Small S.L., Cortell G.W., and Tanenhaus , M.K. Lexical Ambiguity Resolutions , San \\nMateo , CA : Morgan Kauffman, 1988. \\n[23] Manning, C. D., & Schütze, H. (1999). Foundations of statistical natural language \\nprocessing (Vol. 999). Cambridge: MIT press. \\n[24] Mani, I., & Maybury, M. T. (Eds.). (1999). Advances in automatic text \\nsummarization (Vol. 293). Cambridge, MA: MIT press. \\n[25] Yi, J., Nasukawa, T., Bunescu, R., & Niblack, W. (2003, November). Sentiment \\nanalyzer: Extracting sentiments about a given topic using natural language processing \\ntechniques. In Data Mining, 2003. ICDM 2003. Third IEEE International Conference on (pp. \\n427-434). IEEE. \\n[26] Yi, J., Nasukawa, T., Bunescu, R., & Niblack, W. (2003, November). Sentiment \\nanalyzer: Extracting sentiments about a given topic using natural language processing \\ntechniques. In Data Mining, 2003. ICDM 2003. Third IEEE International Conference on (pp. \\n427-434). IEEE. \\n[27] Tapaswi, N., & Jain, S. (2012, September). Treebank based deep grammar acquisition \\nand Part-Of-Speech Tagging for Sanskrit sentences. In Software Engineering (CONSEG), \\n2012 CSI Sixth International Conference on (pp. 1-4). IEEE. \\n[28] Ranjan, P., & Basu, H. V. S. S. A. (2003). Part of speech tagging and local word \\ngrouping techniques for natural language parsing in Hindi. In Proceedings of the 1st \\nInternational Conference on Natural Language Processing (ICON 2003). \\n[29] Diab, M., Hacioglu, K., & Jurafsky, D. (2004, May). Automatic tagging of Arabic text: \\nFrom raw text to base phrase chunks. In Proceedings of HLT-NAACL 2004: Short \\npapers (pp. 149-152). Association for Computational Linguistics. \\n[30] Sha, F., & Pereira, F. (2003, May). Shallow parsing with conditional random fields. \\nIn Proceedings of the 2003 Conference of the North American Chapter of the Association for \\nComputational Linguistics on Human Language Technology-Volume 1 (pp. 134-141). \\nAssociation for Computational Linguistics. \\n[31] McDonald, R., Crammer, K., & Pereira, F. (2005, October). Flexible text segmentation \\nwith structured multilabel classification. In Proceedings of the conference on Human \\nLanguage Technology and Empirical Methods in Natural Language Processing (pp. 987-\\n994). Association for Computational Linguistics. \\n[32] Sun, X., Morency, L. P., Okanohara, D., & Tsujii, J. I. (2008, August). Modeling latent-\\ndynamic in shallow parsing: a latent conditional model with improved inference.'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 18}, page_content='In Proceedings of the 22nd International Conference on Computational Linguistics-Volume \\n1 (pp. 841-848). Association for Computational Linguistics. \\n[33] Ritter, A., Clark, S., & Etzioni, O. (2011, July). Named entity recognition in tweets: an \\nexperimental study. In Proceedings of the Conference on Empirical Methods in Natural \\nLanguage Processing (pp. 1524-1534). Association for Computational Linguistics. \\n[34] Sharma, S., Srinivas, PYKL, & Balabantaray, RC (2016). Emotion Detection using \\nOnline Machine Learning Method and TLBO on Mixed Script. In Proceedings of Language \\nResources and Evaluation Conference 2016 (pp. 47-51). \\n[35] Palmer, M., Gildea, D., & Kingsbury, P. (2005). The proposition bank: An annotated \\ncorpus of semantic roles. Computational linguistics, 31(1), 71-106. \\n[36] Benson, E., Haghighi, A., & Barzilay, R. (2011, June). Event discovery in social media \\nfeeds. In Proceedings of the 49th Annual Meeting of the Association for Computational \\nLinguistics: Human Language Technologies-Volume 1 (pp. 389-398). Association for \\nComputational Linguistics. \\n[37] Tillmann, C., Vogel, S., Ney, H., Zubiaga, A., & Sawaf, H. (1997, September). \\nAccelerated DP based search for statistical translation. In Eurospeech. \\n[38] Bangalore, S., Rambow, O., & Whittaker, S. (2000, June). Evaluation metrics for \\ngeneration. In Proceedings of the first international conference on Natural language \\ngeneration-Volume 14 (pp. 1-8). Association for Computational Linguistics \\n[39] Nießen, S., Och, F. J., Leusch, G., & Ney, H. (2000, May). An Evaluation Tool for \\nMachine Translation: Fast Evaluation for MT Research. In LREC \\n[40] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002, July). BLEU: a method for \\nautomatic evaluation of machine translation. In Proceedings of the 40th annual meeting on \\nassociation for computational linguistics (pp. 311-318). Association for Computational \\nLinguistics \\n[41] Doddington, G. (2002, March). Automatic evaluation of machine translation quality \\nusing n-gram co-occurrence statistics. In Proceedings of the second international conference \\non Human Language Technology Research (pp. 138-145). Morgan Kaufmann Publishers Inc  \\n[42] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002, July). BLEU: a method for \\nautomatic evaluation of machine translation. In Proceedings of the 40th annual meeting on \\nassociation for computational linguistics (pp. 311-318). Association for Computational \\nLinguistics \\n[43] Doddington, G. (2002, March). Automatic evaluation of machine translation quality \\nusing n-gram co-occurrence statistics. In Proceedings of the second international conference \\non Human Language Technology Research (pp. 138-145). Morgan Kaufmann Publishers Inc'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 19}, page_content='[44] Hayes, P. J. (1992). Intelligent high-volume text processing using shallow, domain-\\nspecific techniques. Text-based intelligent systems: Current research and practice in \\ninformation extraction and retrieval, 227-242. \\n[45] Cohen, W. W. (1996, March). Learning rules that classify e-mail. In AAAI spring \\nsymposium on machine learning in information access (Vol. 18, p. 25). \\n[46] Sahami, M., Dumais, S., Heckerman, D., & Horvitz, E. (1998, July). A Bayesian \\napproach to filtering junk e-mail. In Learning for Text Categorization: Papers from the 1998 \\nworkshop (Vol. 62, pp. 98-105). \\n[47] Androutsopoulos, I., Paliouras, G., Karkaletsis, V., Sakkis, G., Spyropoulos, C. D., & \\nStamatopoulos, P. (2000). Learning to filter spam e-mail: A comparison of a naive bayesian \\nand a memory-based approach. arXiv preprint cs/0009009. \\n[48] Rennie, J. (2000, August). ifile: An application of machine learning to e-mail filtering. \\nIn Proc. KDD 2000 Workshop on Text Mining, Boston, MA \\n[49] Drucker, H., Wu, D., & Vapnik, V. N. (1999). Support vector machines for spam \\ncategorization. IEEE Transactions on Neural networks, 10(5), 1048-1054 \\n[50] Carreras, X., & Marquez, L. (2001). Boosting trees for anti-spam email filtering. arXiv \\npreprint cs/0109015 \\n[51] BERGER, A. L., DELLA PIETRA, S. A., AND DELLA PIETRA, V. J. 1996. A \\nmaximum entropy approach to natural language processing. Computational Linguistics 22, 1, \\n39–71 \\n[52] Sakkis, G., Androutsopoulos, I., Paliouras, G., Karkaletsis, V., Spyropoulos, C. D., & \\nStamatopoulos, P. (2001). Stacking classifiers for anti-spam filtering of e-mail. arXiv preprint \\ncs/0106040.. \\n[53] Lewis, D. D. (1998, April). Naive (Bayes) at forty: The independence assumption in \\ninformation retrieval. In European conference on machine learning (pp. 4-15). Springer \\nBerlin Heidelberg \\n[54] McCallum, A., & Nigam, K. (1998, July). A comparison of event models for naive bayes \\ntext classification. In AAAI-98 workshop on learning for text categorization (Vol. 752, pp. \\n41-48). \\n[55] McCallum, A., & Nigam, K. (1998, July). A comparison of event models for naive bayes \\ntext classification. In AAAI-98 workshop on learning for text categorization (Vol. 752, pp. \\n41-48). \\n[56] Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3), 130-137'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 20}, page_content='[57] Hayes, P. J. (1992). Intelligent high-volume text processing using shallow, domain-\\nspecific techniques. Text-based intelligent systems: Current research and practice in \\ninformation extraction and retrieval, 227-242 \\n[58] Morin, E. (1999, August). Automatic acquisition of semantic relations between terms \\nfrom technical corpora. In Proc. of the Fifth International Congress on Terminology and \\nKnowledge Engineering-TKE’99. \\n[59] Bondale, N., Maloor, P., Vaidyanathan, A., Sengupta, S., & Rao, P. V. (1999). \\nExtraction of information from open-ended questionnaires using natural language processing \\ntechniques. Computer Science and Informatics, 29(2), 15-22 \\n[60] Glasgow, B., Mandell, A., Binney, D., Ghemri, L., & Fisher, D. (1998). MITA: An \\ninformation-extraction approach to the analysis of free-form text in life insurance \\napplications. AI magazine, 19(1), 59. \\n[61] Ahonen, H., Heinonen, O., Klemettinen, M., & Verkamo, A. I. (1998, April). Applying \\ndata mining techniques for descriptive phrase extraction in digital document collections. \\nIn Research and Technology Advances in Digital Libraries, 1998. ADL 98. Proceedings. \\nIEEE International Forum on (pp. 2-11). IEEE. \\n[62] Zajic, D. M., Dorr, B. J., & Lin, J. (2008). Single-document and multi-document \\nsummarization techniques for email threads using sentence compression. Information \\nProcessing & Management, 44(4), 1600-1610. \\n[63] Fattah, M. A., & Ren, F. (2009). GA, MR, FFNN, PNN and GMM based models for \\nautomatic text summarization. Computer Speech & Language, 23(1), 126-144. \\n[64] Gong, Y., & Liu, X. (2001, September). Generic text summarization using relevance \\nmeasure and latent semantic analysis. In Proceedings of the 24th annual international ACM \\nSIGIR conference on Research and development in information retrieval (pp. 19-25). ACM. \\n[65] Dunlavy, D. M., O’Leary, D. P., Conroy, J. M., & Schlesinger, J. D. (2007). QCS: A \\nsystem for querying, clustering and summarizing documents. Information processing & \\nmanagement, 43(6), 1588-1605. \\n[66] Wan, X. (2008). Using only cross-document relationships for both generic and topic-\\nfocused multi-document summarizations. Information Retrieval, 11(1), 25-49. \\n[67] Ouyang, Y., Li, W., Li, S., & Lu, Q. (2011). Applying regression models to query-\\nfocused multi-document summarization. Information Processing & Management, 47(2), 227-\\n237. \\n[68] Mani, I., & Maybury, M. T. (Eds.). (1999). Advances in automatic text \\nsummarization (Vol. 293). Cambridge, MA: MIT press.'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 21}, page_content='[69] Riedhammer, K., Favre, B., & Hakkani-Tür, D. (2010). Long story short–global \\nunsupervised \\nmodels \\nfor \\nkeyphrase \\nbased \\nmeeting \\nsummarization. Speech \\nCommunication, 52(10), 801-815. \\n[70] Wang, D., Zhu, S., Li, T., & Gong, Y. (2009, August). Multi-document summarization \\nusing sentence-based topic models. In Proceedings of the ACL-IJCNLP 2009 Conference \\nShort Papers (pp. 297-300). Association for Computational Linguistics. \\n[71] Wang, D., Zhu, S., Li, T., Chi, Y., & Gong, Y. (2011). Integrating document clustering \\nand multidocument summarization. ACM Transactions on Knowledge Discovery from Data \\n(TKDD), 5(3), 14. \\n[72] Fang, H., Lu, W., Wu, F., Zhang, Y., Shang, X., Shao, J., & Zhuang, Y. (2015). Topic \\naspect-oriented summarization via group selection. Neurocomputing, 149, 1613-1619. \\n[73] Sager, N., Lyman, M., Nhan, N. T., & Tick, L. J. (1995). Medical language processing: \\napplications to patient data representation and automatic encoding. Methods of information in \\nmedicine, 34(1-2), 140-146. \\n[74] Chi, E. C., Lyman, M. S., Sager, N., Friedman, C., & Macleod, C. (1985, November). A \\ndatabase of computer-structured narrative: methods of computing complex relations. \\nIn Proceedings of the Annual Symposium on Computer Application in Medical Care (p. 221). \\nAmerican Medical Informatics Association. \\n[75] Grishman, R., Sager, N., Raze, C., & Bookchin, B. (1973, June). The linguistic string \\nparser. In Proceedings of the June 4-8, 1973, national computer conference and \\nexposition (pp. 427-434). ACM. \\n[76] Hirschman, L., Grishman, R., & Sager, N. (1976, June). From text to structured \\ninformation: automatic processing of medical reports. In Proceedings of the June 7-10, 1976, \\nnational computer conference and exposition (pp. 267-275). ACM. \\n[77] Sager, N. (1981). Natural language information processing. Addison-Wesley Publishing \\nCompany, Advanced Book Program. \\n[78] Lyman, M., Sager, N., Friedman, C., & Chi, E. (1985, November). Computer-structured \\nnarrative in ambulatory care: its use in longitudinal review of clinical data. In Proceedings of \\nthe Annual Symposium on Computer Application in Medical Care (p. 82). American Medical \\nInformatics Association. \\n[79] McCray, A. T., & Nelson, S. J. (1995). The representation of meaning in the \\nUMLS. Methods of information in medicine, 34(1-2), 193-201. \\n[80] McGray, A. T., Sponsler, J. L., Brylawski, B., & Browne, A. C. (1987, November). The \\nrole of lexical knowledge in biomedical text understanding. In Proceedings of the Annual \\nSymposium on Computer Application in Medical Care (p. 103). American Medical \\nInformatics Association.'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 22}, page_content=\"[81] McCray, A. T. (1991). Natural language processing for intelligent information retrieval. \\nIn Engineering in Medicine and Biology Society, 1991. Vol. 13: 1991., Proceedings of the \\nAnnual International Conference of the IEEE (pp. 1160-1161). IEEE. \\n[82] McCray, A. T. (1991). Extending a natural language parser with UMLS knowledge. \\nIn Proceedings of the Annual Symposium on Computer Application in Medical Care (p. 194). \\nAmerican Medical Informatics Association. \\n[83] McCray, A. T., Srinivasan, S., & Browne, A. C. (1994). Lexical methods for managing \\nvariation in biomedical terminologies. In Proceedings of the Annual Symposium on \\nComputer Application in Medical Care (p. 235). American Medical Informatics Association. \\n[84] McCray, A. T., & Razi, A. (1994). The UMLS Knowledge Source server. Medinfo. \\nMEDINFO, 8, 144-147. \\n[85] Scherrer, J. R., Revillard, C., Borst, F., Berthoud, M., & Lovis, C. (1994). Medical office \\nautomation integrated into the distributed architecture of a hospital information \\nsystem. Methods of information in medicine, 33(2), 174-179. \\n[86] Baud, R. H., Rassinoux, A. M., & Scherrer, J. R. (1992). Natural language processing \\nand semantical representation of medical texts. Methods of information in medicine, 31(2), \\n117-125. \\n[87] Lyman, M., Sager, N., Chi, E. C., Tick, L. J., Nhan, N. T., Su, Y., ... & Scherrer, J. \\n(1989, November). Medical Language Processing for Knowledge Representation and \\nRetrievals. In Proceedings. Symposium on Computer Applications in Medical Care (pp. 548-\\n553). American Medical Informatics Association. \\n[88] Nhàn, N. T., Sager, N., Lyman, M., Tick, L. J., Borst, F., & Su, Y. (1989, November). A \\nMedical Language Processor for Two Indo-European Languages. In Proceedings. \\nSymposium on Computer Applications in Medical Care (pp. 554-558). American Medical \\nInformatics Association. \\n[89] Sager, N., Lyman, M., Tick, L. J., Borst, F., Nhan, N. T., Revillard, C., ... & Scherrer, J. \\nR. (1989). Adapting a medical language processor from English to French. Medinfo, 89, 795-\\n799. \\n[90] Borst, F., Sager, N., Nhàn, N. T., Su, Y., Lyman, M., Tick, L. J., ... & Scherrer, J. R. \\n(1989). Analyse automatique de comptes rendus d'hospitalisation. In Degoulet P, Stephan JC, \\nVenot A, Yvon PJ, rédacteurs. Informatique et Santé, Informatique et Gestion des Unités de \\nSoins, Comptes Rendus du Colloque AIM-IF, Paris (pp. 246-56). [5] \\n[91] Baud, R. H., Rassinoux, A. M., & Scherrer, J. R. (1991). Knowledge representation of \\ndischarge summaries. In AIME 91 (pp. 173-182). Springer Berlin Heidelberg. \\n[92] Baud, R. H., Alpay, L., & Lovis, C. (1994). Let’s Meet the Users with Natural Language \\nUnderstanding. Knowledge and Decisions in Health Telematics: The Next Decade, 12, 103.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 23}, page_content='[93] Rassinoux, A. M., Baud, R. H., & Scherrer, J. R. (1992). Conceptual graphs model \\nextension for knowledge representation of medical texts. MEDINFO, 92, 1368-1374. \\n[94] Morel-Guillemaz, A. M., Baud, R. H., & Scherrer, J. R. (1990). Proximity Processing of \\nMedical Text. In Medical Informatics Europe’90 (pp. 625-630). Springer Berlin Heidelberg. \\n[95] Rassinoux, A. M., Michel, P. A., Juge, C., Baud, R., & Scherrer, J. R. (1994). Natural \\nlanguage processing of medical texts within the HELIOS environment. Computer methods \\nand programs in biomedicine, 45, S79-96. \\n[96] Rassinoux, A. M., Juge, C., Michel, P. A., Baud, R. H., Lemaitre, D., Jean, F. C., ... & \\nScherrer, J. R. (1995, June). Analysis of medical jargon: The RECIT system. In Conference \\non Artificial Intelligence in Medicine in Europe (pp. 42-52). Springer Berlin Heidelberg. \\n[97] Friedman, C., Cimino, J. J., & Johnson, S. B. (1993). A conceptual model for clinical \\nradiology reports. In Proceedings of the Annual Symposium on Computer Application in \\nMedical Care (p. 829). American Medical Informatics Association. \\n[98] \"Natural Language Processing.\" Natural Language Processing RSS. N.p., n.d. Web. 23 \\nMar. 2017.   \\n[99] [Srihari S. Machine Learning: Generative and Discriminative Models. 2010. http:// \\nwww.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf (accessed 31 May \\n2011).] \\n[100] [Elkan C. Log-Linear Models and Conditional Random Fields. 2008. http://cseweb. \\nucsd.edu/welkan/250B/cikmtutorial.pdf (accessed 28 Jun 2011). 62. Hearst MA, Dumais ST, \\nOsman E, et al. Support vector machines] \\n[101] [Jurafsky D, Martin JH. Speech and Language Processing. 2nd edn. Englewood Cliffs, \\nNJ: Prentice-Hall, 2008.] \\n[102] [Sonnhammer ELL, Eddy SR, Birney E, et al. Pfam: Multiple sequence alignments and \\nHMM-profiles of protein domains. Nucleic Acids Res 1998;26:320] \\n[103] [Sonnhammer, E. L., Eddy, S. R., Birney, E., Bateman, A., & Durbin, R. (1998). Pfam: \\nmultiple sequence alignments and HMM-profiles of protein domains. Nucleic acids \\nresearch, 26(1), 320-322] \\n[104] Systems, RAVN. \"RAVN Systems Launch the ACE Powered GDPR Robot - Artificial \\nIntelligence to Expedite GDPR Compliance.\" Stock Market. PR Newswire, n.d. Web. 19 \\nMar. 2017. \\n [105] \"Here\\'s Why Natural Language Processing is the Future of BI.\" SmartData Collective. \\nN.p., n.d. Web. 19 Mar. 2017'),\n",
       " Document(metadata={'producer': 'Microsoft® Office Word 2007', 'creator': 'Microsoft® Office Word 2007', 'creationdate': '2017-08-17T12:10:59+05:30', 'source': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'file_path': '../data/pdf_files/Natural_language_processing_state_of_the_art_curre.pdf', 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': 'Diksha Khurana', 'subject': '', 'keywords': '', 'moddate': '2017-08-17T12:10:59+05:30', 'trapped': '', 'modDate': \"D:20170817121059+05'30'\", 'creationDate': \"D:20170817121059+05'30'\", 'page': 24}, page_content='[106] \"Using Natural Language Processing and Network Analysis to Develop a Conceptual \\nFramework for Medication Therapy Management Research.\" AMIA ... Annual Symposium \\nproceedings. AMIA Symposium. U.S. National Library of Medicine, n.d. Web. 19 Mar. 2017 \\n[107] Ogallo, W., & Kanter, A. S. (2017, February 10). Using Natural Language Processing \\nand Network Analysis to Develop a Conceptual Framework for Medication Therapy \\nManagement \\nResearch. \\nRetrieved \\nApril \\n10, \\n2017, \\nfrom \\nhttps://www.ncbi.nlm.nih.gov/pubmed/28269895?dopt=Abstract \\n[108] Ochoa, A. (2016, May 25). Meet the Pilot: Smart Earpiece Language Translator. \\nRetrieved April 10, 2017, from https://www.indiegogo.com/projects/meet-the-pilot-smart-\\nearpiece-language-translator-headphones-travel'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/Siddhant_Kochhar_Resume.pdf', 'file_path': '../data/pdf_files/Siddhant_Kochhar_Resume.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Siddhant_Kochhar_Resume', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='SIDDHANT KOCHHAR (22BCE11684) \\nB.Tech (Computer Science and Engineering)  \\nEmail: siddhantkochhar2022@vitbhopal.ac.in | Phone: 8965897560 \\nLinkedIn: https://www.linkedin.com/in/siddhant-kochhar/ \\nLeetCode:  https://leetcode.com/u/Siddhant_Kochhar/  \\nGitHub: https://github.com/Siddhant-kochhar  \\n \\n \\n \\nACADEMICS \\n \\nQualification \\nInstitute \\nBoard / University \\n% / CGPA \\nYear \\nB.Tech (CSE - 7th sem) \\nXII \\nVellore Institute of Technology \\nMaharishi Vidya Mandir, Jabalpur \\nVIT University \\nCBSE \\n8.81/10 \\n78.6% \\n2026 \\n2022 \\nX \\nMaharishi Vidya Mandir, Jabalpur \\nCBSE \\n79.2% \\n2020 \\n \\nCertifications  \\n●\\u200b\\nAWS Academy Graduate - AWS Academy Cloud Architecting \\n●\\u200b\\nCloud Computing by IIT Kharagpur offered through NPTEL \\n●\\u200b\\nThe Bits and Bytes of Computer Networks by Google offered through Coursera \\n●\\u200b\\nPython A-Z™: Python For Data Science offered by Udemy \\n2025 \\n2024 \\n2023 \\n2023 \\n \\nAchievements \\n●\\u200b\\nSelected among top 500 students globally for the Ericsson Academic Training Program \\n●\\u200b\\nAwarded a scholarship under Deen Dayal SPARSH Yojana by India Post \\n2024 \\n2017 \\n \\n \\nINTERNSHIP                                                                                                                                                             2 MONTHS \\n \\n \\nMagicPin (Hybrid) \\nGen AI Intern \\n May 2025 - June 2025 \\nRoles and \\nResponsibilities \\nGen AI & Conversational Interfaces \\n●\\u200b\\nDeveloped a WhatsApp-based GenAI chatbot enabling users to search for food, fashion, etc. \\n●\\u200b\\nIntegrated personalised food recommendation engine using user context and historical data \\n●\\u200b\\nEnhanced Magicpin’s personal support bot with conversational AI capabilities \\n●\\u200b\\nCollaborated with tech and product teams to ensure scalable integration across key user workflows  \\n \\n \\nPROJECTS \\n \\nGet.Fit \\n●\\u200b\\nTech Stack - Python, FastAPI, Google Gemini, Google Fit, MongoDB, HTML, CSS \\n●\\u200b\\nDeveloped a smart health tracking app integrated with Google Fit for vitals monitoring \\n●\\u200b\\nAdded a feature to send regular summaries and real-time alerts for proactive health management \\nAI Tutor \\n●\\u200b\\nTech Stack - Python, FastAPI, MongoDB, OpenAI \\n●\\u200b\\nDeveloped an AI tutor that assesses the user’s proficiency to generate personalized responses \\n●\\u200b\\nDeveloped a YouTube video summarizer, transforming lengthy video content into concise notes \\nEventGenie \\n●\\u200b\\nTech Stack - Python, FastAPI, Google Gemini, MongoDB, MCP, Redis, Google Places, HTML  \\n●\\u200b\\nDeveloped EventGenie, an AI event planner with venues, budgeting, restaurants, and activities. \\n●\\u200b\\nIntegrated Google Gemini and Google Places API for intelligent venues, budgeting, and scheduling. \\nNote-Taking App \\n●\\u200b\\nTech Stack - Python, FastAPI, MongoDB, Bootstrap \\n●\\u200b\\nDeveloped a basic note-taking application using CRUD APIs to learn the FastAPI framework \\n \\nCORE COMPETENCIES \\n \\nData Structure and Algorithm, Operating Systems, Database Management, GenAI, Cloud Computing, Computer Networking \\nProgramming \\nPython, FastAPI, Flask, Pandas, Numpy, HTML, CSS, Bootstrap, Java \\nDatabases/Cache \\nMongoDB, SQL, Redis \\nCloud \\nAWS, GCP \\nAI/Tools \\nOpenAI APIs, Gemini, MCP server, GitHub, Cursor, Postman, MS Office, Canva \\nSoft Skills \\nLeadership, Communication, Teamwork, Problem Solving, Analytical Skills, Learning Agility \\n \\nPOSITIONS OF RESPONSIBILITY \\n \\nMVM, Jabalpur \\n●\\u200b\\nVice Captain, Assembly Committee \\n●\\u200b\\nJunior Prefect of Parashar House \\n \\n \\nCO-CURRICULAR & EXTRACURRICULAR ACTIVITIES \\n \\nTechnical \\n●\\u200b\\nMember, Google Developers Group \\n●\\u200b\\nCompetitive Programming - LeetCode, HackerRank, Code Chef \\n \\nSocial/Sports \\n●\\u200b\\nKarate - Yellow belt \\n \\nInterests / Hobbies \\n●\\u200b\\nTravelling, Cooking, Gaming \\n \\n \\n \\nVIT UNIVERSITY | BATCH OF 2026'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 0}, page_content='Natural Language Processing Journal 7 (2024) 100062\\nContents lists available at ScienceDirect\\nNatural Language Processing Journal\\njournal homepage: www.elsevier.com/locate/nlp\\nUnderstanding latent affective bias in large pre-trained neural language\\nmodels\\nAnoop Kadan a,∗, Deepak P. b, Sahely Bhadra c, Manjary P. Gangan d, Lajish V.L. d\\na School of Psychology, Queen’s University Belfast, UK\\nb School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, UK\\nc Computer Science and Engineering, IIT Palakkad, India\\nd Department of Computer Science, University of Calicut, India\\nA R T I C L E\\nI N F O\\nKeywords:\\nAffective bias in NLP\\nFairness in NLP\\nPre-trained language models\\nTextual emotion detection\\nDeep learning\\nA B S T R A C T\\nGroundbreaking inventions and highly significant performance improvements in deep learning based Natural\\nLanguage Processing are witnessed through the development of transformer based large Pre-trained Language\\nModels (PLMs). The wide availability of unlabeled data within human generated data deluge along with self-\\nsupervised learning strategy helps to accelerate the success of large PLMs in language generation, language\\nunderstanding, etc. But at the same time, latent historical bias/unfairness in human minds towards a particular\\ngender, race, etc., encoded unintentionally/intentionally into the corpora harms and questions the utility and\\nefficacy of large PLMs in many real-world applications, particularly for the protected groups. In this paper,\\nwe present an extensive investigation towards understanding the existence of ‘‘Affective Bias’’ in large PLMs\\nto unveil any biased association of emotions such as anger, fear, joy, etc., towards a particular gender, race\\nor religion with respect to the downstream task of textual emotion detection. We conduct our exploration of\\naffective bias from the very initial stage of corpus level affective bias analysis by searching for imbalanced\\ndistribution of affective words within a domain, in large scale corpora that are used to pre-train and fine-tune\\nPLMs. Later, to quantify affective bias in model predictions, we perform an extensive set of class-based and\\nintensity-based evaluations using various bias evaluation corpora. Our results show the existence of statistically\\nsignificant affective bias in the PLM based emotion detection systems, indicating biased association of certain\\nemotions towards a particular gender, race, and religion.\\n1. Introduction\\nRecently, large scale Natural Language Processing (NLP) models are\\nbeing increasingly deployed in many real-world applications within\\nalmost all domains such as health-care, business, legal systems, etc.,\\nVelupillai et al. (2018), Soni and Roberts (2020), Mishev et al. (2020),\\nDale (2019), Rahman and Siddiqui (2019) and Rahman and Siddiqui\\n(2021) due to its efficacy to make data-driven decisions and capability\\nof natural language understanding even better than humans1 (He et al.,\\n2021). Transformer based large Pre-trained Language Models (PLMs)\\nhave been hugely influential in NLP due to their capability to gener-\\nate powerful contextual representations. PLMs are mostly built based\\non a self-supervised learning strategy that highly relies on unlabeled\\ndata abundantly available from the human generated data deluge (He\\net al., 2021). But, since this historical data of textual write-ups has its\\nroots within human thought, they often reflect latent social stereotypes\\n(Suresh and Guttag, 2021; Garg et al., 2018). For example, the Social\\n∗Correspondence to: School of Psychology, Queen’s University Belfast, Northern Ireland, UK.\\nE-mail addresses: a.kadan@qub.ac.uk (A. Kadan), deepaksp@acm.org (Deepak P.), sahely@iitpkd.ac.in (S. Bhadra), manjaryp_dcs@uoc.ac.in\\n(M. P. Gangan), lajish@uoc.ac.in (Lajish V.L.).\\n1 https://www.infoq.com/news/2021/01/google-microsoft-superhuman/.\\nRole Theory by Eagly and Steffen (1984) demonstrates that the idea of\\ngender stereotype develops from perceivers’ observations, associating\\nthe capabilities and personality attributes of different genders with the\\nactivities in which they engage in their day-to-day lives over time,\\nbuilding rigid stereotypes in human minds and their writings, on how\\nthese genders behave (e.g. women are highly emotional), where they\\nwork (e.g. women preferred in children’s daycare), etc. Hence the data\\nfrom such human generated data repositories eventually convey these\\nstereotypes as linguistic biases through the NLP algorithms (Raza et al.,\\n2024), especially those built on large PLMs that utilize huge amounts\\nof data (Suresh and Guttag, 2021).\\nIn this direction, investigation of ‘‘Affective Bias’’ in NLP is a re-\\ncent stream of research to study the existence of any unfair/biased\\nassociation of emotions (anger, fear, joy, etc.) or sentiments (positive,\\nnegative, etc.) towards underrepresented or protected groups in a\\ndomain, that may lead affective computing systems such as sentiment\\nor emotion detection systems to deliver unfavorable outcomes for these\\nhttps://doi.org/10.1016/j.nlp.2024.100062\\nReceived 5 November 2023; Received in revised form 6 January 2024; Accepted 1 March 2024\\n2949-7191/© 2024 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY-NC license\\n(http://creativecommons.org/licenses/by-nc/4.0/).'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 1}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nTable 1\\nDifferences in predicted emotion classes/intensities between sentences that differ only on a target term\\nrepresenting various social groups, indicating existence of ‘Affective Bias’.\\nSentence\\nEmotion prediction\\nMen know how to drive\\nJoy\\nWomen know how to drive\\nFear\\nWhite people are very ambitious as a whole\\nJoy\\nBlack people are very ambitious as a whole\\nAnger\\nTasha fell in love with the Christian man’s looks\\nJoy\\nTasha fell in love with the Muslim man’s looks\\nFear\\nHe eventually wants to become a doctor\\nJoy: 0.700\\nShe eventually wants to become a doctor\\nJoy: 0.506\\nMy man friend made me feel alarmed\\nFear: 0.694\\nMy woman friend made me feel alarmed\\nFear: 0.750\\nMy queer friend made me feel alarmed\\nFear: 0.940\\nBlack people can dance very well\\nJoy: 0.491\\nWhite people can dance very well\\nJoy: 0.746\\nIt is always the Christian people who think their films are the best\\nJoy: 0.996\\nIt is always the Muslim people who think their films are the best\\nJoy: 0.488\\nprotected groups (Anoop et al., 2022). For instance, a model consis-\\ntently associating women with a different class of emotion or same\\nemotion differing in emotion intensities vis-a-vis predictions for male\\n(Shields, 2002) could be seen as a manifestation of affective bias.\\nSimilarly, association of a particular religion always with a specific\\nemotion (Abid et al., 2021a) represents affective bias too. A real world\\nscenario of affective bias is the case of Google sentiment analyzer\\njudging that being gay is bad by assigning high negative sentiments\\nto sentences such as ‘I’m a gay black woman’, ‘I’m a homosexual’, etc.,.2\\nFor better understandability of affective bias, we illustrate in Table 1,\\na sample set of affectively biased emotion predictions from PLM based\\ntextual emotion detection models constructed in this study for affective\\nbias analysis (detailed explanation of the models are provided in Sec-\\ntion 4.1). The first set in the table demonstrates affective bias due to\\ndifferences in predicted emotion classes, whereas the second set shows\\naffective bias due to differences in predicted emotion intensities.\\nSimilar to other general algorithmic biases like gender bias, racial\\nbias, etc., a possible stimuli to affective biases are the latent emotion\\nbased stereotypes about different social groups in the data. Studies\\nreport that such emotion based stereotyping influence socialization\\nof emotions leading to propagation of stereotypes such as associating\\nwomen’s (or men’s) experiences and expressions being aligned with\\nfear and sadness (or anger and pride) (Plant et al., 2000). Similarly,\\naffective bias within systems could facilitate a higher association of\\nblack women to the emotion anger when considering emotions with\\nthe domains race and gender (Ashley, 2014). In addition to biased\\ndata, another reason for bias is based on how the model/algorithmic\\ndesign considers or treats the underrepresented or protected attributes\\nconcerning a domain (Hooker, 2021). Similar to any other general\\nsocial biases, the existence of these affective biases make textual af-\\nfective computing systems generate unfair or biased decisions that can\\nharm its utility towards socially marginalized populations by denying\\nopportunities/resources or by false portrayal of these groups when\\ndeployed in the real-world. Hence, understanding affective bias in NLP\\n2 https://www.vice.com/en/article/j5jmj8/google-artificial-intelligence-\\nbias.\\nplays a vital role in achieving algorithmic fairness, by protecting the\\nsocio-political and moral equality of marginalized groups.\\nIn this context, we present an extensive experimental analysis to\\nunderstand and illustrate the existence of latent ‘‘Affective Bias’’ in\\ntransformer based large PLMs3 with respect to the downstream task\\nof textual emotion detection. Hence, we set our research question: Do\\npredictions made by large PLM based textual emotion detection sys-\\ntems systematically or consistently exemplify ‘Affective Bias’ towards\\ndemographic groups? Our investigation of affective bias in large PLMs\\nprimarily aims to identify the existence of gender, racial, and religious\\naffective biases and set aside the task of affective bias mitigation in\\nthe scope for future work. We start with an exploration of corpus level\\naffective bias or affect imbalance in corpus to find out any biased emo-\\ntion associations in the large scale corpora that are used to pre-train and\\nfine-tune the PLMs, by analyzing the distribution of emotions or their\\nassociations with demographic target terms (e.g., Islam, Quran) related\\nto a social group (e.g., Muslim) concerning a domain (e.g., Religion).\\nLater, we explore the prediction level affective bias in four popular\\ntransformer based PLMs, BERT (Bidirectional Encoder Representation\\nfrom Transformers) (Devlin et al., 2019), OpenAI GPT-2 (Generative\\nPre-trained Transformer) (Radford et al., 2019), XLNet (Yang et al.,\\n2019), and T5 (Text-to-Text Transfer Transformer) (Raffel et al., 2020),\\nthat are fine-tuned using a popular corpora SemEval-2018 EI-oc (Mo-\\nhammad et al., 2018) for the task of textual emotion detection. To\\nquantify prediction level affective bias, we subject the PLMs to an\\nextensive set of class-based and intensity-based evaluations using three\\ndifferent evaluation corpora EEC (Kiritchenko and Mohammad, 2018),\\nBITS (Venkit and Wilson, 2021) and CSP (Nangia et al., 2020). A\\ndetailed sketch of the overall analysis is shown in Fig. 1.\\nThe rest of the paper is organized as follows. Section 2 presents\\nthe relevant related works. Section 3 presents corpus level affective\\n3 Even though, the current interpretation of large language models seems to\\nbe changing to billions of parameters (for e.g., LLaMA (Touvron et al., 2023),\\nFLAN-T5 XXL (Chung et al., 2022), etc.), there are works that utilize the term\\n‘large PLMs’ to indicate PLMs trained on millions of parameters (e.g., Navigli\\net al. (2023)). In this study also, we use the term ‘large PLMs’ in the context\\nof having a PLM trained on millions of parameters.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 2}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nFig. 1. Workflow of Affective bias analysis.\\nbias analysis with corresponding methodology and results. Section 4\\npresents the exploration towards prediction level affective bias with\\ndetails of constructing PLM based textual emotion detection model,\\nmethodology of analysis, and the corresponding results. Section 5\\npresents a discussion based on the entire results and finally, Section 6\\ndraws the conclusions.\\n2. Related works\\nHere we review two categories of algorithmic bias analysis pertinent\\nto our work, i.e., the general affect-agnostic bias analysis and affect-\\noriented bias analysis, and demarcate our work from these related\\nworks.\\n2.1. General affect agnostic bias analysis\\nRecent works in the literature have focused on several approaches\\nto identify the existence of latent biases in PLMs by inspecting at\\nvarious levels, commencing from bias analysis at the corpus level to the\\ndownstream-task level (Anoop et al., 2022; Suresh and Guttag, 2021).\\nWorks addressing bias at the corpus level analyze the terms relating\\na domain and their associations with key terms against which bias\\nis examined, e.g., the association between gender and stereotypically\\ngendered occupation terms (Bordia and Bowman, 2019; Tan and Celis,\\n2019). In model level analysis, bias are quantified using various metrics\\ndepending on the tasks, where evaluating geometry of the word vector\\nspace (Bolukbasi et al., 2016), performing association tests such as\\nWord Embedding Association Test (Caliskan et al., 2017) and Sentence\\nEncoder Association Test (May et al., 2019), measuring bias of classifi-\\ncation tasks using demographic parity and equal opportunity (Du et al.,\\n2021), etc., are popular approaches in the literature. At the downstream\\ntask level, bias is quantified by comparing the performance scores of a\\nmodel for a set of sentence pairs in an evaluation corpus that differs\\nonly on target terms in which the domain of bias is being studied.\\nFor example, comparing performances of a model for gender-swapped\\nsentences like ‘She is here’ versus ‘He is here’, where the model exhibits\\ngender bias if it produces different performance scores for both sets\\nof sentence pairs. Bias identification at the downstream task level is\\nexplored for a variety of tasks like identification of toxic comments\\n(Dixon et al., 2018), text generation (Nadeem et al., 2021), coreference\\nresolution (Zhao et al., 2018; Lu et al., 2020), etc.\\n2.2. Affect-oriented bias analysis\\nMost affect-oriented bias analysis studies in the literature predom-\\ninantly focus on the coarse-grained sentiment perspective of these\\nbiases (i.e. positive, negative, and neutral sentiments), and that too\\nmostly specific to gender domain (Yang et al., 2021; Bhaskaran and\\nBhallamudi, 2019; Rozado, 2020; Shen et al., 2018; Sweeney and\\nNajafian, 2020). But, affective bias in context of fine-grained emotion\\nclasses like anger, fear, joy, etc., and the variability of these biases\\nin diverse domains such as religion, politics, race, or intersectional\\nbiases, are not well explored (Anoop et al., 2022), except in Kiritchenko\\nand Mohammad (2018) and Venkit and Wilson (2021). In Kiritchenko\\nand Mohammad (2018) Kiritchenko and Mohammad identify affective\\nbias in the emotion prediction systems developed for the shared task\\nSemEval-2018 Task 1 Affect in Tweets, and in Venkit and Wilson (2021)\\nVenkit et al. identifies affective bias in the domain of persons with\\ndisabilities in sentiment analysis and toxicity classification models; both\\nthese works use a synthetics evaluation corpus to identify affective bias.\\nAffect-oriented bias analysis are seen to be conducted in lexicon\\nand deep learning based sentiment analysis systems (Shen et al., 2018;\\nZhiltsova et al., 2019), and in non-contextual word embeddings such as\\nFastText, GloVe, and Word2Vec to address bias in sentiment analysis\\nand toxicity classification (Sweeney and Najafian, 2020), age-related\\nbias (Díaz et al., 2018) and other underreported bias types (Rozado,\\n2020). Recently several works also address bias in contextual repre-\\nsentations of large PLMs. But most of these works in PLMs address\\ngeneral affect-agnostic biases (Liang et al., 2021; Nadeem et al., 2021;\\nTan and Celis, 2019; Zhao et al., 2019), very few works address affect-\\noriented biases in PLMs through sentiment perspective (Bhaskaran and\\nBhallamudi, 2019; Yang et al., 2021; Huang et al., 2020), and to our\\nbest knowledge only the work in Mao et al. (2022) investigates affective\\nbias in large PLMs through the perspective of fine-grained emotions, so\\nfar, and that too specifically in prompt-based sentiment and emotion\\ndetection tasks.\\n2.3. Our work in context\\nTo put our work in context, we conduct experiments to identify\\naffective bias in large PLMs through the perspective of fine-grained\\nemotions. Hence, as a natural first step, we consider textual emotion\\ndetection systems, unlike the considerable amount of bias analysis\\nworks in large PLMs relying on text generation, coreference resolution,\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 3}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nprompt-based classification, etc., Mao et al. (2022), Liang et al. (2021),\\nNadeem et al. (2021) and Huang et al. (2020). Our work, in particular,\\nconsiders investigating affective bias in transformer based large PLMs\\ndue to their wide applicability in developing textual emotion detection\\nsystems (Acheampong et al., 2021). Distinct from the recent work (Mao\\net al., 2022) that addresses affective bias in PLMs with respect to label-\\nword, prompt template, etc., specifically focusing on prompt-based\\nsentiment and emotion detection, our work investigates affective bias\\nin four different PLMs with respect to the domains gender, race, and\\nreligion, focusing on fine-tuning based emotion classification. Unlike\\nthe works (Venkit and Wilson, 2021; Kiritchenko and Mohammad,\\n2018) addressing affective bias, we start our investigation from the very\\ninitial stage of corpus level affective bias analysis, inspired by the works\\n(Bordia and Bowman, 2019; Tan and Celis, 2019) that address corpus\\nlevel general affect-agnostic biases, and later we progress towards\\nanalyzing affective bias in predictions of the PLM based textual emotion\\ndetection models. We conduct a much broader intensity based and class\\nbased affective bias analysis using a set of synthetic (template based)\\nevaluation corpora as well as non-synthetic (crowdsourced) evaluation\\ncorpus that much more suits the real-world scenario.\\n3. Corpus level affective bias\\nThe existence of bias in PLM based language processing systems are\\nobserved due to many sources such as data, annotation, representa-\\ntions, model, etc. (Anoop et al., 2022; Hovy and Prabhumoye, 2021).\\nA substantial amount of works that address general social biases on\\ngender and race lines report the existence of data bias from innate\\nhistorical biases as the most primeval source of bias (Corbett-Davies\\net al., 2017; Bordia and Bowman, 2019; Tan and Celis, 2019; Zhao\\net al., 2019). To the best of our knowledge, this is the first attempt\\nthat explore affective bias in large scale textual corpora utilized by\\nPLMs. Hence, as an initial step to explore the affective bias, we conduct\\nexperiments to understand the existence of affective bias if any, in the\\npre-training corpora that are integral ingredients of large PLMs and\\nfine-tuning corpora used to build the textual emotion detection systems.\\nData quality issues, uneven distributions of data, and class imbal-\\nances that target marginalized groups, etc., are the root factors that\\ncontribute towards data bias (Navigli et al., 2023; Hovy and Prab-\\nhumoye, 2021; Subramanian et al., 2021; Anoop et al., 2022). Many\\nworks that address affect agnostic biases focus on exploring data bias by\\nunderstanding any uneven distributions of the target terms associated\\nwithin the domain of interest (Tan and Celis, 2019; Zhao et al., 2019).\\nMotivated by these lines of works, as an initial attempt to unveil the\\ncorpus level affective bias, we follow this simple approach of analyzing\\nthe distributions of affective target terms. A detailed description of pre-\\ntraining and fine-tuning corpora, the method to measure corpus level\\naffective bias, and the analysis of corpus level affective bias are given\\nbelow.\\n3.1. Training corpora\\nOur choice of large scale datasets for corpus level affective bias\\nanalysis hinges on the large PLMs, BERT (Devlin et al., 2019), GPT-2\\n(Radford et al., 2019), XLNet (Yang et al., 2019), and T5 (Raffel et al.,\\n2020). BERT is trained on Wikipedia dump (WikiEn)4 and BookCorpus\\n(Zhu et al., 2015), GPT-2 is trained on WebText (Radford et al., 2019),\\nXLNet is trained on WikiEn, BookCorpus, Giga5,5 ClueWeb6 and Com-\\nmon Crawl,7 and T5 is trained on Colossal Clean Crawled Corpus (C4).8\\n4 https://dumps.wikimedia.org/enwiki/.\\n5 https://catalog.ldc.upenn.edu/LDC2011T07.\\n6 https://lemurproject.org/clueweb12/index.php.\\n7 http://commoncrawl.org/.\\n8 https://www.tensorflow.org/datasets/catalog/c4.\\nTable 2\\nDetails of training corpora used for corpus level affective bias analysis.\\nCorpus\\nSize\\nNumber of\\nPLM\\nsentences\\nBERT\\nGPT-2\\nXLNeta\\nT5\\nPre-training corpora\\nWikiEn\\n19.8 GB\\n95 917 189\\n✓\\n✓\\nBookCorpus\\n6.19 GB\\n91 025 872\\n✓\\n✓\\nWebText-250\\n620 MB\\n5 314 965\\n✓\\nC4-Val\\n731 MB\\n4 959 563\\n✓\\nFine-tuning corpora\\nSemEval-2018\\n925 KB\\n10 030\\na Giga5, ClueWeb, & Common Crawl used to pre-train XLNet are omitted.\\nFrom these set of large-scale pre-training datasets, we chose WikiEn,9\\nBookCorpus, WebText, and C4, for our study. The details regarding size\\nof these corpora and number of sentences are shown in Table 2. We\\nomit Giga5 and ClueWeb due to their unavailability as open-source\\ncorpora and Common Crawl as it is reported to have significant data\\nquality issues due to a large number of unintelligible document content\\n(Trinh and Le, 2018; Radford et al., 2019). Since BookCorpus10 is no\\nlonger hosted by the authors, we choose its open version available in\\nHugging Face.11 We make use of the partially released 250K documents\\nfrom WebText test set, similar to Tan and Celis (2019), since WebText\\ncorpora has not been fully released and call it WebText-250.12 As\\nthe train split of C4 corpus is very large (305 GB with 364868892\\ndocuments) and cumbersome to process, we use only a part of the\\ncorpus, i.e., the validation split, and call it C4-Val. Apart from the above\\nmentioned pre-training datasets, we also consider SemEval-2018 EI-oc\\n(Mohammad et al., 2018) that is used to fine-tune the textual emotion\\ndetection model, for our analysis.\\n3.2. Measuring corpus level affective bias\\nInspired by the recent methods to identify gender bias in datasets\\nwith respect to occupations (Tan and Celis, 2019; Zhao et al., 2019), we\\nidentify the existence of affective bias in the large scale corpora used to\\ntrain large PLMs with respect to various domains such as gender, race,\\nand religion. That is, for a corpus, we identify any imbalances in the\\ndistribution of emotions, or any imbalanced association of the emotions\\ntowards social groups within a domain. Accordingly, for each corpus,\\nwe measure the occurrence of emotion terms representing or related\\nto an emotion and their co-occurrence or association with target terms\\nrepresenting a social group in a domain.\\nAlgorithm 1 illustrates the method of computing occurrence and\\nco-occurrence for a training corpora 𝐷that is considered as a set of\\nsentences [𝑆1, 𝑆2, 𝑆3, …] derived from documents in the corpus, where\\neach sentence consists of a sequence of words [𝑤1, 𝑤2, 𝑤3, …]. The algo-\\nrithm sifts through each word in the sentences of the corpus 𝐷. Once\\na word belonging to the set of emotion terms related to an emotion\\n𝐸(i.e., 𝐸𝑡𝑒𝑟𝑚𝑠) is encountered in a sentence, the algorithm increments\\nthe occurrence of that emotion 𝑜𝑐𝑐𝐸, for that corpus. Similarly in a\\nsentence, once a word related to the emotion 𝐸co-occurs with a\\nterm belonging to the set of target terms related to a social group 𝑇\\nin a domain (i.e., 𝑇𝑡𝑒𝑟𝑚𝑠), the algorithm increments the co-occurrence\\nof that emotion with the corresponding social group 𝑐𝑜𝑜𝑐𝑐𝑇\\n𝐸, for that\\ncorpus. For example, we increment the occurrence of the emotion Joy\\n(i.e., 𝑜𝑐𝑐𝑗𝑜𝑦), for a corpus, once an emotion term related to Joy like\\n‘happy’, ‘bliss’, ‘cheer’, etc., is encountered in a sentence of the corpus.\\nWe increment the co-occurrence of Joy-Male (i.e., 𝑐𝑜𝑜𝑐𝑐𝑚𝑎𝑙𝑒\\n𝑗𝑜𝑦), for the\\n9 Latest Wikipedia dump (date: 02/June/2022), extracted using https://\\ngithub.com/attardi/wikiextractor.\\n10 https://yknzhu.wixsite.com/mbweb.\\n11 https://huggingface.co/datasets/bookcorpus.\\n12 https://github.com/openai/gpt-2-output-dataset.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 4}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\ncorpus, if an emotion term related to Joy co-occurs with target terms\\nrelated to the social group Male like ‘husband’, ‘boy’, ‘brother’, etc.,\\nand increment the co-occurrence of Joy-Female (i.e., 𝑐𝑜𝑜𝑐𝑐𝑓𝑒𝑚𝑎𝑙𝑒\\n𝑗𝑜𝑦\\n) if an\\nemotion term related to Joy co-occurs with target terms related to the\\nsocial group Female like ‘wife’, ‘girl’, ‘sister’, etc., in a sentence of the\\ncorpus. Finally, for each social group in a domain, the co-occurrence\\nvalues with respect to each emotion are expressed in percentages.\\nAlgorithm 1: Occurrence and Co-occurrence\\ninput\\n: Corpus 𝐷\\nEmotion terms for emotion 𝐸(𝐸𝑡𝑒𝑟𝑚𝑠)\\nTarget terms for social group 𝑇(𝑇𝑡𝑒𝑟𝑚𝑠)\\noutput\\n: Emotion occurrence 𝑜𝑐𝑐𝐸\\nEmotion and Social group co-occurrence 𝑐𝑜𝑜𝑐𝑐𝑇\\n𝐸\\n1 Let 𝐷= [𝑆1, 𝑆2, … , 𝑆𝑚] and 𝑆= [𝑤1, 𝑤2, … , 𝑤𝑛] ;\\n2 initialize 𝑜𝑐𝑐𝐸= 0; 𝑐𝑜𝑜𝑐𝑐𝑇\\n𝐸= 0; 𝑓𝑙𝑎𝑔= 𝐹𝑎𝑙𝑠𝑒;\\n3 for (𝑗= 1; 𝑗≤𝑚; 𝑗+ +) do\\n4\\nfor (𝑖= 1; 𝑖≤𝑛; 𝑖+ +) do\\n5\\nif (𝑤𝑖∈𝐸𝑡𝑒𝑟𝑚𝑠) then\\n6\\n𝑓𝑙𝑎𝑔= 𝑇𝑟𝑢𝑒;\\n7\\n𝑜𝑐𝑐𝐸= 𝑜𝑐𝑐𝐸+ 1;\\n8\\nbreak;\\n9\\nend\\n10\\nend\\n11\\nfor (𝑖= 1; 𝑖≤𝑛; 𝑖+ +) do\\n12\\nif (𝑤𝑖∈𝑇𝑡𝑒𝑟𝑚𝑠and 𝑓𝑙𝑎𝑔= 𝑇𝑟𝑢𝑒) then\\n13\\n𝑐𝑜𝑜𝑐𝑐𝑇\\n𝐸= 𝑐𝑜𝑜𝑐𝑐𝑇\\n𝐸+ 1 ;\\n14\\nbreak;\\n15\\nend\\n16\\nend\\n17 end\\n18 output 𝑜𝑐𝑐𝐸, 𝑐𝑜𝑜𝑐𝑐𝑇\\n𝐸\\nTo conduct this study on corpus level affective bias, we maintain\\na list of emotion terms (or affective terms) for the basic emotions\\n𝐸= {𝑎𝑛𝑔𝑒𝑟, 𝑓𝑒𝑎𝑟, 𝑗𝑜𝑦, 𝑠𝑎𝑑𝑛𝑒𝑠𝑠}, because our emotion prediction models\\n(discussed in Section 4.1, to identify affective bias in model predic-\\ntions) relies on these categories of basic emotions. Hence, initially, we\\nprocure a list of affective terms collectively from Parrott’s primary,\\nsecondary, and tertiary emotions,13 and refer the works Kiritchenko and\\nMohammad (2018) and Venkit and Wilson (2021), to represent these\\nbasic emotions. Later, we extend this list of affective terms by including\\nlinguistic inflections of each word in the list using Merriam-Webster14\\ndictionary and an automated python package pyinflect.15 As a result\\nthe entire list contains 735 affective terms (given in supplementary\\nmaterial), where 162 represent anger, 143 fear, 222 joy, and 208\\nsadness.\\nA similar procedure is carried out to procure target terms related to\\na social group within gender, race, and religion, the domains that are\\nconsidered in this study. In domain gender, the target terms considered\\nrepresent three social groups 𝑇= {𝑀, 𝐹, 𝑁𝑏} for Male, Female, and\\nNon-binary groups. Similarly in domain race, we consider European\\nAmerican and African American social groups i.e., 𝑇= {𝐸𝐴, 𝐴𝐴}, and\\nfor religion, we consider Christian, Muslim, and Jewish social groups\\ni.e., 𝑇\\n= {𝐶ℎ, 𝑀𝑢, 𝐽𝑤}. An initial list of target terms representing\\nthese social groups is prepared collectively by referring to the works\\n(Bolukbasi et al., 2016; Lu et al., 2020; Guo and Caliskan, 2021;\\nNadeem et al., 2021; Liang et al., 2021; Kaneko and Bollegala, 2022),\\nwhich is later expanded by adding linguistic inflections. As these works\\ndo not consider target terms related to the non-binary social group\\nin the gender domain, we manually curated the corresponding target\\n13 https://en.wikipedia.org/wiki/Emotion_classification#Parrott’s_emotions_\\nby_groups.\\n14 https://www.merriam-webster.com/.\\n15 https://pypi.org/project/pyinflect/.\\nterms from various articles and web resources (e.g. Center (2022)) and\\nverified these terms with the help of an expert in gender studies. The\\nentire list contains 507, 167, and 332 target terms in the domains\\nof gender, race, and religion, respectively (given in supplementary\\nmaterial), with 199 male, 211 female, and 97 non-binary target terms\\nfor the gender domain, 82 African American and 85 European American\\ntarget terms for the racial domain, and 122 Muslim, 111 Jewish, and\\n99 Christian target terms for the religious domain.\\n3.3. Results and analysis of corpus level affective bias\\nIn this section, we present the results of occurrence of emotions\\nin the corpora and their co-occurrence with social groups in various\\ndomains of gender, race, and religion to analyze corpus level affective\\nbias.\\n3.3.1. Occurrence of emotions in the corpora\\nResults of the occurrence statistics of emotions for our corpus level\\naffective bias analysis are shown in Table 3. The trends of emotion\\noccurrence illustrate that, for all the corpora, the occurrence of affective\\nterms related to joy is consistently higher than all other emotions;\\nescalating joy from the next highest occurring emotions fear and sadness\\nminimally by a factor of 1.1 in SemEval-2018 EI-oc and maximum by\\na factor of 5.6 in C4-Val, respectively. The predominance of joy in\\ntextual corpora can be possibly due to the reason that, psychologically\\npeople are inclined towards expressing more positive emotions on the\\nweb (Vittengl and Holt, 1998; De Choudhury et al., 2012; Staiano and\\nGuerini, 2014; Waterloo et al., 2018). On the other side, for all the\\ncorpora, the instances of anger are consistently very low in count. The\\nstandard deviation computed to measure the dispersion between the\\noccurrence of various emotions within a corpus shows that there exists\\na large disparity between the occurrence of emotions within a corpus,\\nparticularly in the large scale corpora used to pre-train PLMs. In total,\\nthe occurrence statistics over the four basic emotions anger, fear, joy\\nand sadness, clearly affirms the existence of emotion imbalances in both\\nPLM pre-training and fine-tuning corpora.\\nBookCorpus contains the highest number of total affective words\\namong all other corpora considered. This brings to another observation\\nthat despite BookCorpus being almost one-third of the size of WikiEn,\\nthe number of affective words in BookCorpus exceeds WikiEn by a\\nfactor of 1.3. We presume this is because BookCorpus being a large\\ncorpus curated from books in the web, contains more affective words\\nthan WikiEn curated from Wikipedia articles in the web.\\n3.3.2. Co-occurrence of emotions with social groups\\nThe co-occurrence statistics of basic emotions with various social\\ngroups in gender, racial and religious domains for each corpus is\\nillustrated in Table 4, where the domains are separated column wise\\nand emotions are grouped across the rows. We look into each domain\\nseparately, (in the order of gender, race, and religion) and analyze the\\nassociation of emotion categories (in the order of anger, fear, joy, and\\nsadness) with social groups in these domains.\\n(A) Emotion Co-occurrence with Gender Domain: In the gender do-\\nmain, anger mostly co-occurs with the non-binary and female\\nsocial groups than male. Fear is always highly associated with\\nthe non-binary group, followed secondly by female. The positive\\nemotion joy is found to mostly co-occur with male, but, it has\\nthe least co-occurrence with non-binary gender. Sadness mostly\\nco-occurs with non-binary and female groups, similar to anger.\\nFor the fine-tuning corpus SemEval-2018, in particular, there\\nis no instance of co-occurrence between any of the emotions\\nand non-binary gender, this is due to the lack of non-binary\\ngender terms in the corpus; also, for this corpus, negative emo-\\ntions such as, anger, fear, and sadness are always found to have\\nhigh co-occurrence with female gender and the positive emotion\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 5}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nTable 3\\nOccurrence statistics of emotions in the corpora.\\nCorpus\\nAnger\\nFear\\nJoy\\nSadness\\nTotal affective words\\nStandard deviation\\nWikiEn\\n533 111\\n745 221\\n2 479 326\\n1 802 466\\n5 560 124\\n914 103.94\\nBookCorpus\\n1 049 407\\n1 647 267\\n3 143 907\\n1 400 423\\n7 241 004\\n922 324.00\\nWebText-250k\\n50 207\\n85 325\\n220 354\\n88 749\\n444 635\\n74 851.63\\nC4-Val\\n33 182\\n66 239\\n394 413\\n69 686\\n563 520\\n169 821.19\\nSemEval-2018\\n984\\n1 472\\n1 579\\n1 131\\n5 166\\n280.21\\nTable 4\\nCo-occurrence statistics of basic emotions with various domains in corpora (in\\npercentage).\\nCorpus\\nCo-occurrence with\\nGender\\nRace\\nReligion\\nM\\nF\\nNb\\nEA\\nAA\\nCh\\nMu\\nJw\\nAnger\\nWikiEn\\n12.12\\n13.41\\n14.25\\n10.44\\n10.68\\n8.55\\n11.69\\n13.93\\nBookCorpus\\n17.61\\n16.15\\n19.02\\n15.09\\n17.06\\n12.20\\n13.74\\n18.64\\nWebText-250k\\n14.13\\n14.24\\n11.46\\n15.05\\n16.53\\n12.86\\n15.05\\n19.55\\nC4-Val\\n9.32\\n9.08\\n6.02\\n7.06\\n7.71\\n6.22\\n11.19\\n13.49\\nSemEval-2018\\n22.36\\n24.56\\n0\\n22.55\\n52.17\\n15.79\\n15.06\\n0\\nFear\\nWikiEn\\n12.61\\n15.09\\n21.01\\n14.73\\n14.62\\n9.81\\n17.03\\n16.05\\nBookCorpus\\n22.03\\n24.00\\n25.05\\n23.09\\n23.52\\n14.65\\n21.42\\n16.44\\nWebText-250k\\n19.56\\n21.80\\n23.02\\n21.11\\n21.02\\n16.66\\n36.00\\n28.39\\nC4-Val\\n13.95\\n13.79\\n16.87\\n13.56\\n13.46\\n9.33\\n23.09\\n19.70\\nSemEval-2018\\n25.36\\n26.06\\n0\\n31.37\\n10.87\\n36.84\\n62.16\\n75.00\\nJoy\\nWikiEn\\n40.81\\n40.81\\n39.18\\n45.46\\n45.31\\n51.94\\n36.47\\n41.93\\nBookCorpus\\n41.09\\n40.01\\n38.40\\n44.01\\n41.07\\n51.12\\n44.53\\n40.77\\nWebText-250k\\n44.25\\n40.01\\n42.79\\n43.69\\n42.44\\n47.54\\n25.06\\n27.53\\nC4-Val\\n57.76\\n61.28\\n55.42\\n63.49\\n63.95\\n68.05\\n44.28\\n45.75\\nSemEval-2018\\n33.53\\n30.83\\n0\\n34.31\\n13.04\\n27.02\\n12.16\\n25.00\\nSadness\\nWikiEn\\n34.46\\n30.70\\n25.56\\n29.37\\n29.38\\n29.70\\n34.81\\n28.09\\nBookCorpus\\n19.76\\n19.84\\n21.02\\n18.11\\n18.55\\n22.03\\n20.30\\n24.14\\nWebText-250k\\n24.05\\n25.25\\n20.83\\n20.75\\n20.51\\n22.94\\n24.09\\n24.52\\nC4-Val\\n18.96\\n16.95\\n21.69\\n15.89\\n14.88\\n16.40\\n21.44\\n21.05\\nSemEval-2018\\n17.75\\n19.05\\n0\\n11.76\\n23.91\\n21.05\\n10.81\\n0\\njoy is found to have high co-occurrence with male. The over-\\nall co-occurrence statistics of the gender domain illustrate that\\nnegative emotions mostly co-occur with the non-binary gender\\ngroup, followed by female, and conversely, positive emotions\\nco-occur mostly with the male group. The observations thus\\nclearly dictate imbalanced associations between affective terms\\nand social groups of gender domain, in both pre-training and\\nfine-tuning corpora.\\n(B) Emotion Co-occurrence with Racial Domain: Evaluation results\\nover the racial domain illustrate that the negative emotions\\nanger and sadness mostly co-occur with African American race\\ngroup, whereas negative emotion fear and the positive emotion\\njoy mostly co-occur with European American. But, for all the\\npre-training corpora, the imbalance of co-occurrence values in\\nthe racial domain is comparatively less than the previously\\ndiscussed gender domain; for example, imbalance in the co-\\noccurrence of all emotions with the racial groups is negligible\\nin the case of WikiEn corpus. Contrary to the observations of\\npre-training corpora, in fine-tuning corpus SemEval-2018, there\\nexists a large difference in co-occurrence values between African\\nand European American groups. That is, in SemEval-2018, the\\nnegative emotions anger and sadness co-occur with the African\\nAmerican race double the times than European American, indi-\\ncating highly imbalanced association of anger and sadness with\\nAfrican American race. Whereas, the co-occurrence of negative\\nemotion fear and positive emotion joy with European American\\ngroup is almost thrice African American, again indicating a\\nhighly imbalanced association, that of fear and joy emotions in\\nSemEval-2018 with European American group.\\n(C) Emotion Co-occurrence with Religious Domain: Analysis in the do-\\nmain of religion shows that anger mostly co-occurs with Jewish\\nand fear mostly co-occurs with Muslim. Whereas, joy is always\\nfound to have maximum co-occurrence with Christian. Sadness is\\nfound to mostly co-occur with Muslim and Jew religious groups\\nthan Christian. The results thus shows existence of high co-\\noccurrence between negative emotions anger, fear, and sadness\\nwith Muslim and Jew, whereas the positive emotion joy with\\nChristian. Moreover, when considering previous observations\\nof gender and racial domains, the imbalance in the religious\\ndomain is comparatively higher.\\nThe entire occurrence and co-occurrence analysis over gender, race\\nand religious domains thus consolidate the existence of corpus level\\naffective bias in pre-training and fine-tuning corpora. The extensions of\\nsuch corpora holding latent affect imbalances, to build computational\\nmodels may eventually trigger chances of bias in learning models,\\nespecially when building large scale contextual pre-trained language\\nmodels that extract all possible properties of a language.\\n4. Prediction level affective bias\\nTo identify the existence of prediction level affective bias, if any,\\nin the perspective of large PLMs, we utilize textual emotion detection\\nsystems built using popular large PLMs that are fine-tuned using an\\nemotion detection corpus. We evaluate the existence of affective bias\\nin the context of domains gender, race, and religion via different\\nsynthetic and non-synthetic paired evaluation sentence corpora and\\nan extensive set of evaluation measures. Details of our investigation,\\nincluding description and settings of textual emotion detection models\\nbased on large PLMs, the method to measure prediction level affective\\nbias with the details of evaluation corpora and measures, and the\\nresults and analysis of prediction level affective bias, are given below.\\n4.1. Textual emotion detection using large PLMs\\nWe formulate the task of textual emotion detection as a four-class\\nclassification system with classes being the basic emotions anger, fear,\\njoy, and sadness. For this classification task, we utilize pre-trained lan-\\nguage models and fine-tune them with an aim to find the best-fit map-\\nping function 𝑓∶𝑦= 𝑓(𝑥) for the fine-tuning data (𝑥1, 𝑦1), (𝑥2, 𝑦2), … ,\\n(𝑥𝑁, 𝑦𝑁) with 𝑁documents, where 𝑥𝑖indicates ith document in the\\nfine-tuning corpus and 𝑦𝑖indicates the corresponding ground-truth\\nemotion.\\nThe choice of PLMs, GPT-2 (Radford et al., 2019), BERT (De-\\nvlin et al., 2019), XLNet (Yang et al., 2019), and T5 (Raffel et al.,\\n2020), that are utilized in this study to identify affective bias, is\\nmotivated by considering their acceptance as relevant and neoteric\\ncontextualized models with high performance efficacy towards textual\\nemotion detection (Adoma et al., 2020; Acheampong et al., 2021)\\nand the much related task of sentiment analysis (Zhang et al., 2020;\\nTabinda Kokab et al., 2022) within the area of affective computing.\\nGPT and BERT are the very popular PLMs that follow the most ef-\\nfective auto-regressive and auto-encoding self-supervised pre-training\\nobjectives, respectively, where GPT uses transformer decoder blocks,\\nwhereas BERT uses transformer encoder blocks. The autoregressive\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 6}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nTable 5\\nFine-tuning corpus statistics.\\nEmotions\\nNumber of documents\\nTraining\\nValidation\\nAnger\\n2089\\n388\\nFear\\n2641\\n389\\nJoy\\n1906\\n290\\nSadness\\n1930\\n397\\nnature of GPT helps to effectively encode sequential knowledge and\\nachieve good results (Radford et al., 2019). On the other hand, by\\neliminating the autoregressive objective and alleviating unidirectional\\nconstraints through the masked language model pre-training objective,\\nBERT attains powerful bi-directional representations. This ability of\\nBERT to learn context from both sides of a word makes it an empirically\\npowerful state-of-the-art model (Devlin et al., 2019). XLNet brings back\\nthe auto-regressive pre-training objective with alternate ways to extract\\ncontext from both sides of a word and overcome the pretrain-finetune\\ndiscrepancy of BERT outperforming it in several downstream NLP tasks\\n(Yang et al., 2019). The development of T5 explores the landscape of\\nNLP transfer learning and proposes a unified framework that converts\\nall textual language related problems into the text-to-text format and\\nachieves improved performance (Raffel et al., 2020).\\nEach pre-trained language model (PLM) after fine-tuning and appli-\\ncation of softmax function at the final layer forms the textual emotion\\ndetection model (i.e., softmax(PLM)). For each textual document 𝑑, the\\nfine-tuned textual emotion detection models predict an emotion class\\n̂𝑒𝑐𝑙𝑎𝑠𝑠by finding the highest prediction intensity score ̂𝑒𝑠𝑐𝑜𝑟𝑒among 𝐸\\nclasses of emotions (namely anger, fear, joy, and sadness, for our task)\\nrepresented as,\\n̂𝑒𝑐𝑙𝑎𝑠𝑠(𝑑) = argmax\\n𝑘∈1,2,…,𝐸\\n𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑃𝐿𝑀(𝑑))\\n(1)\\n̂𝑒𝑠𝑐𝑜𝑟𝑒(𝑑) =\\nmax\\n𝑘∈1,2,…,𝐸𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑃𝐿𝑀(𝑑))\\n(2)\\nTo fine-tune PLMs and build emotion detection models, we use\\n24-layered version of the pre-trained BERT, GPT-2, XLNet, and T5\\navailable at HuggingFace,16 i.e., bert-large-uncased,17 gpt2-medium,18\\nxlnet-large-cased,19 and t5-large,20 respectively, and update these ar-\\nchitectures by adding a final dense layer of four neurons with softmax\\nactivation function on top of the base models to suit our four class\\nclassification task. For our study, the choice of GPT-2 instead of the\\nlatest version GPT-3 (Brown et al., 2020) is due to its unavailability as\\nan open-source pre-trained model. All four models are fine-tuned using\\na popular affect detection corpus SemEval-2018 EI-oc (Mohammad\\net al., 2018) that consists a total of 10 030 data instances for the\\nemotions anger, fear, joy, and sadness. The fine-tuning corpus is split as\\n8566 data instances for training and 1464 data instances for validation;\\ndetails of the number of data instances belonging to each emotion\\ncategory in the train and validation splits are shown in Table 5.\\nThe hyperparameters that can aid the reproducibility of our emotion\\ndetection models are, for GPT-2, XLNet, and T5 we use Adam optimizer\\nwith learning rate 0.000001, categorical crossentropy loss function,\\nand 100 epochs, whereas for BERT the learning rate is 0.00001 and\\nrest of the above mentioned parameters are the same. The batch size\\nis set to 80 for BERT, XLNet, and T5, whereas 64 for GPT-2. The\\ntotal number of trainable parameters for our BERT, GPT-2, XLNet,\\nand T5 textual emotion detection models come out as 335145988,\\n354827268, 360272900, and 334943748, respectively. All experiments\\nwere conducted on a deep learning workstation equipped with Intel\\n16 https://huggingface.co/.\\n17 https://huggingface.co/docs/transformers/model_doc/bert.\\n18 https://huggingface.co/docs/transformers/model_doc/gpt2.\\n19 https://huggingface.co/docs/transformers/model_doc/xlnet.\\n20 https://huggingface.co/docs/transformers/model_doc/t5.\\nXeon Silver 4208 CPU at 2.10 GHz, 256 GB RAM, and two GPUs\\nof NVIDIA Quadro RTX 5000 (16 GB for each), using the libraries\\nTensorflow (version 2.8.0), Keras (version 2.8.0), Transformer (version\\n4.17.0), and NLTK (version 3.6.5).\\n4.2. Measuring prediction level affective bias\\nThe textual emotion detection models, when supplied with a docu-\\nment/sentence, predict as output the emotion class and corresponding\\nemotion intensity of the document/sentence. To identify prediction\\nlevel affective bias in textual emotion detection models, we input into\\nthese models a sentence pair that differs only in key terms representing\\ndifferent social groups, with an aim to compare and contrast between\\nemotion predictions of sentences in that pair. For instance, sentence\\npairs such as ‘She made me feel angry’ versus ‘He made me feel angry’\\nthat only differ in key terms representing female and male social groups\\nconcerning gender domain, or ‘African American people can dance very\\nwell’ versus ‘European American people can dance very well’ that only\\ndiffer in key terms representing African American and European Amer-\\nican social groups concerning racial domain, are input to the models\\nto compare and contrast between emotion predictions of sentences in\\nthese pairs. Comparing emotion predictions using such sentence pairs\\nhelps to pair-wise analyze and understand whether algorithmic deci-\\nsions of emotion classification are similar (or different) across different\\nsocial groups within a domain. Accordingly, to identify prediction level\\naffective bias, we use evaluation corpora that consist of sentence pairs\\ndiffering only in key terms representing various social groups.\\nThe prediction of emotion class for a sentence is decided by the\\nintensity of emotions predicted by the textual emotion detection model\\nfor that sentence. For example, for a prediction ̂𝐸𝑠𝑐𝑜𝑟𝑒(𝑑) = {0.5, 0.2,\\n0.1, 0.2}, the choice of emotion class from the set 𝐸= {𝑎𝑛𝑔𝑒𝑟, 𝑓𝑒𝑎𝑟,\\n𝑗𝑜𝑦, 𝑠𝑎𝑑𝑛𝑒𝑠𝑠}, would be anger. Differences in the intensities of emotion\\npredictions between sentences in a pair show existence of affective bias\\nat the intensity level, which when higher enough can alter the predic-\\ntion of emotion class and thereby cause affective bias at the class level.\\nThat is, an unbiased model is expected to predict the same emotion\\nclass and intensities for the sentence pairs that only differ in key terms\\nrepresenting different social groups. Hence, to analyze affective bias in\\nthe predictions, we utilize class based and intensity based evaluation\\nmeasures capable of comparing predictions of these sentence pairs. The\\nevaluation corpora and measures are detailed below.\\n4.2.1. Evaluation corpora\\nOur choice of bias evaluation corpora is based on the objective\\nto identify affective bias in textual emotion detection models using\\nsentence pairs that only differ in key terms representing social groups,\\nconcerning either gender, racial, or religious domain. Suitably, we\\nutilize three different evaluation corpora, Equity Evaluation Corpus\\n(EEC) (Kiritchenko and Mohammad, 2018), Bias Identification Test\\nin Sentiments (BITS) corpus (Venkit and Wilson, 2021), and Crowd-\\nsourced Stereotype Pairs (CSP) corpus (Nangia et al., 2020). Similar\\nto most bias evaluation corpora, EEC and BITS contain template based\\nsynthetically created sentences along with ground truth emotions. On\\nthe contrary, CSP is a crowd sourced non-synthetic bias evaluation\\ncorpus that possesses greater diversity within data in the perspective\\nof context expressed and structure of sentence pairs, but it does not\\ncontain ground truth emotions.\\nEEC consists of a total of 8640 sentences capable of evaluating\\ngender and racial domains, from which we select 8400 sentences for\\nour study after excluding 240 sentences with no emotion words. For\\nthe gender domain, the sentence pairs in EEC represent only two social\\ngroups, female and male. To tease out affective bias towards the gender\\ndomain, we select 1400 sentence pairs. For the racial domain, the\\nsentence pairs represent African American and European American\\nsocial groups, and we select 2800 sentence pairs to analyze affective\\nbias in racial domain.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 7}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nBITS consists of a total of 720 sentences capable of evaluating\\ngender and racial domains. In BITS, the sentence pairs for gender\\ndomain also include the non-binary social group besides female and\\nmale groups. For this study, we select the available 120 sentences\\nfor each non-binary, female, and male social group in the gender\\ndomain. The racial domain includes social groups such as American\\nIndian, Asian, and Latino, apart from African American and European\\nAmerican groups. For this study, we only select the available small\\nset of 72 sentence pairs representing African American and European\\nAmerican social groups, as we limit our investigation only to these two\\nracial groups.\\nCSP contains 1508 sentence pairs that cover nine domains, includ-\\ning gender, race, religion, age, nationality, etc., that, unlike EEC and\\nBITS, do not differ ‘only’ on a single key term. A sample sentence pair in\\nCSP for the racial domain is ‘African American people are not very ambi-\\ntious as a whole’ versus ‘European American people are very ambitious as a\\nwhole’. Such sentence pairs violate our evaluation setting of considering\\nsentence pairs that ‘only’ differ in key terms representing certain social\\ngroups, an evaluation strategy to identify bias. But considering the\\nusefulness of such a non-synthetic corpus that suits the real-world con-\\ntext, we manually evaluate all sentence pairs in CSP and modify them\\n(if required) to suit our evaluation settings. An example of modified\\nversions for the above mentioned sentence pair is, ‘African American\\npeople are not very ambitious as a whole’ versus ‘European American people\\nare not very ambitious as a whole’, and ‘African American people are\\nvery ambitious as a whole’ versus ‘European American people are very\\nambitious as a whole’. Finally, after such modifications and exclusion\\nof pairs belonging to domains other than gender, race, and religion,\\nwe gather 1970 sentences, where the gender domain consists of 263\\nsentence pairs representing female and male, the racial domain consists\\nof 566 sentence pairs representing African Americans and European\\nAmericans, and religious domain consists of 104 sentences each for\\nChristian, Jew, and Muslim social groups.\\nEven though in some evaluation corpora, certain domains consist\\nof three social groups (e.g. in BITS, the gender domain consists of\\nmale, female, and non-binary social groups, in CSP, the religious do-\\nmain consists of Christian, Jew, and Muslim groups), our evaluation\\nstrategies are limited to pair-wise evaluations, to maintain commonality\\namong all the domains. That is, for all the evaluation corpora, from the\\navailable set of social groups, we conduct pair-wise evaluations for the\\npairs, Male versus Female (M × F), Male versus Non-binary (M × Nb),\\nor Female versus Non-binary (F × Nb) in gender domain, European\\nAmerican versus African American (EA × AA) in the racial domain, and\\nChristian versus Muslim (Ch × Mu), Christian versus Jew (Ch × Jw) or\\nMuslim versus Jew (Mu × Jw) in the religious domain.\\n4.2.2. Evaluation measures\\nFor an evaluation corpus with 𝑁sentence pairs, we denote 𝑠𝑝𝑔1\\n𝑖\\nand\\n𝑠𝑝𝑔2\\n𝑖\\nas the ith sentence pair representing two social groups 𝑔1 and 𝑔2\\n(e.g. Male versus Female), respectively, in a domain (e.g. gender). We\\nevaluate the existence of prediction level affective bias using different\\nmeasures that rely on class (̂𝑒𝑐𝑙𝑎𝑠𝑠) and intensity (̂𝑒𝑠𝑐𝑜𝑟𝑒) predictions of\\nthe textual emotion detection models, details follow.\\n• Demographic Parity (DP): A popular class based measure to quan-\\ntify group fairness/bias of a classifier system, commonly used\\nto address general affect-agnostic biases like gender bias, racial\\nbias, etc. Du et al. (2021). We utilize this measure to identify\\nthe existence of affective bias and check whether the model’s\\nemotion classifications are similar (or different) across different\\nsocial groups within a domain. Accordingly, we say that a textual\\nemotion detection model satisfies demographic parity if,\\nDP = 𝑃(̂𝑒𝑐𝑙𝑎𝑠𝑠(𝑠𝑝𝑔1) = 𝑒|𝑧= 𝑔1)\\n𝑃(̂𝑒𝑐𝑙𝑎𝑠𝑠(𝑠𝑝𝑔2) = 𝑒|𝑧= 𝑔2) ,\\n𝑒∈𝐸and 𝑔1, 𝑔2 ∈𝑇\\n(3)\\nwhere, 𝑃(̂𝑒𝑐𝑙𝑎𝑠𝑠(𝑠𝑝𝑔1) = 𝑒|𝑧= 𝑔1) and 𝑃(̂𝑒𝑐𝑙𝑎𝑠𝑠(𝑠𝑝𝑔2) = 𝑒|𝑧= 𝑔2)\\nindicates the probabilities of the two social groups 𝑔1 and 𝑔2,\\nrespectively, to predict an emotion 𝑒; 𝑔2 is taken as the group\\nwith higher probability (Feldman et al., 2015). 𝐸is the set of\\nall emotions, and 𝑇is the set of social groups in a domain.\\nDemographic parity advocates the likelihood of emotion predic-\\ntion outcomes of sentence pairs that differ only in key terms\\ndenoting a certain social group should be the same; as a result,\\nDP=1 indicates an ideal unbiased scenario, whereas, lower the\\nvalues higher the existence of bias. Therefore, we use the general\\nthreshold 𝜏= 0.80, lower than which indicates biased predictions\\n(Feldman et al., 2015).\\n• Average Difference of Prediction Intensity Scores (𝑎𝑣𝑔.𝛥): An\\nintensity based measure that computes the average difference of\\nemotion prediction intensity scores between the sentence pairs\\nof two social groups in a domain (Kiritchenko and Mohammad,\\n2018).\\n𝑎𝑣𝑔.𝛥= 1\\n𝑁\\n𝑁\\n∑\\n𝑖=1\\n|̂𝑒𝑠𝑐𝑜𝑟𝑒(𝑠𝑝𝑔1\\n𝑖) −̂𝑒𝑠𝑐𝑜𝑟𝑒(𝑠𝑝𝑔2\\n𝑖)|\\n(4)\\nwhere, ̂𝑒𝑠𝑐𝑜𝑟𝑒(𝑠𝑝𝑔1\\n𝑖) and ̂𝑒𝑠𝑐𝑜𝑟𝑒(𝑠𝑝𝑔2\\n𝑖) indicates emotion prediction\\nintensity scores corresponding to the social groups 𝑔1 and 𝑔2,\\nrespectively, for the ith sentence pair concerning a domain, and\\n𝑁denotes the total number of sentence pairs. That is, 𝑎𝑣𝑔.𝛥\\nindicates the average dissimilarity in prediction scores between\\na pair of sentences; 0 indicates perfect similarity, and higher the\\nvalues more the dissimilarity.\\n• Prediction Score Significance (𝑝-value): A measure that shows\\nwhether dissimilarity in prediction scores between the sentence\\npairs is statistically significant or not. To compute prediction\\nscore significance, we perform a paired statistical significance\\ntest, 𝑡-Test (Kiritchenko and Mohammad, 2018) over the predic-\\ntion scores of sentence pairs, ̂𝑒𝑠𝑐𝑜𝑟𝑒(𝑠𝑝𝑔1\\n𝑖) and ̂𝑒𝑠𝑐𝑜𝑟𝑒(𝑠𝑝𝑔2\\n𝑖), using the\\nconventional significance level, i.e., a 𝑝-value of 0.05.\\n• Average Confidence Score (ACS): A measure that illustrates model\\nbias towards a particular social group using the average ratio\\nbetween prediction intensity scores of sentence pairs (Nangia\\net al., 2020), computed as,\\nACS = 1\\n𝑁\\n𝑁\\n∑\\n𝑖=1\\n1 −\\n̂𝑒𝑠𝑐𝑜𝑟𝑒(𝑠𝑝𝑔1\\n𝑖)\\n̂𝑒𝑠𝑐𝑜𝑟𝑒(𝑠𝑝𝑔2\\n𝑖)\\n(5)\\nACS value of an unbiased model will peak around zero, but if\\nit tends to negative values, then the measure indicates that the\\nmodel prediction intensities of the social group 𝑔1 are higher than\\n𝑔2, and if it tends to positive values, it indicates that prediction\\nintensities of the social group 𝑔2 are higher than 𝑔1.\\n4.3. Results and analysis of prediction level affective bias\\nWe examine emotion predictions of each PLM based textual emotion\\ndetection system and could observe the existence of affective bias in\\nthe predicted emotion classes, as well as their intensities, for gender,\\nrace, and religious domains. The sample set of predictions presented in\\nTable 1 is a small subset of these affectively biased emotion predictions\\nfrom the emotion detection models that employ BERT and T5. More sets\\nof affectively biased predictions from the PLM based textual emotion\\ndetection systems, are provided in the supplementary material. In the\\nfollowing subsections, we evaluate the results of each PLM separately.\\n4.3.1. Affective bias in BERT\\nTable 6 shows evaluation results observed for the textual emotion\\ndetection model built using BERT, analyzing gender, racial and re-\\nligious domains using three different evaluation corpora EEC, BITS,\\nand CSP, and various evaluation measures. The pairs of social groups\\naddressed by the evaluation corpora within each domain are presented\\ncolumn wise, the measures are presented row wise, and the emotions\\nare grouped across the rows.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 8}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nTable 6\\nResults of BERT (Boldface is used to highlight the values of DP < threshold 𝜏= 0.80 and p-values < 0.05).\\nEvaluation\\nGender\\nRace\\nReligion\\nmeasures\\nEEC\\nM × F\\nBITS\\nM × F\\nCSP\\nM × F\\nBITS\\nM × Nb\\nBITS\\nF × Nb\\nEEC\\nEA × AA\\nBITS\\nEA × AA\\nCSP\\nEA × AA\\nCSP\\nCh × Mu\\nCSP\\nCh × Jw\\nCSP\\nMu × Jw\\nAnger\\nDP\\n0.964\\n1.000\\n0.836\\n0.866\\n0.867\\n0.996\\n0.948\\n1.000\\n0.923\\n0.923\\n1.000\\navg.𝛥\\n0.018\\n0.016\\n0.049\\n0.038\\n0.030\\n0.031\\n0.012\\n0.052\\n0.076\\n0.078\\n0.100\\np-value\\n0.003\\n0.036\\n0.037\\n0.047\\n0.132\\n0.417\\n0.431\\n0.730\\n0.038\\n0.042\\n2e−04\\nACS\\n0.010\\n0.017\\n0.025\\n0.036\\n0.020\\n−0.005\\n−0.008\\n−0.001\\n0.050\\n−0.084\\n−0.148\\nFear\\nDP\\n0.954\\n1.000\\n1.000\\n0.938\\n0.938\\n0.961\\n1.000\\n0.743\\n0.857\\n0.885\\n0.968\\navg.𝛥\\n0.019\\n0.049\\n0.086\\n0.085\\n0.086\\n0.049\\n0.058\\n0.109\\n0.076\\n0.089\\n0.073\\np-value\\n9.2e−12\\n0.864\\n0.767\\n0.043\\n0.063\\n5.3e−27\\n0.748\\n1.2e−6\\n0.044\\n0.439\\n0.001\\nACS\\n0.019\\n−0.010\\n−0.015\\n−0.094\\n−0.088\\n−0.055\\n−0.016\\n−0.123\\n0.031\\n−0.041\\n−0.082\\nJoy\\nDP\\n0.994\\n1.000\\n0.971\\n1.000\\n1.000\\n1.000\\n1.000\\n0.797\\n0.455\\n0.637\\n0.713\\navg.𝛥\\n0.002\\n9.9e−5\\n0.072\\n0.001\\n0.001\\n0.005\\n0.001\\n0.076\\n0.148\\n0.031\\n0.130\\np-value\\n0.400\\n0.061\\n0.014\\n0.360\\n0.394\\n0.002\\n0.611\\n0.001\\n0.033\\n0.425\\n0.021\\nACS\\n−0.001\\n−5.8e−5\\n0.064\\n−0.001\\n−0.001\\n−0.004\\n−1e−4\\n−0.080\\n−0.240\\n−0.022\\n0.169\\nSadness\\nDP\\n0.953\\n1.000\\n0.872\\n0.938\\n0.938\\n0.977\\n0.950\\n0.724\\n0.666\\n0.666\\n1.000\\navg.𝛥\\n0.027\\n0.013\\n0.076\\n0.024\\n0.033\\n0.056\\n0.012\\n0.116\\n0.124\\n0.100\\n0.051\\np-value\\n1.8e−4\\n0.045\\n0.019\\n0.461\\n0.156\\n0.600\\n0.924\\n1e−12\\n0.065\\n0.201\\n0.146\\nACS\\n−0.020\\n−0.019\\n−0.064\\n0.006\\n0.022\\n−0.010\\n−0.002\\n0.100\\n−0.279\\n−0.169\\n0.064\\n(A) Affective Gender Bias: Initially, looking into the gender domain,\\nfor class based measure DP, throughout all the emotions, we can\\nobserve that there is almost no affective bias in the predictions\\nmade by BERT between male and female groups when evaluated\\nusing the EEC corpus (since, DP > 0.8 in all cases), and ideally\\nno affective bias when evaluated using BITS corpus (since, DP =\\n1 in all cases). This ideal scenario in BITS might be because\\nBITS is a small corpus containing short-length synthetically cre-\\nated sentences with explicit emotion terms that do not suit the\\nreal-world context. When compared to synthetic corpora (EEC\\nand BITS), evaluations using the real-world context and non-\\nsynthetic corpus CSP shows more disparity (lower values of DP)\\nbetween male and female groups for all the emotions except\\nfear. For pairs involving non-binary genders, the values of DP\\nare much less than those involving male and female groups of\\nsynthetic corpora EEC and BITS, for all emotions except joy.\\nThis indicate more disparity of male and female groups with\\nnon-binary gender, with respect to anger, fear and sadness. Since\\nthe evaluation of affective bias in non-binary social groups is\\nonly possible with BITS corpus, it may limit the exploration\\nof affective bias towards this group and also the magnitude of\\naffective bias. For the measure DP, when looking across each\\nemotion, the most disparity (lowest value for DP) is observed\\nfor anger between male versus female when evaluated using CSP\\ncorpus, followed by male versus non-binary, and female versus\\nnon-binary, for the same emotion, when evaluated using BITS\\ncorpus. Whereas, for joy, very less disparity is observed across\\nthe gender groups. In total, even though disparities are shown\\nby DP, any of the gender pairs do not have values of DP less than\\nthe threshold 𝜏= 0.80. Hence DP does not establish the existence\\nof gender affective bias in the predictions of BERT using these\\nevaluation corpora.\\nComing to the intensity based measure avg.𝛥in the gender\\ndomain, similar to DP, more disparity is observed for male\\nversus female pairs when evaluated using CSP corpus and also\\nfor the pairs involving non-binary social groups in BITS, across\\nall the emotions. Different from the measure DP, avg.𝛥reports\\nhighest disparity for fear, but similar to DP, avg.𝛥shows very\\nless disparity for joy. For the next measure 𝑝-value, at least one\\nof the evaluation corpora reports values less than 0.05 or statisti-\\ncally significant difference between male and female predictions\\nacross the emotions, indicating the existence of affective bias.\\nThe 𝑝-value also shows that difference between male and non-\\nbinary predictions for anger and fear are statistically significant.\\nAnalyzing the prediction intensity plots of pairs with statistically\\nsignificant differences (e.g. Figs. 2(a) and 2(b)), shows that their\\nintensity plots also depict more dispersion between data points\\nas well as more disparity between the corresponding mean val-\\nues. Conversely, in the plots of sentence pairs with statistically\\ninsignificant differences in prediction intensities (e.g. Fig. 2(c)),\\nthere is very less dispersion between data points and less dispar-\\nity between the mean values. Therefore 𝑝-value evidently reports\\nthe existence of affective bias in emotion prediction intensities\\nof male and female groups with respect to all emotions, and for\\nmale and non-binary groups with respect to anger and fear.\\nIn the case of intensity based measure ACS, for emotion anger,\\nthe positive values in Male versus Female sentence pairs of EEC,\\nBITS, and CSP indicates that prediction intensities for anger are\\nhigher for the Female when compared to Male, and positive\\nvalues in Male versus Non-binary and Female versus Non-binary\\nsentence pairs of BITS indicates that anger prediction intensities\\nare higher for the Non-binary group when compared to Male and\\nFemale. Similarly, when examining across evaluation corpora,\\nprediction intensities of fear and joy are higher for Male and\\nFemale genders, and prediction intensities of sadness are higher\\nfor Male and Non-binary genders. Therefore in the gender do-\\nmain, the measure ACS also indicates affective bias in prediction\\nintensities.\\n(B) Affective Racial Bias: The European and African American racial\\ngroups when evaluated using CSP corpus, for the measure DP,\\nshows the presence of affective bias for all emotions except\\nanger, where EEC and BITS fail to identify it. Similarly, the avg.𝛥\\ndisparities among intensity predictions of these racial groups\\nare also much more visible when evaluated using CSP corpus.\\nEither or both, EEC and CSP corpora shows that the difference\\nin intensity predictions of these racial groups are statistically\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 9}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nFig. 2. Intensity plots of emotion predictions from BERT.\\nsignificant with p-values less than 0.05, for all emotions ex-\\ncept anger, similar to the observations of the measure DP. The\\nmeasure ACS also shows disparities in prediction intensities\\nbetween the racial groups, where, for all emotions, prediction\\nintensities of European American race are mostly higher than\\nAfrican American.\\n(C) Affective Religious Bias: In the religious domain, the measure\\nDP evidently shows affective bias in the emotion joy with very\\nlow values for all three religious pairs and also in sadness for\\nChristian versus Muslim and Christian versus Jew pairs. For\\nall the emotions, the values of DP indicate more bias in the\\nChristian versus Muslim and Christian versus Jew sentence pairs\\nthan in the Muslim versus Jew pairs. The measure avg.𝛥shows\\nthat there exist disparities between prediction intensities of reli-\\ngious pairs, and these disparities are found to be comparatively\\nhigher than the pairs of gender and racial domains. The 𝑝-value\\nindicates statistically significant differences in intensity predic-\\ntions of anger between all three religious pairs. Also, Christian\\nversus Muslim and Muslim versus Jew pairs show statistically\\nsignificant differences in intensity predictions of all emotions\\nexcept sadness. The measure ACS shows that for BERT anger and\\nfear prediction intensities are higher for Muslim followed by\\nChristian, and joy and sadness prediction intensities are higher\\nfor Christian followed by Jew.\\n4.3.2. Affective bias in GPT-2\\n(A) Affective Gender Bias: Table 7 shows evaluation results observed\\nfor GPT-2 where similar to BERT, no gender affective bias is\\nobserved with the measure DP for any of the emotion class\\npredictions. Whereas intensity based disparities are shown by\\nthe measure avg.𝛥, which is highly visible when evaluated using\\nCSP corpus. The difference in prediction intensities between\\nMale versus Female when evaluated using EEC corpus for all\\nemotions except joy, and Male versus Non-binary and Female\\nversus Non-binary when evaluated using BITS corpus for all\\nemotions except fear, are statistically significant with p-values\\n< 0.05, indicating the existence of affective bias in emotion\\nprediction intensities. The measure ACS indicates that, in GPT-\\n2, anger and joy prediction intensities are higher for Male and\\nFemale genders, fear prediction intensities are higher mainly for\\nFemale, and sadness prediction intensities are higher mainly for\\nMale gender.\\n(B) Affective Racial Bias: In the racial domain, similar to gender,\\nDP does not show racial affective bias for any of the emotion\\nclass predictions, whereas intensity based disparities are shown\\nby the measure avg.𝛥. Here also, the disparities for class based\\nmeasure DP and intensity based measure avg.𝛥, are more visible\\nwhen evaluated using CSP corpus. Whereas BITS reports an ideal\\nunbiased scenario for DP and very low disparity for avg.𝛥. The\\nmeasure 𝑝-value reports that the difference in prediction inten-\\nsities of European and African American races are statistically\\nsignificant for all emotions except sadness. The measure ACS\\nshows that, in GPT-2, prediction intensities of anger and sadness\\nare mostly higher for African American race, whereas predic-\\ntion intensities of fear and joy are mostly higher for European\\nAmerican race.\\n(C) Affective Religious Bias: Unlike gender and race, in the religious\\ndomain the class based measure DP reports affective bias (with\\nvalues of DP < 0.8) in the predictions of all emotions except\\nfear. The measure avg.𝛥also shows disparities in prediction\\nintensities of religious pairs. The p-values indicate that differ-\\nence in fear prediction intensities for the pairs Christian versus\\nMuslim and Muslim versus Jew are statistically significant. The\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 10}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nTable 7\\nResults of GPT-2 (Boldface is used to highlight the values of DP < threshold 𝜏= 0.80 and p-values < 0.05).\\nEvaluation\\nGender\\nRace\\nReligion\\nmeasures\\nEEC\\nM × F\\nBITS\\nM × F\\nCSP\\nM × F\\nBITS\\nM × Nb\\nBITS\\nF × Nb\\nEEC\\nEA × AA\\nBITS\\nEA × AA\\nCSP\\nEA × AA\\nCSP\\nCh × Mu\\nCSP\\nCh × Jw\\nCSP\\nMu × Jw\\nAnger\\nDP\\n0.992\\n0.926\\n0.954\\n0.960\\n0.889\\n0.980\\n1.000\\n0.920\\n0.600\\n0.867\\n0.692\\navg.𝛥\\n0.023\\n0.006\\n0.039\\n0.008\\n0.008\\n0.038\\n0.010\\n0.050\\n0.059\\n0.048\\n0.021\\np-value\\n2.5e−05\\n0.103\\n0.772\\n0.031\\n0.004\\n3.4e−5\\n0.015\\n0.037\\n0.580\\n0.788\\n0.626\\nACS\\n0.013\\n0.007\\n−0.005\\n−0.006\\n−0.008\\n0.011\\n0.012\\n0.015\\n−0.044\\n−0.018\\n0.010\\nFear\\nDP\\n1.000\\n1.000\\n0.991\\n0.960\\n0.960\\n0.996\\n1.000\\n0.901\\n0.883\\n0.985\\n0.870\\navg.𝛥\\n0.016\\n0.007\\n0.058\\n0.017\\n0.015\\n0.030\\n0.010\\n0.063\\n0.139\\n0.069\\n0.158\\np-value\\n0.048\\n0.372\\n0.505\\n0.917\\n0.787\\n0.012\\n0.101\\n0.183\\n6.9e−13\\n0.262\\n7e−13\\nACS\\n−0.003\\n0.002\\n0.001\\n3.7e−4\\n−0.001\\n−0.011\\n−0.014\\n0.005\\n0.159\\n−0.040\\n−0.277\\nJoy\\nDP\\n0.985\\n1.000\\n0.914\\n1.000\\n1.000\\n0.995\\n1.000\\n0.936\\n0.545\\n0.600\\n0.909\\navg.𝛥\\n0.008\\n3.3e−5\\n0.073\\n0.001\\n0.001\\n0.017\\n2e−4\\n0.101\\n0.114\\n0.100\\n0.089\\np-value\\n0.640\\n0.713\\n0.761\\n0.018\\n0.017\\n0.872\\n0.204\\n6.1e−5\\n0.110\\n0.944\\n0.069\\nACS\\n−7.3e−5\\n5.3e−6\\n−0.023\\n−0.001\\n−0.001\\n−0.003\\n−2e−4\\n−0.108\\n0.135\\n−0.011\\n−0.129\\nSadness\\nDP\\n0.985\\n0.951\\n0.927\\n1.000\\n0.951\\n0.996\\n1.000\\n0.938\\n0.467\\n0.933\\n0.502\\navg.𝛥\\n0.011\\n0.002\\n0.047\\n0.014\\n0.014\\n0.018\\n0.010\\n0.055\\n0.039\\n0.045\\n0.045\\np-value\\n4.5e−29\\n0.262\\n0.313\\n0.042\\n0.042\\n0.178\\n0.725\\n0.283\\n0.310\\n0.429\\n0.343\\nACS\\n−0.012\\n−0.001\\n−0.020\\n−0.013\\n−0.011\\n−0.002\\n0.001\\n0.006\\n−0.058\\n0.028\\n0.060\\nmeasure ACS shows that for GPT-2 anger prediction intensities\\nare mostly higher for Christian, fear and joy prediction intensi-\\nties are higher for Muslim and Christian, and sadness prediction\\nintensities are mostly higher for Jew groups.\\n4.3.3. Affective bias in XLNet\\n(A) Affective Gender Bias: Table 8 shows evaluation results of XLNet,\\nwhere the class based measure DP shows negligible affective bias\\n(values of DP is almost one) in emotion predictions of gender\\npairs, whereas avg.𝛥shows disparities in emotion prediction\\nintensities of these pairs. The p-values report that differences be-\\ntween intensity predictions are statistically significant for Male\\nversus Female pairs for all emotions, and also for pairs involving\\nthe Non-binary group for emotion anger. The measure ACS indi-\\ncates high anger and fear prediction intensities for Female and\\nMale genders, and high joy and sadness prediction intensities for\\nMale and Non-binary genders.\\n(B) Affective Racial Bias: Similar to the gender domain, the mea-\\nsure DP does not confirm class based affective racial bias in\\nXLNet, but avg.𝛥shows disparity in intensities of predictions\\nwith 𝑝-value indicating statistically significant differences be-\\ntween prediction intensities of both races, for all emotions. The\\nmeasure ACS shows that anger and sadness prediction intensities\\nare higher for African American, whereas fear and joy prediction\\nintensities are higher for European American race.\\n(C) Affective Religious Bias: In the religious domain, even though the\\nvalues of DP are less compared to gender and racial domains,\\nit is not sufficient to confirm class based affective religious bias\\nin the emotions except sadness whose values are very low and\\nreporting bias. The measure avg.𝛥shows disparity in prediction\\nintensities, with 𝑝-value indicating statistically significant differ-\\nences between Christian versus Muslim and Muslim versus Jew\\nreligious pairs, for anger and sadness. The measure ACS indicates\\nthat anger prediction intensities are mostly higher for Muslim\\nreligion followed by Christian, fear mostly higher for Christian\\nfollowed by Muslim, and joy and sadness higher for Christian and\\nJew.\\n4.3.4. Affective bias in T5\\n(A) Affective Gender Bias: Table 9 shows evaluation results of T5.\\nIn the gender domain, class based measure DP shows affective\\nbias in the predictions of Male versus Female pair for anger and\\nfear when evaluated using CSP corpus. The avg.𝛥measure shows\\ndisparities in prediction intensities, and p-values indicate that\\ndifferences in prediction intensities of Male versus Female pair\\nfor all emotions except fear and in pairs involving Non-binary\\ngender for emotions anger and fear are statistically significant.\\nThe measure ACS indicates high prediction intensities for anger,\\njoy and sadness mostly by Male gender and high prediction\\nintensities for fear mostly by Female and Non-binary genders.\\n(B) Affective Racial Bias: The measure DP does not confirm class\\nbased affective racial bias in T5 predictions, whereas avg.𝛥\\nshows intensity based affective racial bias, with statistically\\nsignificant differences in intensity predictions of the racial pairs\\nfor all the emotions. ACS indicates prediction intensities of\\nAfrican American race are higher for anger, whereas prediction\\nintensities of European American are higher for fear, joy and\\nsadness.\\n(C) Affective Religious Bias: In the religious pairs, the measure DP\\nindicates affective bias in Muslim versus Jew pairs for all emo-\\ntions, in Muslim versus Christian pairs for all emotions except\\nanger, and in Christian versus Jew pairs for joy. The avg.𝛥shows\\nintensity based disparities in all emotions, and p-values indicate\\nthat the differences in prediction intensities are statistically sig-\\nnificant in the case of Muslim versus Jew pair for all emotions\\nexcept joy and in Christian versus Jew pair for the emotion\\nfear. ACS indicates that anger and joy prediction intensities are\\nhigher for Jew religion followed by Christian, fear prediction\\nintensities are higher for Christian followed by Muslim, and\\nsadness prediction intensities are higher for Christian followed\\nby Jew.\\n5. Discussion\\n5.1. Affective bias - Across the PLMs\\nThis study analyzes affective bias in the predictions of textual\\nemotion detection models at class level and intensity level. In most\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 11}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nTable 8\\nResults of XLNet (Boldface is used to highlight the values of DP < threshold 𝜏= 0.80 and p-values < 0.05).\\nEvaluation\\nGender\\nRace\\nReligion\\nmeasures\\nEEC\\nM × F\\nBITS\\nM × F\\nCSP\\nM × F\\nBITS\\nM × Nb\\nBITS\\nF × Nb\\nEEC\\nEA × AA\\nBITS\\nEA × AA\\nCSP\\nEA × AA\\nCSP\\nCh × Mu\\nCSP\\nCh × Jw\\nCSP\\nMu × Jw\\nAnger\\nDP\\n0.983\\n1.000\\n1.000\\n1.000\\n1.000\\n0.976\\n1.000\\n0.974\\n0.825\\n0.869\\n0.950\\navg.𝛥\\n0.017\\n0.005\\n0.053\\n0.017\\n0.019\\n0.048\\n0.004\\n0.061\\n0.115\\n0.083\\n0.110\\np-value\\n1.7e−6\\n0.002\\n0.226\\n0.035\\n0.014\\n0.041\\n0.561\\n0.063\\n0.008\\n0.842\\n0.001\\nACS\\n0.015\\n0.005\\n−0.028\\n−0.015\\n−0.020\\n−0.021\\n0.002\\n0.015\\n0.077\\n−0.032\\n−0.153\\nFear\\nDP\\n0.991\\n1.000\\n0.989\\n1.000\\n1.000\\n0.988\\n1.000\\n0.938\\n0.810\\n1.000\\n0.810\\navg.𝛥\\n0.012\\n0.030\\n0.080\\n0.060\\n0.071\\n0.038\\n0.036\\n0.067\\n0.054\\n0.070\\n0.047\\np-value\\n0.032\\n0.809\\n0.680\\n0.667\\n0.642\\n0.228\\n0.004\\n0.003\\n0.561\\n0.807\\n0.703\\nACS\\n0.004\\n−0.001\\n−0.003\\n−0.008\\n−0.013\\n−0.007\\n−0.050\\n−0.062\\n−0.029\\n−0.005\\n−0.019\\nJoy\\nDP\\n0.993\\n1.000\\n0.974\\n1.000\\n1.000\\n0.970\\n1.000\\n0.804\\n0.856\\n1.000\\n0.857\\navg.𝛥\\n0.010\\n0.013\\n0.084\\n0.006\\n0.018\\n0.022\\n0.009\\n0.084\\n0.027\\n0.077\\n0.086\\np-value\\n0.457\\n0.118\\n0.028\\n0.158\\n0.125\\n0.011\\n0.573\\n0.024\\n0.357\\n0.410\\n0.397\\nACS\\n−0.003\\n−0.018\\n0.056\\n0.006\\n0.019\\n−0.012\\n0.004\\n−0.073\\n−0.055\\n0.073\\n0.133\\nSadness\\nDP\\n0.998\\n1.000\\n0.989\\n1.000\\n1.000\\n0.997\\n1.000\\n0.902\\n0.533\\n0.833\\n0.640\\navg.𝛥\\n0.009\\n0.003\\n0.050\\n0.007\\n0.008\\n0.028\\n0.007\\n0.083\\n0.094\\n0.065\\n0.104\\np-value\\n0.013\\n0.010\\n0.553\\n0.203\\n0.061\\n0.253\\n0.075\\n5.1e−6\\n0.048\\n0.637\\n0.010\\nACS\\n−0.003\\n−0.003\\n−0.031\\n0.002\\n0.005\\n−0.004\\n0.009\\n0.046\\n−0.131\\n0.007\\n0.124\\nTable 9\\nResults of T5 (Boldface is used to highlight the values of DP < threshold 𝜏= 0.80 and p-values < 0.05).\\nEvaluation\\nGender\\nRace\\nReligion\\nmeasures\\nEEC\\nM × F\\nBIT\\nM × F\\nCSP\\nM × F\\nBITS\\nM × Nb\\nBITS\\nF × Nb\\nEEC\\nEA × AA\\nBITS\\nEA × AA\\nCSP\\nEA × AA\\nCSP\\nCh × Mu\\nCSP\\nCh × Jw\\nCSP\\nMu × Jw\\nAnger\\nDP\\n0.983\\n0.966\\n0.765\\n0.897\\n0.866\\n0.933\\n0.952\\n0.903\\n0.968\\n0.816\\n0.790\\navg.𝛥\\n0.039\\n0.016\\n0.077\\n0.021\\n0.022\\n0.101\\n0.004\\n0.106\\n0.082\\n0.113\\n0.097\\np-value\\n3.6e−20\\n0.530\\n0.385\\n0.017\\n0.043\\n0.001\\n0.458\\n6.8e−8\\n0.118\\n0.491\\n0.041\\nACS\\n−0.044\\n0.006\\n−0.037\\n−0.029\\n−0.032\\n0.005\\n0.002\\n0.070\\n−0.086\\n0.014\\n0.064\\nFear\\nDP\\n0.994\\n1.000\\n0.778\\n0.897\\n1.000\\n0.966\\n1.000\\n0.867\\n0.783\\n0.915\\n0.717\\navg.𝛥\\n0.017\\n0.029\\n0.079\\n0.079\\n0.068\\n0.039\\n0.067\\n0.099\\n0.079\\n0.148\\n0.145\\np-value\\n0.309\\n0.318\\n0.662\\n0.003\\n0.004\\n3.1e−7\\n0.022\\n9.2e−5\\n0.602\\n0.001\\n2.8e−5\\nACS\\n0.002\\n0.008\\n−0.025\\n0.071\\n0.063\\n−0.035\\n−0.087\\n−0.111\\n−0.005\\n−0.242\\n−0.263\\nJoy\\nDP\\n0.990\\n1.000\\n0.848\\n1.000\\n1.000\\n0.961\\n1.000\\n0.971\\n0.624\\n0.375\\n0.600\\navg.𝛥\\n0.009\\n2e−4\\n0.062\\n1e−4\\n2.8e−4\\n0.029\\n0.009\\n0.068\\n0.183\\n0.001\\n0.075\\np-value\\n0.003\\n0.025\\n0.885\\n0.605\\n0.115\\n0.122\\n0.332\\n0.001\\n0.122\\n0.468\\n0.423\\nACS\\n−0.009\\n−2e−4\\n−0.025\\n−1.6e−5\\n1.8e−4\\n−0.014\\n−0.014\\n−0.078\\n−0.320\\n0.001\\n0.075\\nSadness\\nDP\\n0.998\\n0.973\\n0.952\\n0.925\\n0.900\\n0.998\\n0.955\\n0.972\\n0.500\\n0.900\\n0.450\\navg.𝛥\\n0.023\\n0.006\\n0.082\\n0.009\\n0.014\\n0.074\\n0.007\\n0.103\\n0.095\\n0.118\\n0.085\\np-value\\n8.6e−15\\n0.035\\n0.689\\n0.223\\n0.871\\n0.002\\n0.048\\n0.957\\n0.121\\n0.751\\n0.020\\nACS\\n−0.026\\n−0.006\\n−0.027\\n−0.008\\n−0.002\\n−0.040\\n−0.007\\n−0.030\\n−0.150\\n−0.002\\n0.099\\ncases, class based measures that are capable of identifying differences\\nin emotion classes predicted for two different social groups, do not\\nshow affective bias, whereas intensity based measures mostly identify\\nthe existence of affective bias in predicted emotion intensities. This\\nis because the differences in predicted emotion intensities between\\nthe social groups might not be that very high to alter the choice of\\nemotion class predictions, but even then there exists affective bias due\\nto differences in the predicted emotion intensities. When comparing\\nacross the PLMs, class based affective gender bias is only observed in\\nT5, whereas intensity based affective gender bias is observed in all\\nthe PLMs. Similarly, class based affective racial bias is only observed\\nin BERT, whereas intensity based affective racial bias is observed in\\nall the PLMs. But, in the domain of religion, all four PLMs show\\nhigh magnitudes of class based and intensity based affective bias,\\ni.e., compared to gender and race, the religious domain is observed to have\\nhigh existence of affective bias. We believe this could be a reflection\\nof comparatively high affect imbalance with respect to the religious\\ndomain in the pre-training corpora (from Table 4).\\nXLNet is observed to have the least class based affective bias, with\\nbias only observed in the case of the religious domain for the emo-\\ntion sadness. XLNet is also observed to have the least intensity based\\naffective bias among all the PLMs when considering the measures avg.𝛥\\n(i.e., the top five values of avg.𝛥do not have any instance of XLNet)\\nand 𝑝-value (i.e., the number of instances in XLNet with statistically\\nsignificant differences are also low). Whereas T5 has the maximum\\nclass based biased instances, and also high intensity based affective bias\\namong all the PLMs when considering the measures avg.𝛥(i.e., top five\\nvalues of avg.𝛥have three instances of T5) and 𝑝-value (i.e., the number\\nof instances in T5 with statistically significant differences are also high).\\nBERT also shows class based and intensity based affective bias, nearly\\nsimilar but comparatively less than T5, followed by GPT-2.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 12}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nThis study explores affective bias in large PLMs that are trained on\\nmillions of parameters. However, rapid growth in the data processing\\ntechnology and plenty availability of data has recently, very quickly\\nevolved the category of large PLMs to being trained on billions of\\nparameters, the PLMs such as LLaMA (Touvron et al., 2023), Flan-T5\\nXXL (Chung et al., 2022), PaLM (Chowdhery et al., 2023), LaMDA\\n(Thoppilan et al., 2022), etc. All these PLMs have the benefits of\\nhuge improvements in their performance, capable of performing several\\ndownstream tasks, and supporting multi-lingual and multi-modal data\\nprocessing. All such large PLMs highly rely on the large availability\\nof massive amounts of textual data especially collected from the web,\\nWikipedia, Book Corpus, etc. In most cases, which proportion of data\\nis extracted from a source to train a PLM is not fully transparent.\\nFor example, LLaMA uses different data proportions from different\\ncommonly available data deluges such as CommonCrawl (67.0%), C4\\n(15.0%), Wikipedia (4.5%), etc., where which data proportions are\\nextracted from each of the sources is not fully transparent. Also, the\\ntraining data quality is unmanageable and unverifiable by even a large\\ngroup of human crowd (Navigli et al., 2023), where there are chances\\nof the existence of affective biases in these recent large PLMs. For\\nexample, LLaMA is trained on data proportions from several corpora in-\\ncluding C4 and Wikipedia, which are already investigated in this study\\nand identified with the affect imbalances. The large PLMs PaLM and\\nLaMDA are also trained on billions of tokens extracted from web pages,\\nbooks, Wikipedia, and news articles, indicating chances of existence of\\naffective bias. Similarly, Flan-T5 is a variant of the T5, and in this study\\nwe observed that T5 has the highest affective bias amongst the PLMs\\nXLNet, BERT, and GPT-2.\\n5.2. Affect imbalance in corpora and affective bias in predictions\\nWhen revisiting the analysis of corpora involved in training PLMs,\\nwe have already observed (in Table 4) that these corpora have imbal-\\nanced co-occurrences of emotions with certain social groups in gender,\\nracial and religious domains. Further at the prediction level, PLMs that\\nutilize these corpora seems to reflect some of these imbalances hinting\\nat the propagation of affect imbalance in data towards affective bias\\nin predictions. For example, in pre-training and fine-tuning corpora\\nof BERT (i.e., WikiEn, BookCorpus, and SemEval-2018), the emotion\\nanger has high co-occurrence with Non-binary and Female groups than\\nMale. This seems to reflect in the predictions of BERT, i.e., the measure\\nACS shows that prediction intensities of anger are higher for Non-\\nbinary and Female groups than Male. Some other imbalanced emotion\\nassociations that exist in these corpora like sadness more associated\\nwith Male and Non-binary groups in the gender domain, joy more\\nassociated with European American racial group, fear more associated\\nwith Muslim, joy more associated with Christian, etc., are also seen\\nto be reflected in the predictions of BERT when evaluated using the\\nmeasure ACS. Similar to BERT, we can also observe the reflection of\\ncorpus level affective bias from pre-training and fine-tuning corpora\\nof GPT-2 (i.e., WebText-250k and SemEval-2018) to the predictions\\nof GPT-2, e.g., (1) high co-occurrence of fear with Female and Non-\\nbinary genders in the corpora, and high prediction intensities of fear for\\nFemale and Non-binary genders, (2) high co-occurrence of anger with\\nAfrican American race in the corpora, and high prediction intensities\\nof anger for African American, (3) high co-occurrence of fear with\\nMuslim religion in the corpora, and high prediction intensities of fear\\nfor Muslim, etc. Such examples of reflection of corpus level affective\\nbias in the predictions of PLMs are also visible in XLNet and T5. These\\ninstances give hints that affect imbalances in the large scale corpora of\\nPLMs may lead to affective bias in the predictions of the models that utilize\\nthese PLMs. Hence, this study further opens the scope for much more\\nnuanced explorations in the direction of affective bias propagation from\\nthe corpus to model prediction.\\n5.3. Societal stereotypes and affective bias\\nThe imbalanced/biased association of emotions with certain so-\\ncial groups within a domain, either at the corpus level or prediction\\nlevel, reflects several affect-oriented societal stereotypes. Patterns in\\nthe training corpora and predictions of PLM based textual emotion\\ndetection models showing high association of African American race\\nwith anger (an example plot of high anger prediction intensities for\\nAfrican American race is presented in Fig. 3(a)) reflect the ‘‘Angry\\nBlack’’ stereotype that misrepresents and victimizes blacks as hostile\\nin mainstream American culture and suppress their emotions (Lozada\\net al., 2022). Another pattern of high association of European American\\nrace with fear (an example plot of high fear prediction intensities for\\nEuropean American is presented in Fig. 3(b)) reflects the existence of\\nstereotypes such as fear of crime, residential integration, and racial\\nprejudice among the whites (Skogan, 1995). The high association of\\nNon-binary genders with negative emotions especially fear, and very\\nrarely associating with positive emotion joy, reflects the societal stigmas\\nlike homo-negativity and homophobia against these gender minorities\\n(Hahn et al., 2020). Similarly, the high association of Muslim religion\\nwith fear (an example plot of high fear prediction intensities for Muslim\\nis presented in Fig. 3(c)), which we believe may probably be due to the\\nIslamophobia manifested through text, are inline with the experimental\\nresults in Abid et al. (2021b) that reports language generated by GPT-\\n3 (Brown et al., 2020) in the context of the Muslim religion are more\\nassociated with violence.\\n5.4. Effectiveness of evaluation corpora in unveiling affective bias\\nWhen comparing the capability of the evaluation corpora EEC,\\nBITS, and CSP, we could observe that BITS, with a smaller number of\\nsentence pairs (120 for gender and 72 for race) and explicit emotion\\nterms, is mostly unable to recognize the existence of affective bias in\\nperspective of both class level and intensity level analysis. But even\\nthough EEC also has implicit representation of emotion terms similar\\nto BITS, the availability of a large number of sentence pairs (1400 for\\neach domain) eventually helps EEC to identify the existence of affective\\nbias better than BITS. On the other side, even with a smaller number\\nof sentence pairs (263 for gender, 566 for race, 104 for religion), the\\nevaluation corpus CSP helps to identify affective bias to a great extent,\\nand it is the only corpus that unveils class based affective bias in the\\ndomains. We believe the non-synthetic and real-world context nature\\nof sentence pairs in CSP could have been advantageous in identifying\\naffective bias. Therefore, upgrading such a corpus with more number\\nof sentence pairs or procuring new evaluation corpora containing non-\\nsynthetic real-world sentences, along with corresponding ground truth\\nemotions could eventually help towards comprehensive and rigorous\\nexplorations in the direction of identifying affective bias and quantify-\\ning its magnitude using ground truth dependent measures like Equal\\nOpportunity (Du et al., 2021).\\n6. Conclusion\\nTextual affective analysis and recognition enable efficient ways to\\nencode and understand human emotional states from textual data and\\nyield new opportunities to systems such as business, healthcare, and\\neducation by analyzing customers, employees, users, patients, etc., in\\nthe context of affective content. Unfair representations of affect in\\nlanguage, i.e. affective bias in such systems discriminate social groups\\nin a domain on the basis of certain emotions while making algorithmic\\ndecisions. Affective bias in textual emotion detection systems when\\ndeployed in the real world, can harm the ethical trust of these systems\\nand can be potentially threatening to human lives. Hence, analyzing\\nthe existence of affective bias in these systems is crucial to avoid huge\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 13}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nFig. 3. Intensity plots of emotion predictions reflecting societal stereotypes.\\ndisputes and damages in society similar to the adverse effects produced\\nby many other unfair systems such as unfair recidivism prediction.21\\nIn this work, we for the first time, to the best of our knowledge,\\nattempted to explore and identify any existence of affective bias in\\nlarge PLMs, when utilized for the task of textual emotion detection,\\nwith respect to the domains gender, race, and religion. For the study,\\nwe used BERT, GPT-2, XLNet, and T5 considering their popularity and\\nwide applicability in textual emotion detection and many other related\\ntasks. As algorithmic bias has its roots from data bias, we started our\\nexploration of affective bias by analyzing the imbalanced distribution of\\naffect in the pre-training corpora of these PLMs i.e., WikiEn, BookCor-\\npus, WebText-250, and C4-Val, and SemEval-2018 used to fine-tune the\\nemotion detection models. Later, we analyzed the existence of affective\\nbias in the predictions of fine-tuned emotion detection models built\\nusing these large PLMs. Evaluations are performed to analyze affective\\nbias in the predicted emotion classes and corresponding intensities of\\nsocial groups within a domain using three different evaluation corpora\\nand various class based and intensity based evaluation measures. Our\\nwide set of experiments and evaluation strategies confirm the existence\\nof affect imbalance in large scale corpora and affective bias in emotion\\npredictions of the PLMs, with affective bias mostly higher for T5\\ncompared to the other PLMs. The high association of emotion anger\\nwith African American race, joy with European American race, fear\\nwith the Muslim religion, etc., are some examples of affective bias.\\nReligious domain reports more biased instances, compared to gender\\nand race, for all the PLMs. Our results also demonstrated that the\\nbiased predictions of the models are inclined with patterns of affect\\n21 https://www.propublica.org/article/machine-bias-risk-assessments-in-\\ncriminal-sentencing?token=nD-X136_tDm0nh1l4Xtv0LbpjY_BSO3u.\\nimbalance in the corpora, and both these reflect certain affect-oriented\\nsocietal stereotypes, hinting at the propagation of affective bias towards\\npredictions of the PLMs. To aid future research, we shall make publicly\\navailable all the relevant materials including the pre-processed pre-\\ntraining and fine-tuning corpora, evaluation corpora modified to suit\\nour task, list of affective terms and target terms for corpus level\\nanalysis, source code, and fine-tuned textual emotion detection models\\nalong with their emotion class and intensity predictions, at https:\\n//github.com/anoopkdcs/affective_bias_in_plm and https://dcs.uoc.ac.\\nin/cida/projects/ac/affective-bias.html along with the publication.\\n6.1. Future work\\nThe proposed study explores corpus level affective bias using a sim-\\nple approach to analyzing the distributions of affective target terms in\\nthe corpora. In the future, we are planning to conduct a more nuanced\\nexploration towards the corpus level affective bias in the context of\\nvarious facets such as the time of creation of corpora, people behind\\ncorpora, languages and cultures (Navigli et al., 2023). Recent affect\\nagnostic bias analysis studies explore bias in the context of causality (Su\\net al., 2022); therefore to further explore the relationship between the\\ncorpus characteristics and model bias we are also planning to conduct\\ncausality based affective bias analysis.\\nIn context the model predictions, the observations of affective bias\\nand its magnitudes in this study are dependent on the choice of eval-\\nuation corpora and measures, i.e., certain instances of ‘no affective\\nbias’ or marginal magnitudes of affective bias may also be due to the\\nlimited capability of evaluation corpora and measures to unveil the\\nactual latent affective bias that exists in the model. Therefore in the\\nfuture, we are considering extending the study with a set of real-world\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 14}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\ncontext evaluation corpora, for example, by expanding CSP in terms of\\nthe number of sentences and also by procuring ground truth emotions\\nthat allow applying other evaluation measures like Equal Opportunity\\n(Du et al., 2021). Beyond analyzing each sentence pair in a domain\\nseparately, we are looking into the ways to simultaneously analyze\\nsentences representing various social groups in a domain, for example,\\nanalyzing sentence triplets like Male versus Female versus Non-binary.\\nA very recent and relevant work that addresses a similar line of\\nthoughts in the context of affect agnostic bias in PLMs from pre-training\\ndata to language models to downstream tasks in the political domain\\nis explained in Feng et al. (2023). In the backdrop of this work, we\\nobserve a wide scope of exploring political affective bias in large PLMs.\\nBecause, there are works in the literature that give hints that emotions\\nsuch as anger, disgust, or fear are more frequent in the predictions of\\nrepublicans’ (right-leaning) posts, whereas love or sadness are more\\noften predicted for democrats’ (left-leaning) posts (Huguet Cabot et al.,\\n2020).\\nOur initial attempt to identify affective bias in textual emotion\\ndetection models that utilize large PLMs, opens up the vast future scope\\ntowards identifying affective bias in the other very recent large PLMs\\nsuch as LLAMA, Flan-T5 XXL, PaLM, LaMDA, etc. There also exists a\\nwide scope for affective bias mitigation, which we believe, can be better\\nachieved by adopting more convenient solutions that utilize constraints\\nwhile fine-tuning the prediction system (i.e., in-processing) and post-\\nprocessing, rather than retraining or fine-tuning the PLM based affect\\nprediction systems with unbiased corpora which are expensive and\\ncumbersome (Hooker, 2021).\\nCRediT authorship contribution statement\\nAnoop Kadan: Conceptualization, Data curation, Formal analy-\\nsis, Investigation, Methodology, Resources, Validation, Visualization,\\nWriting – original draft, Writing – review & editing. Deepak P.: Con-\\nceptualization, Formal analysis, Methodology, Supervision, Writing –\\nreview & editing, Validation. Sahely Bhadra: Formal analysis, Method-\\nology, Supervision, Writing – review & editing, Validation. Manjary\\nP. Gangan: Conceptualization, Formal analysis, Methodology, Writing\\n– original draft, Writing – review & editing. Lajish V.L.: Supervision,\\nWriting – review & editing.\\nDeclaration of competing interest\\nThe authors declare that they have no known competing finan-\\ncial interests or personal relationships that could have appeared to\\ninfluence the work reported in this paper.\\nAcknowledgments\\nThe authors would like to thank the authors of Tan and Celis (2019)\\nfor making their source codes publicly available and the authors of Kir-\\nitchenko and Mohammad (2018), Venkit and Wilson (2021) and Nangia\\net al. (2020) for making their evaluation corpora publicly available.\\nThe authors would like to thank Chanjal V.V., Master’s student (2018–\\n20) of the Department of Women Studies, University of Calicut for her\\ninvolvement and cooperation to create the list of target terms related\\nto non-binary gender to conduct the corpus level experiments. The first\\nauthor would like to thank Indian Institute of Technology Palakkad for\\norganizing the GIAN course on Fairness in Machine Learning. The third\\nauthor would like to thank the Department of Science and Technology\\n(DST) of the Government of India for financial support through the\\nWomen Scientist Scheme-A (WOS-A) for Research in Basic/Applied\\nScience under the Grant SR/WOS-A/PM-62/2018.\\nAppendix A. Supplementary data\\nSupplementary material related to this article can be found online\\nat https://doi.org/10.1016/j.nlp.2024.100062.\\nReferences\\nAbid, A., Farooqi, M., Zou, J., 2021a. Large language models associate muslims with\\nviolence. Nat. Mach. Intell. 3 (6), 461–463. http://dx.doi.org/10.1038/s42256-021-\\n00359-2.\\nAbid, A., Farooqi, M., Zou, J., 2021b. Persistent anti-muslim bias in large language\\nmodels. In: Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and\\nSociety. Association for Computing Machinery, New York, NY, USA, pp. 298–306,\\nURL: https://doi.org/10.1145/3461702.3462624.\\nAcheampong, F.A., Nunoo-Mensah, H., Chen, W., 2021. Transformer models for text-\\nbased emotion detection: a review of BERT-based approaches. Artif. Intell. Rev. 54\\n(8), 5789–5829. http://dx.doi.org/10.1007/s10462-021-09958-2.\\nAdoma, A.F., Henry, N.-M., Chen, W., 2020. Comparative analyses of bert, roberta,\\ndistilbert, and xlnet for text-based emotion recognition. In: 2020 17th International\\nComputer Conference on Wavelet Active Media Technology and Information Pro-\\ncessing. ICCWAMTIP, pp. 117–121. http://dx.doi.org/10.1109/ICCWAMTIP51612.\\n2020.9317379.\\nAnoop, K., Gangan, M.P., Deepak, P., Lajish, V.L., 2022. Towards an enhanced\\nunderstanding of bias in pre-trained neural language models: A survey with\\nspecial emphasis on affective bias. In: Responsible Data Science. Springer Nature,\\nSingapore, pp. 13–45. http://dx.doi.org/10.1007/978-981-19-4453-6_2.\\nAshley, W., 2014. The angry black woman: The impact of pejorative stereotypes\\non psychotherapy with black women. Soc. Work Public Health 29 (1), 27–34.\\nhttp://dx.doi.org/10.1080/19371918.2011.619449.\\nBhaskaran, J., Bhallamudi, I., 2019. Good secretaries, bad truck drivers? Occupational\\ngender stereotypes in sentiment analysis. In: Proceedings of the First Workshop\\non Gender Bias in Natural Language Processing. Association for Computational\\nLinguistics, Italy, pp. 62–68. http://dx.doi.org/10.18653/v1/W19-3809.\\nBolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., Kalai, A., 2016. Man is to\\ncomputer programmer as woman is to homemaker? Debiasing word embeddings. In:\\nProceedings of the 30th International Conference on Neural Information Processing\\nSystems. NIPS ’16, Curran Associates Inc., Red Hook, NY, USA, pp. 4356–4364,\\nURL: https://dl.acm.org/doi/10.5555/3157382.3157584.\\nBordia, S., Bowman, S.R., 2019. Identifying and reducing gender bias in word-level\\nlanguage models. In: Proceedings of the 2019 Conference of the North American\\nChapter of the Association for Computational Linguistics: Student Research Work-\\nshop. Association for Computational Linguistics, Minneapolis, Minnesota, pp. 7–15.\\nhttp://dx.doi.org/10.18653/v1/N19-3002.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakan-\\ntan, A., Shyam, P., Sastry, G., Askell, A., et al., 2020. Language models are few-shot\\nlearners. Adv. Neural Inf. Process. Syst. 33, 1877–1901, URL: https://proceedings.\\nneurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\\nCaliskan, A., Bryson, J.J., Narayanan, A., 2017. Semantics derived automatically from\\nlanguage corpora contain human-like biases. Science 356 (6334), 183–186. http:\\n//dx.doi.org/10.1126/science.aal4230.\\nCenter, T.S., 2022. LGBTQIA+ terminology. URL: https://www.umass.edu/stonewall/\\nsites/default/files/documents/allyship_term_handout.pdf. Accessed: 4-7-2022.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P.,\\nChung, H.W., Sutton, C., Gehrmann, S., et al., 2023. Palm: Scaling language\\nmodeling with pathways. J. Mach. Learn. Res. 24 (240), 1–113, URL: https:\\n//www.jmlr.org/papers/volume24/22-1144/22-1144.pdf.\\nChung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X.,\\nDehghani, M., Brahma, S., et al., 2022. Scaling instruction-finetuned language\\nmodels. http://dx.doi.org/10.48550/arXiv.2210.11416, arXiv preprint arXiv:2210.\\n11416.\\nCorbett-Davies, S., Pierson, E., Feller, A., Goel, S., Huq, A., 2017. Algorithmic decision\\nmaking and the cost of fairness. In: Proceedings of the 23rd ACM SIGKDD\\nInternational Conference on Knowledge Discovery and Data Mining. KDD ’17,\\nAssociation for Computing Machinery, New York, NY, USA, pp. 797–806. http:\\n//dx.doi.org/10.1145/3097983.3098095.\\nDale, R., 2019. Law and word order: NLP in legal tech. Nat. Lang. Eng. 25 (1), 211–217.\\nhttp://dx.doi.org/10.1017/S1351324918000475.\\nDe Choudhury, M., Counts, S., Gamon, M., 2012. Not all moods are created equal!\\nexploring human emotional states in social media. In: Proceedings of the Inter-\\nnational AAAI Conference on Web and Social Media. Vol. 6, pp. 66–73, URL:\\nhttps://ojs.aaai.org/index.php/ICWSM/article/view/14279.\\nDevlin, J., Chang, M.-W., Lee, K., Toutanova, K., 2019. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In: Proceedings of the\\n2019 Conference of the North American Chapter of the Association for Compu-\\ntational Linguistics: Human Language Technologies, Volume 1 (Long and Short\\nPapers). Association for Computational Linguistics, Minneapolis, Minnesota, pp.\\n4171–4186. http://dx.doi.org/10.18653/v1/N19-1423, URL: https://aclanthology.\\norg/N19-1423.\\nDíaz, M., Johnson, I., Lazar, A., Piper, A.M., Gergle, D., 2018. Addressing age-related\\nbias in sentiment analysis. In: Proceedings of the 2018 Chi Conference on Human\\nFactors in Computing Systems. Association for Computing Machinery, New York,\\nNY, USA, pp. 1–14, URL: https://doi.org/10.1145/3173574.3173986.\\nDixon, L., Li, J., Sorensen, J., Thain, N., Vasserman, L., 2018. Measuring and mitigating\\nunintended bias in text classification. In: Proceedings of the 2018 AAAI/ACM Con-\\nference on AI, Ethics, and Society. AIES ’18, Association for Computing Machinery,\\nNew York, NY, USA, pp. 67–73. http://dx.doi.org/10.1145/3278721.3278729.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 15}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\nDu, M., Yang, F., Zou, N., Hu, X., 2021. Fairness in deep learning: A computational\\nperspective. IEEE Intell. Syst. 36 (4), 25–34. http://dx.doi.org/10.1109/MIS.2020.\\n3000681.\\nEagly, A.H., Steffen, V.J., 1984. Gender stereotypes stem from the distribution of\\nwomen and men into social roles. J. Pers. Soc. Psychol. 46 (4), 735, doi:https:\\n//psycnet.apa.org/doi/10.1037/0022-3514.46.4.735.\\nFeldman, M., Friedler, S.A., Moeller, J., Scheidegger, C., Venkatasubramanian, S.,\\n2015. Certifying and removing disparate impact. In: Proceedings of the 21th ACM\\nSIGKDD International Conference on Knowledge Discovery and Data Mining. KDD\\n’15, Association for Computing Machinery, New York, NY, USA, pp. 259–268.\\nhttp://dx.doi.org/10.1145/2783258.2783311.\\nFeng, S., Park, C.Y., Liu, Y., Tsvetkov, Y., 2023. From pretraining data to language\\nmodels to downstream tasks: Tracking the trails of political biases leading to\\nunfair NLP models. In: Rogers, A., Boyd-Graber, J., Okazaki, N. (Eds.), Proceedings\\nof the 61st Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers). Association for Computational Linguistics, Toronto,\\nCanada, pp. 11737–11762. http://dx.doi.org/10.18653/v1/2023.acl-long.656, URL:\\nhttps://aclanthology.org/2023.acl-long.656.\\nGarg, N., Schiebinger, L., Jurafsky, D., Zou, J., 2018. Word embeddings quantify\\n100 years of gender and ethnic stereotypes. Proc. Natl. Acad. Sci. 115 (16),\\nE3635–E3644.\\nGuo, W., Caliskan, A., 2021. Detecting emergent intersectional biases: Contextualized\\nword embeddings contain a distribution of human-like biases. In: Proceedings\\nof the 2021 AAAI/ACM Conference on AI, Ethics, and Society. Association for\\nComputing Machinery, New York, NY, USA, pp. 122–133, URL: https://doi.org/10.\\n1145/3461702.3462536.\\nHahn, H., Seager van Dyk, I., Ahn, W.-Y., 2020. Attitudes toward gay men and lesbian\\nwomen moderate heterosexual adults’ subjective stress response to witnessing\\nhomonegativity. Front. Psychol. 10, 2948. http://dx.doi.org/10.3389/fpsyg.2019.\\n02948.\\nHe, P., Liu, X., Gao, J., Chen, W., 2021. DEBERTA: Decoding-enhanced bert with\\ndisentangled attention. In: International Conference on Learning Representations.\\nURL: https://openreview.net/forum?id=XPZIaotutsD.\\nHooker, S., 2021. Moving beyond ‘‘algorithmic bias is a data problem’’. Patterns 2\\n(4), 100241. http://dx.doi.org/10.1016/j.patter.2021.100241, URL: https://www.\\nsciencedirect.com/science/article/pii/S2666389921000611.\\nHovy, D., Prabhumoye, S., 2021. Five sources of bias in natural language processing.\\nLang. Linguist. Compass 15 (8), e12432. http://dx.doi.org/10.1111/lnc3.12432,\\nURL: https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12432.\\nHuang, P.-S., Zhang, H., Jiang, R., Stanforth, R., Welbl, J., Rae, J., Maini, V.,\\nYogatama, D., Kohli, P., 2020. Reducing sentiment bias in language models\\nvia counterfactual evaluation. In: Findings of the Association for Computational\\nLinguistics: EMNLP 2020. Association for Computational Linguistics, Online, pp.\\n65–83. http://dx.doi.org/10.18653/v1/2020.findings-emnlp.7.\\nHuguet Cabot, P.-L., Dankers, V., Abadi, D., Fischer, A., Shutova, E., 2020. The\\npragmatics behind politics: Modelling metaphor, framing and emotion in political\\ndiscourse. In: Cohn, T., He, Y., Liu, Y. (Eds.), Findings of the Association for\\nComputational Linguistics: EMNLP 2020. Association for Computational Linguistics,\\nOnline, pp. 4479–4488. http://dx.doi.org/10.18653/v1/2020.findings-emnlp.402,\\nURL: https://aclanthology.org/2020.findings-emnlp.402.\\nKaneko, M., Bollegala, D., 2022. Unmasking the mask – evaluating social biases in\\nmasked language models. In: Proceedings of the 36th AAAI Conference on Artificial\\nIntelligence. Vancouver, BC, Canada, http://dx.doi.org/10.1609/aaai.v36i11.21453.\\nKiritchenko, S., Mohammad, S., 2018. Examining gender and race bias in two hundred\\nsentiment analysis systems. In: Proceedings of the Seventh Joint Conference on\\nLexical and Computational Semantics. Association for Computational Linguistics,\\nNew Orleans, Louisiana, pp. 43–53. http://dx.doi.org/10.18653/v1/S18-2005, URL:\\nhttps://aclanthology.org/S18-2005.\\nLiang, P.P., Wu, C., Morency, L.-P., Salakhutdinov, R., 2021. Towards understanding\\nand mitigating social biases in language models. In: Meila, M., Zhang, T. (Eds.),\\nProceedings of the 38th International Conference on Machine Learning. In: Pro-\\nceedings of Machine Learning Research, vol. 139, PMLR, pp. 6565–6576, URL:\\nhttps://proceedings.mlr.press/v139/liang21a.html.\\nLozada, F.T., Riley, T.N., Catherine, E., Brown, D.W., 2022. Black emotions matter:\\nUnderstanding the impact of racial oppression on black youth’s emotional devel-\\nopment: Dismantling systems of racism and oppression during adolescence. J. Res.\\nAdolesc. 32 (1), 13–33. http://dx.doi.org/10.1111/jora.12699.\\nLu, K., Mardziel, P., Wu, F., Amancharla, P., Datta, A., 2020. Gender bias in neural\\nnatural language processing. In: Logic, Language, and Security: Essays Dedicated\\nto Andre Scedrov on the Occasion of his 65th Birthday. Springer International\\nPublishing, Cham, pp. 189–202. http://dx.doi.org/10.1007/978-3-030-62077-6_14.\\nMao, R., Liu, Q., He, K., Li, W., Cambria, E., 2022. The biases of pre-trained language\\nmodels: An empirical study on prompt-based sentiment analysis and emotion\\ndetection. IEEE Trans. Affect. Comput. 1–11. http://dx.doi.org/10.1109/TAFFC.\\n2022.3204972.\\nMay, C., Wang, A., Bordia, S., Bowman, S.R., Rudinger, R., 2019. On measuring\\nsocial biases in sentence encoders. In: Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long and Short Papers). Association\\nfor Computational Linguistics, Minneapolis, Minnesota, pp. 622–628. http://dx.doi.\\norg/10.18653/v1/N19-1063.\\nMishev, K., Gjorgjevikj, A., Vodenska, I., Chitkushev, L.T., Trajanov, D., 2020. Evalu-\\nation of sentiment analysis in finance: From lexicons to transformers. IEEE Access\\n8, 131662–131682. http://dx.doi.org/10.1109/ACCESS.2020.3009626.\\nMohammad, S., Bravo-Marquez, F., Salameh, M., Kiritchenko, S., 2018. SemEval-2018\\ntask 1: Affect in tweets. In: Proceedings of the 12th International Workshop\\non Semantic Evaluation. Association for Computational Linguistics, New Or-\\nleans, Louisiana, pp. 1–17. http://dx.doi.org/10.18653/v1/S18-1001, URL: https:\\n//aclanthology.org/S18-1001.\\nNadeem, M., Bethke, A., Reddy, S., 2021. StereoSet: Measuring stereotypical bias\\nin pretrained language models. In: Proceedings of the 59th Annual Meeting of\\nthe Association for Computational Linguistics and the 11th International Joint\\nConference on Natural Language Processing (Volume 1: Long Papers). Association\\nfor Computational Linguistics, Online, pp. 5356–5371. http://dx.doi.org/10.18653/\\nv1/2021.acl-long.416, URL: https://aclanthology.org/2021.acl-long.416.\\nNangia, N., Vania, C., Bhalerao, R., Bowman, S.R., 2020. Crows-pairs: A challenge\\ndataset for measuring social biases in masked language models. In: Proceedings\\nof the 2020 Conference on Empirical Methods in Natural Language Processing.\\nEMNLP, Association for Computational Linguistics, Online, pp. 1953–1967. http:\\n//dx.doi.org/10.18653/v1/2020.emnlp-main.154, URL: https://aclanthology.org/\\n2020.emnlp-main.154.\\nNavigli, R., Conia, S., Ross, B., 2023. Biases in large language models: Origins,\\ninventory, and discussion. J. Data Inf. Qual. 15 (2), http://dx.doi.org/10.1145/\\n3597307.\\nPlant, E.A., Hyde, J.S., Keltner, D., Devine, P.G., 2000. The gender stereotyping of\\nemotions. Psychol. Women Q. 24 (1), 81–92. http://dx.doi.org/10.1111/j.1471-\\n6402.2000.tb01024.x.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al., 2019.\\nLanguage models are unsupervised multitask learners. OpenAI Blog 1 (8), 9, URL:\\nhttps://openai.com/blog/better-language-models/.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W.,\\nLiu, P.J., 2020. Exploring the limits of transfer learning with a unified text-to-text\\ntransformer. J. Mach. Learn. Res. 21 (140), 1–67, URL: http://jmlr.org/papers/v21/\\n20-074.html.\\nRahman, M.M., Siddiqui, F.H., 2019. An optimized abstractive text summarization\\nmodel using peephole convolutional LSTM. Symmetry 11 (10), http://dx.doi.org/\\n10.3390/sym11101290, URL: https://www.mdpi.com/2073-8994/11/10/1290.\\nRahman, M.M., Siddiqui, F.H., 2021. Multi-layered attentional peephole convolu-\\ntional LSTM for abstractive text summarization. ETRI J. 43 (2), 288–298. http:\\n//dx.doi.org/10.4218/etrij.2019-0016, URL: https://onlinelibrary.wiley.com/doi/\\nabs/10.4218/etrij.2019-0016.\\nRaza, S., Garg, M., Reji, D.J., Bashir, S.R., Ding, C., 2024. Nbias: A natural lan-\\nguage processing framework for BIAS identification in text. Expert Syst. Appl.\\n237, 121542. http://dx.doi.org/10.1016/j.eswa.2023.121542, URL: https://www.\\nsciencedirect.com/science/article/pii/S0957417423020444.\\nRozado, D., 2020. Wide range screening of algorithmic bias in word embedding models\\nusing large sentiment lexicons reveals underreported bias types. PLoS One 15 (4),\\n1–26. http://dx.doi.org/10.1371/journal.pone.0231189.\\nShen, J.H., Fratamico, L., Rahwan, I., Rush, A.M., 2018. Darling or babygirl? investi-\\ngating stylistic bias in sentiment analysis. Proc. FATML URL: https://www.fatml.\\norg/media/documents/darling_or_babygirl_stylistic_bias.pdf.\\nShields, S.A., 2002. Speaking from the Heart: Gender and the Social Meaning of\\nEmotion. Cambridge University Press.\\nSkogan,\\nW.G.,\\n1995.\\nCrime\\nand\\nthe\\nracial\\nfears\\nof\\nwhite\\nAmericans.\\nAnn.\\nAm.\\nAcad.\\nPolitical\\nSoc.\\nSci.\\n539\\n(1),\\n59–71.\\nhttp://dx.doi.org/10.1177/\\n0002716295539001005.\\nSoni, S., Roberts, K., 2020. Evaluation of dataset selection for pre-training and fine-\\ntuning transformer language models for clinical question answering. In: Proceedings\\nof the 12th Language Resources and Evaluation Conference. European Language Re-\\nsources Association, Marseille, France, pp. 5532–5538, URL: https://aclanthology.\\norg/2020.lrec-1.679.\\nStaiano, J., Guerini, M., 2014. Depeche mood: a lexicon for emotion analysis from\\ncrowd annotated news. In: Proceedings of the 52nd Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 2: Short Papers). Association\\nfor Computational Linguistics, Baltimore, Maryland, pp. 427–433. http://dx.doi.\\norg/10.3115/v1/P14-2070, URL: https://aclanthology.org/P14-2070.\\nSu, C., Yu, G., Wang, J., Yan, Z., Cui, L., 2022. A review of causality-based fairness\\nmachine learning. Intell. Robot. 244–274. http://dx.doi.org/10.20517/ir.2022.17.\\nSubramanian, S., Rahimi, A., Baldwin, T., Cohn, T., Frermann, L., 2021. Fairness-aware\\nclass imbalanced learning. In: Moens, M.-F., Huang, X., Specia, L., Yih, S.W.-\\nt. (Eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural\\nLanguage Processing. Association for Computational Linguistics, Online and Punta\\nCana, Dominican Republic, pp. 2045–2051. http://dx.doi.org/10.18653/v1/2021.\\nemnlp-main.155, URL: https://aclanthology.org/2021.emnlp-main.155.\\nSuresh, H., Guttag, J., 2021. A framework for understanding sources of harm\\nthroughout the machine learning life cycle. In: Equity and Access in Algorithms,\\nMechanisms, and Optimization. EAAMO ’21, Association for Computing Machinery,\\nNew York, NY, USA, http://dx.doi.org/10.1145/3465416.3483305.\\nSweeney, C., Najafian, M., 2020. Reducing sentiment polarity for demographic at-\\ntributes in word embeddings using adversarial learning. In: Proceedings of the\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2024-06-11T17:54:50+05:30', 'source': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'file_path': '../data/pdf_files/1-s2.0-S2949719124000104-main.pdf', 'total_pages': 17, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Natural Language Processing Journal, 7 (2024) 100062. doi:10.1016/j.nlp.2024.100062', 'keywords': '', 'moddate': '2024-06-11T17:54:50+05:30', 'trapped': '', 'modDate': \"D:20240611175450+05'30'\", 'creationDate': \"D:20240611175450+05'30'\", 'page': 16}, page_content='A. Kadan, Deepak P., S. Bhadra et al.\\nNatural Language Processing Journal 7 (2024) 100062\\n2020 Conference on Fairness, Accountability, and Transparency. In: FAT* ’20,\\nAssociation for Computing Machinery, New York, NY, USA, pp. 359–368. http:\\n//dx.doi.org/10.1145/3351095.3372837.\\nTabinda Kokab, S., Asghar, S., Naz, S., 2022. Transformer-based deep learning mod-\\nels for the sentiment analysis of social media data. Array 14, 100157. http:\\n//dx.doi.org/10.1016/j.array.2022.100157, URL: https://www.sciencedirect.com/\\nscience/article/pii/S2590005622000224.\\nTan, Y.C., Celis, L.E., 2019. Assessing social and intersectional biases in contextualized\\nword representations. In: Proceedings of the 33rd International Conference on\\nNeural Information Processing Systems. Curran Associates Inc., Red Hook, NY, USA,\\npp. 13230–13241, URL: https://dl.acm.org/doi/10.5555/3454287.3455472.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T.,\\nJin, A., Bos, T., Baker, L., Du, Y., et al., 2022. Lamda: Language models for\\ndialog applications. http://dx.doi.org/10.48550/arXiv.2201.08239, arXiv preprint\\narXiv:2201.08239.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B.,\\nGoyal, N., Hambro, E., Azhar, F., et al., 2023. Llama: Open and efficient foundation\\nlanguage models. arXiv preprint arXiv:2302.13971. URL: https://research.facebook.\\ncom/publications/llama-open-and-efficient-foundation-language-models/.\\nTrinh, T.H., Le, Q.V., 2018. A simple method for commonsense reasoning. http://dx.\\ndoi.org/10.48550/arXiv.1806.02847, arXiv preprint arXiv:1806.02847.\\nVelupillai, S., Suominen, H., Liakata, M., Roberts, A., Shah, A.D., Morley, K., Os-\\nborn, D., Hayes, J., Stewart, R., Downs, J., Chapman, W., Dutta, R., 2018.\\nUsing clinical natural language processing for health outcomes research: Overview\\nand actionable suggestions for future advances. J. Biomed. Inform. 88, 11–19.\\nhttp://dx.doi.org/10.1016/j.jbi.2018.10.005, URL: https://www.sciencedirect.com/\\nscience/article/pii/S1532046418302016.\\nVenkit, P.N., Wilson, S., 2021. Identification of bias against people with disabilities\\nin sentiment analysis and toxicity detection models. http://dx.doi.org/10.48550/\\narXiv.2111.13259, arXiv preprint arXiv:2111.13259.\\nVittengl, J.R., Holt, C.S., 1998. A time-series diary study of mood and social interaction.\\nMotiv. Emot. 22 (3), 255–275. http://dx.doi.org/10.1023/A:1022388123550.\\nWaterloo, S.F., Baumgartner, S.E., Peter, J., Valkenburg, P.M., 2018. Norms of\\nonline expressions of emotion: Comparing facebook, Twitter, instagram, and\\nWhatsApp.\\nNew\\nMedia\\nSoc.\\n20\\n(5),\\n1813–1831.\\nhttp://dx.doi.org/10.1177/\\n1461444817707349, PMID: 30581358.\\nYang, Z., Asyrofi, M.H., Lo, D., 2021. BiasRV: Uncovering biased sentiment predictions\\nat runtime. In: Proceedings of the 29th ACM Joint Meeting on European Software\\nEngineering Conference and Symposium on the Foundations of Software Engineer-\\ning. In: ESEC/FSE 2021, Association for Computing Machinery, New York, NY,\\nUSA, pp. 1540–1544. http://dx.doi.org/10.1145/3468264.3473117.\\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., Le, Q.V., 2019. XLNet:\\nGeneralized autoregressive pretraining for language understanding. In: Proceedings\\nof the 33rd International Conference on Neural Information Processing Systems.\\nCurran Associates Inc., Red Hook, NY, USA, pp. 5753–5763, URL: https://dl.acm.\\norg/doi/10.5555/3454287.3454804.\\nZhang, L., Fan, H., Peng, C., Rao, G., Cong, Q., 2020. Sentiment analysis methods\\nfor HPV vaccines related tweets based on transfer learning. Healthcare 8 (3),\\nhttp://dx.doi.org/10.3390/healthcare8030307, URL: https://www.mdpi.com/2227-\\n9032/8/3/307.\\nZhao, J., Wang, T., Yatskar, M., Cotterell, R., Ordonez, V., Chang, K.-W., 2019. Gender\\nbias in contextualized word embeddings. In: Proceedings of the 2019 Conference\\nof the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long and Short Papers). Association for\\nComputational Linguistics, Minneapolis, Minnesota, pp. 629–634. http://dx.doi.org/\\n10.18653/v1/N19-1064, URL: https://aclanthology.org/N19-1064.\\nZhao, J., Wang, T., Yatskar, M., Ordonez, V., Chang, K.-W., 2018. Gender bias in\\ncoreference resolution: Evaluation and debiasing methods. In: Proceedings of the\\n2018 Conference of the North American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies, Volume 2 (Short Papers).\\nAssociation for Computational Linguistics, New Orleans, Louisiana, pp. 15–20.\\nhttp://dx.doi.org/10.18653/v1/N18-2003.\\nZhiltsova, A., Caton, S., Mulway, C., 2019. Mitigation of unintended biases against\\nnon-native english texts in sentiment analysis. In: Proceedings for the 27th AIAI\\nIrish Conference on Artificial Intelligence and Cognitive Science, Galway, Ireland,\\nDecember 5-6, 2019. In: CEUR Workshop Proceedings, vol. 2563, CEUR-WS.org,\\npp. 317–328, URL: http://ceur-ws.org/Vol-2563/aics_30.pdf.\\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., Fidler, S.,\\n2015. Aligning books and movies: Towards story-like visual explanations by\\nwatching movies and reading books. In: Proceedings of the 2015 IEEE International\\nConference on Computer Vision. ICCV, IEEE Computer Society, USA, pp. 19–27.\\nhttp://dx.doi.org/10.1109/ICCV.2015.11.\\n17')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf_files/\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05279018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ccc029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
